# Work in progress

temporary linkï¼š[zhihu](https://zhuanlan.zhihu.com/p/678869912)

**Oral** 1

1. <details>
          <summary>Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!</summary>
           Keywords: LLM, Fine-tuning, AI Alignment, Jailbreaking
   </details>


**Spotlight**

1. <details>
          <summary>Beyond Memorization: Violating Privacy via Inference with Large Language Models</summary>
          Keywords: Prvicay, LLM
   </details>

2. <details>
          <summary>Overthinking the Truth: Understanding how Language Models Process False Demonstrations</summary>
          Keywords: Mechanistic Interpretability, AI Safety, Interpretability, Science of ML, few-shot learning, Large Language Models
   </details>

3. <details>
          <summary>Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation</summary>
          Keywords: Large Language Model, Alignment, Attack
   </details>

4. <details>
          <summary>Safe RLHF: Safe Reinforcement Learning from Human Feedback</summary>
          Keywords: Large Language Model, RL
   </details>

5. <details>
          <summary>Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models</summary>
          Keywords: Adversarial attacks, Vision encoders, Jailbreak, Prompt Injection, Security, Embedding space attacks, Black box, LLM, Vision-Language Models, Multi-Modal Models, VLM, Alignment, Cross-Modality alignment
   </details>

6. <details>
          <summary>Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks</summary>
          Keywords: ensitive Information Deletion, Privacy Attacks, Model editing, Language Models
   </details>

7. <details>
          <summary>Identifying the Risks of LM Agents with an LM-Emulated Sandbox</summary>
          Keywords: Language Model Agent, Tool Use, Evaluation, Safety, Language Model
   </details>

8. <details>
          <summary>Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory</summary>
          Keywords: Contextual Integrity, Privacy, Theory of Mind
   </details>

9. <details>
          <summary>DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer</summary>
          Keywords: large language model, privacy, prompt tuing
   </details>

10. <details>
          <summary>Illusory Attacks: Information-theoretic detectability matters in adversarial attacks</summary>
          Keywords: sequential decision making, adversarial attacks, robust human-AI systems, robust mixed-autonomy systems
   </details>


11. <details>
          <summary>Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game</summary>
          Keywords:  large language models, LLMs, security, adversarial examples, prompt extraction, prompt injection, prompt hijacking, prompt engineering
   </details>

