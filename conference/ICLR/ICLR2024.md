# Work in progress

temporary linkï¼š[zhihu](https://zhuanlan.zhihu.com/p/678869912)


**Oral** 1

1. <details>
          <summary>Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!</summary>
           Keywords: LLM, Fine-tuning, AI Alignment, Jailbreaking
   </details>


**Spotlight**

1. <details>
          <summary>Beyond Memorization: Violating Privacy via Inference with Large Language Models</summary>
          Keywords: Prvicay, LLM
   </details>

2. <details>
          <summary>Overthinking the Truth: Understanding how Language Models Process False Demonstrations</summary>
          Keywords: Mechanistic Interpretability, AI Safety, Interpretability, Science of ML, few-shot learning, Large Language Models
   </details>

3. <details>
          <summary>Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation</summary>
          Keywords: Large Language Model, Alignment, Attack
   </details>

4. <details>
          <summary>Safe RLHF: Safe Reinforcement Learning from Human Feedback</summary>
          Keywords: Large Language Model, RL
   </details>

5. <details>
          <summary>Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models</summary>
          Keywords: Adversarial attacks, Vision encoders, Jailbreak, Prompt Injection, Security, Embedding space attacks, Black box, LLM, Vision-Language Models, Multi-Modal Models, VLM, Alignment, Cross-Modality alignment
   </details>

6. <details>
          <summary>Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks</summary>
          Keywords: ensitive Information Deletion, Privacy Attacks, Model editing, Language Models
   </details>

7. <details>
          <summary><b>Identifying the Risks of LM Agents with an LM-Emulated Sandbox</b></summary>
          Keywords: Language Model Agent, Tool Use, Evaluation, Safety, Language Model
   </details>

8. <details>
          <summary>Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory</summary>
          Keywords: Contextual Integrity, Privacy, Theory of Mind
   </details>

9. <details>
          <summary>DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer</summary>
          Keywords: large language model, privacy, prompt tuing
   </details>

10. <details>
          <summary>Illusory Attacks: Information-theoretic detectability matters in adversarial attacks</summary>
          Keywords: sequential decision making, adversarial attacks, robust human-AI systems, robust mixed-autonomy systems
   </details>


11. <details>
           <summary><b>Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game</b></summary>
          Keywords:  large language models, LLMs, security, adversarial examples, prompt extraction, prompt injection, prompt hijacking, prompt engineering
   </details>

**Poster**

**Training Socially Aligned Language Models on Simulated Social Interactions**

**Curiosity-driven Red-teaming for Large Language Models**

Teach LLMs to Phish: Stealing Private Information from Language Models

Multilingual Jailbreak Challenges in Large Language Models

Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions

Language Model Inversion 

BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models

RAIN: Your Language Models Can Align Themselves without Finetuning

An LLM can Fool Itself: A Prompt-Based Adversarial Attack

Demystifying Poisoning Backdoor Attacks from a Statistical Perspectivepdf icon

Safe and Robust Watermark Injection with a Single OoD Image

On the Vulnerability of Adversarially Trained Models Against Two-faced Attacks

BRUSLEATTACK: A QUERY-EFFICIENT SCORE- BASED BLACK-BOX SPARSE ADVERSARIAL ATTACK

AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models

Universal Jailbreak Backdoors from Poisoned Human Feedback

GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher
