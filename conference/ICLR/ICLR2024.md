# Work in progress

temporary linkï¼š[zhihu](https://zhuanlan.zhihu.com/p/678869912)

**Oral** 1

1. <details>
          <summary>Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!</summary>
           Keywords: LLM, Fine-tuning, AI Alignment, Jailbreaking
   </details>


**Spotlight**

1. <details>
          <summary>Beyond Memorization: Violating Privacy via Inference with Large Language Models</summary>
          Keywords: Prvicay, LLM
   </details>

2. <details>
          <summary>Overthinking the Truth: Understanding how Language Models Process False Demonstrations</summary>
          Keywords: Mechanistic Interpretability, AI Safety, Interpretability, Science of ML, few-shot learning, Large Language Models
   </details>

3. <details>
          <summary>Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation</summary>
          Keywords: Large Language Model, Alignment, Attack
   </details>

3. <details>
          <summary>Safe RLHF: Safe Reinforcement Learning from Human Feedback</summary>
          Keywords: Large Language Model, RL
   </details>

4. <details>
          <summary>Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models</summary>
          Keywords: Adversarial attacks, Vision encoders, Jailbreak, Prompt Injection, Security, Embedding space attacks, Black box, LLM, Vision-Language Models, Multi-Modal Models, VLM, Alignment, Cross-Modality alignment
   </details>

5. <details>
          <summary>Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models</summary>
          Keywords: Adversarial attacks, Vision encoders, Jailbreak, Prompt Injection, Security, Embedding space attacks, Black box, LLM, Vision-Language Models, Multi-Modal Models, VLM, Alignment, Cross-Modality alignment
   </details>

6. <details>
          <summary>Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks</summary>
          Keywords: ensitive Information Deletion, Privacy Attacks, Model editing, Language Models
   </details>

7. <details>
          <summary>Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks</summary>
          Keywords: ensitive Information Deletion, Privacy Attacks, Model editing, Language Models
   </details>


8. <details>
          <summary>Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks</summary>
          Keywords: Data poisoning attack; generalization; deep learning
   </details>

9. <details>
          <summary>Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks</summary>
          Keywords: Data poisoning attack; generalization; deep learning
   </details>

10. <details>
          <summary>Identifying the Risks of LM Agents with an LM-Emulated Sandbox</summary>
          Keywords: Language Model Agent, Tool Use, Evaluation, Safety, Language Model
   </details>
