
Oral

1. <details>
          <summary>Students Parrot Their Teachers: Membership Inference on Model Distillation</summary>
          TL; DR: Model distillation is not resistant to strong membership inference attack.
   </details>

2. <details>
          <summary>Jailbroken: How Does LLM Safety Training Fail?</summary>
   </details>

Spotlight

1. <details>
          <summary>Honesty Is the Best Policy: Defining and Mitigating AI Deception</summary>
          TL; DR: We formally define deception in the causal game framework and mitigate deception in reinforcement learning agents and language models.
   </details>

2.  <details>
          <summary>ProPILE: Probing Privacy Leakage in Large Language Models</summary>
   </details>

3. <details>
          <summary>Django: Detecting Trojans in Object Detection Models via Gaussian Focus Calibration</summary>
          Keywords: Backdoor
   </details>

4. <details>
          <summary>Django: Label Poisoning is All You Need</summary>
          Keywords: Backdoor
   </details>

5. <details>
          <summary>**Are aligned neural networks adversarially aligned?**</summary>
          TL;DR: We show "aligned" models that are normally harmless are not "adversarially aligned" and can be adversarially attacked to disregard their RLHF training and produce harmful output---especially when they are multimodal.
   </details>

6. 
