# ğŸ›¡ï¸Awesome LLM-SafetyğŸ›¡ï¸[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

<p align="center">
<a href=""> <img src="https://img.shields.io/github/stars/ydyjya/Awesome-LLM-Safety?style=flat-square&logo=github" alt="GitHub stars"></a>
<a href=""> <img src="https://img.shields.io/github/forks/ydyjya/Awesome-LLM-Safety?style=flat-square&logo=github" alt="GitHub forks"></a>
<a href=""> <img src="https://img.shields.io/github/issues/ydyjya/Awesome-LLM-Safety?style=flat-square&logo=github" alt="GitHub issues"></a>
<a href=""> <img src="https://img.shields.io/github/last-commit/ydyjya/Awesome-LLM-Safety?style=flat-square&logo=github" alt="GitHub Last commit"></a>
</p>

## ğŸ¤—Introduction


ğŸ¥°ğŸ¥°ğŸ¥°**Welcome to our awesome-llm-safety repository!** ğŸ¥°ğŸ¥°ğŸ¥°

In this repo, you can retrieve the latestğŸ˜‹, comprehensiveğŸ˜, safety papers on large language models. In addition, we will also do our best to update Talks, Presentations, Tutorials, Workshops, news and ArticlesğŸ¤—.


If the paper touches on multiple subtopics under security, we will place it under each subtopic.

> Like Awesome-LLM-Safety will appear in Tutorials for each subtopicğŸ¤©!

**Letâ€™s start LLM Safety tutorial!**

---

## ğŸš€Table of Contents

- [ğŸ›¡ï¸Awesome LLM-SafetyğŸ›¡ï¸](#ï¸awesome-llm-safetyï¸)
  - [ğŸ¤—Introduction](#introduction)
  - [ğŸš€Table of Contents](#table-of-contents)
  - [ğŸ”Security Tutorial](#security-tutorial)
    - [ğŸ“‘Papers](#papers)
    - [ğŸ“–Tutorials, Articles, Presentations and Talks](#tutorials-articles-presentations-and-talks)
    - [Other](#other)
  - [ğŸ”Privacy Tutorial](#privacy-tutorial)
    - [ğŸ“‘Papers](#papers-1)
    - [ğŸ“–Tutorials, Articles, Presentations and Talks](#tutorials-articles-presentations-and-talks-1)
    - [Other](#other-1)
  - [ğŸ“°Truthfulness\&Misinformation Tutorial](#truthfulnessmisinformation-tutorial)
    - [ğŸ“‘Papers](#papers-2)
    - [ğŸ“–Tutorials, Articles, Presentations and Talks](#tutorials-articles-presentations-and-talks-2)
    - [Other](#other-2)
  - [ğŸ˜ˆJailBreak \& Attacks Tutorial](#jailbreak--attacks-tutorial)
    - [ğŸ“‘Papers](#papers-3)
    - [ğŸ“–Tutorials, Articles, Presentations and Talks](#tutorials-articles-presentations-and-talks-3)
    - [Other](#other-3)
  - [ğŸ›¡ï¸Defenses Tutorial](#ï¸defenses-tutorial)
    - [ğŸ“–Tutorials, Articles, Presentations and Talks](#tutorials-articles-presentations-and-talks-4)
    - [Other](#other-4)
  - [ğŸ’¯Datasets \& Benchmark Tutorial](#datasets--benchmark-tutorial)
    - [ğŸ“‘Papers](#papers-4)
    - [ğŸ“–Tutorials, Articles, Presentations and Talks](#tutorials-articles-presentations-and-talks-5)
    - [ğŸ“šResourceğŸ“š](#resource)
    - [Other](#other-5)
  - [ğŸ§‘â€ğŸ“Author](#author)



---
## ğŸ”Security Tutorial

### ğŸ“‘Papers
|Date|Institute|Publication|Paper|
|:-:|:-:|:-:|:-:|
|20.10|Facebook AI Research|arxiv|[Recipes for Safety in Open-domain Chatbots](https://arxiv.org/abs/2010.07079)|
|23.07|UC Berkeley|arxiv|[Jailbroken: How Does LLM Safety Training Fail?](https://arxiv.org/abs/2307.02483)|
|23.07|CMU|arxiv|[Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)|


### ğŸ“–Tutorials, Articles, Presentations and Talks

|Date|Type|Title|URL|
|:-:|:-:|:-:|:-:|
|23.10|Tutorials|Awesome-LLM-Safety|[link](https://github.com/ydyjya/Awesome-LLM-Safety)|


### Other

ğŸ‘‰[Latest&Comprehensive Security Paper](.//subtopic/Security.md)

---
## ğŸ”Privacy Tutorial


### ğŸ“‘Papers
|Date|Institute|Publication|Paper|
|:-:|:-:|:-:|:-:|



### ğŸ“–Tutorials, Articles, Presentations and Talks

|Date|Type|Title|URL|
|:-:|:-:|:-:|:-:|
|23.10|Tutorials|Awesome-LLM-Safety|[link](https://github.com/ydyjya/Awesome-LLM-Safety)|


### Other

ğŸ‘‰[Latest&Comprehensive Privacy Paper](.//subtopic/Privacy.md)

---
## ğŸ“°Truthfulness&Misinformation Tutorial


### ğŸ“‘Papers
|Date|Institute|Publication|Paper|
|:-:|:-:|:-:|:-:|
|21.09|University of Oxford|ACL2022|[TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)|


### ğŸ“–Tutorials, Articles, Presentations and Talks

|Date|Type|Title|URL|
|:-:|:-:|:-:|:-:|
|23.10|Tutorials|Awesome-LLM-Safety|[link](https://github.com/ydyjya/Awesome-LLM-Safety)|

### Other

ğŸ‘‰[Latest&Comprehensive Truthfulness&Misinformation Paper](./subtopic/Truthfulness&Misinformation.md)

---
## ğŸ˜ˆJailBreak & Attacks Tutorial

### ğŸ“‘Papers
|Date|Institute|Publication|Paper|
|:-:|:-:|:-:|:-:|
|22.11|AE Studio|NIPS2022(ML Safety Workshop)|[Ignore Previous Prompt: Attack Techniques For Language Models](https://arxiv.org/abs/2211.09527)|
|23.06|Google|arxiv|[Are aligned neural networks adversarially aligned?](https://arxiv.org/abs/2306.15447)|
|23.07|CMU|arxiv|[Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)|

### ğŸ“–Tutorials, Articles, Presentations and Talks

|Date|Type|Title|URL|
|:-:|:-:|:-:|:-:|
|23.10|Tutorials|Awesome-LLM-Safety|[link](https://github.com/ydyjya/Awesome-LLM-Safety)|

### Other

ğŸ‘‰[Latest&Comprehensive JailBreak & Attacks Paper](./subtopic/JailBreak&Attacks.md)

---
## ğŸ›¡ï¸Defenses Tutorial

|Date|Institute|Publication|Paper|
|:-:|:-:|:-:|:-:|
|22.04|Anthropic|arxiv|[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2204.05862)|



### ğŸ“–Tutorials, Articles, Presentations and Talks

|Date|Type|Title|URL|
|:-:|:-:|:-:|:-:|
|23.10|Tutorials|Awesome-LLM-Safety|[link](https://github.com/ydyjya/Awesome-LLM-Safety)|

### Other

ğŸ‘‰[Latest&Comprehensive Defenses Paper](./subtopic/Defenses.md)


--- 
## ğŸ’¯Datasets & Benchmark Tutorial

### ğŸ“‘Papers
|Date|Institute|Publication|Paper|
|:-:|:-:|:-:|:-:|
|20.09|University of Washington|EMNLP2020(findings)|[RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/abs/2009.11462)|
|21.09|University of Oxford|ACL2022|[TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)|
|22.03|MIT|ACL2022|[ToxiGen: A Large-Scale Machine-Generated datasets for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/abs/2203.09509)|


### ğŸ“–Tutorials, Articles, Presentations and Talks

|Date|Type|Title|URL|
|:-:|:-:|:-:|:-:|
|23.10|Tutorials|Awesome-LLM-Safety|[link](https://github.com/ydyjya/Awesome-LLM-Safety)|

### ğŸ“šResourceğŸ“š
- Toxicity - [RealToxicityPrompts datasets](https://toxicdegeneration.allenai.org/)
- Truthfulness - [TruthfulQA datasets](https://github.com/sylinrl/TruthfulQA)

### Other
ğŸ‘‰[Latest&Comprehensive datasets & Benchmark Paper](./subtopic/Datasets&Benchmark.md)


---
## ğŸ§‘â€ğŸ“Author


**ğŸ¤—If you have any questions, please contact our authors!ğŸ¤—**

âœ‰ï¸: [ydyjya](https://github.com/ydyjya) â¡ï¸ zhouzhenhong@bupt.edu.cn


[![Star History Chart](https://api.star-history.com/svg?repos=ydyjya/Awesome-LLM-Safety&type=Date)](https://star-history.com/#ydyjya/Awesome-LLM-Safety&Date)

**[â¬† Back to ToC](#table-of-contents)**