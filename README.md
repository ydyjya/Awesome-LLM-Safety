# LLM-SecurityPapers


## Table of Contents

- [LLM-SecurityPapers](#llm-securitypapers)
  - [Table of Contents](#table-of-contents)
  - [Introduction](#introduction)
  - [Security](#security)
      - [ğŸ“‘PapersğŸ“‘](#papers)
      - [ğŸ’»Presentations \& TalksğŸ’»](#presentations--talks)
      - [ğŸ“–Tutorials \& WorkshopsğŸ“–](#tutorials--workshops)
      - [ğŸ“°News \& ArticlesğŸ“°](#news--articles)
  - [Privacy](#privacy)
      - [ğŸ“‘PapersğŸ“‘](#papers-1)
      - [ğŸ’»Presentations \& TalksğŸ’»](#presentations--talks-1)
      - [ğŸ“–Tutorials \& WorkshopsğŸ“–](#tutorials--workshops-1)
      - [ğŸ“°News \& ArticlesğŸ“°](#news--articles-1)
  - [Misinformation \& Hallucination](#misinformation--hallucination)
      - [ğŸ“‘PapersğŸ“‘](#papers-2)
      - [ğŸ’»Presentations \& TalksğŸ’»](#presentations--talks-2)
      - [ğŸ“–Tutorials \& WorkshopsğŸ“–](#tutorials--workshops-2)
      - [ğŸ“°News \& ArticlesğŸ“°](#news--articles-2)
  - [Adversarial Attacks](#adversarial-attacks)
      - [ğŸ“‘PapersğŸ“‘](#papers-3)
      - [ğŸ’»Presentations \& TalksğŸ’»](#presentations--talks-3)
      - [ğŸ“–Tutorials \& WorkshopsğŸ“–](#tutorials--workshops-3)
      - [ğŸ“°News \& ArticlesğŸ“°](#news--articles-3)
  - [Defenses](#defenses)
      - [ğŸ“‘PapersğŸ“‘](#papers-4)
      - [ğŸ’»Presentations \& TalksğŸ’»](#presentations--talks-4)
      - [ğŸ“–Tutorials \& WorkshopsğŸ“–](#tutorials--workshops-4)
      - [ğŸ“°News \& ArticlesğŸ“°](#news--articles-4)
  - [Dataset \& Benchmark](#dataset--benchmark)
      - [ğŸ“‘PapersğŸ“‘](#papers-5)
      - [ğŸ“šResourceğŸ“š](#resource)
  - [Author](#author)

## Introduction


ğŸ¥°ğŸ¥°ğŸ¥°**Welcome to our awesome-llm-safety-paper-list repository!** ğŸ¥°ğŸ¥°ğŸ¥°

In this repo, you can retrieve the latestğŸ˜‹, comprehensiveğŸ˜, safety papers on large language models. In addition, we will also do our best to update Talks, Presentations, Tutorials, Workshops, news and ArticlesğŸ¤—.

ğŸ˜ŠIn order to allow everyone to search for the paper they need more quickly, we will mark the paper with several keywords. 

For example, 
- 23.10(**Timestamp**) - GitHub(**Publication Information**) - [awesome-LLM-safety-paper-list](https://github.com/ydyjya/awesome-LLM-safety-paper-list)
  - **Repo(keyword1)**&**Tutorials(keyword2)**

If the paper touches on multiple subtopics under security, we will place it under each subtopic.

> Like awesome-LLM-safety-paper-list will appear in Tutorials for each subtopicğŸ¤©!

**Letâ€™s start learning LLM Safety!**

---
## Security

#### ğŸ“‘PapersğŸ“‘

- 23.07 - arxiv - [Jailbroken: How Does LLM Safety Training Fail?](https://arxiv.org/abs/2307.02483)
  - **Jailbreak**&**Security**
- 23.07 - arxiv  - [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)
  - **Jailbreak**

#### ğŸ’»Presentations & TalksğŸ’»


#### ğŸ“–Tutorials & WorkshopsğŸ“–

- 23.10 - GitHub - [awesome-LLM-safety-paper-list](https://github.com/ydyjya/awesome-LLM-safety-paper-list)
  - **Repo**&**Tutorials**

#### ğŸ“°News & ArticlesğŸ“°


---
## Privacy

#### ğŸ“‘PapersğŸ“‘
- 23.10 - arxiv - [Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory](https://arxiv.org/abs/2310.17884)
  - **Benchmark**

#### ğŸ’»Presentations & TalksğŸ’»


#### ğŸ“–Tutorials & WorkshopsğŸ“–

- 23.10 - GitHub - [awesome-LLM-safety-paper-list](https://github.com/ydyjya/awesome-LLM-safety-paper-list)
  - **Repo**&**Tutorials**

#### ğŸ“°News & ArticlesğŸ“°


---
## Misinformation & Hallucination

#### ğŸ“‘PapersğŸ“‘
- 21.09 - ACL2022 - [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)
  - **Hallucination**
- 23.10 - arxiv - [Lost in Translation -- Multilingual Misinformation and its Evolution](https://arxiv.org/abs/2310.18089)
  - **Misinformation**
- 23.10 - arxiv - [Personas as a Way to Model Truthfulness in Language Models](https://arxiv.org/abs/2310.18168)
  - **Truthfulness**

#### ğŸ’»Presentations & TalksğŸ’»


#### ğŸ“–Tutorials & WorkshopsğŸ“–

- 23.10 - GitHub - [awesome-LLM-safety-paper-list](https://github.com/ydyjya/awesome-LLM-safety-paper-list)
  - **Repo**&**Tutorials**

#### ğŸ“°News & ArticlesğŸ“°


---
## Adversarial Attacks

#### ğŸ“‘PapersğŸ“‘

- 23.06 - arixiv - [Are aligned neural networks adversarially aligned?](https://arxiv.org/abs/2306.15447)
  - **Multimodal**&**Jailbreak**
- 23.07 - arxiv - [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)
  - **Jailbreak**

#### ğŸ’»Presentations & TalksğŸ’»


#### ğŸ“–Tutorials & WorkshopsğŸ“–

- 23.10 - GitHub - [awesome-LLM-safety-paper-list](https://github.com/ydyjya/awesome-LLM-safety-paper-list)
  - **Repo**&**Tutorials**

#### ğŸ“°News & ArticlesğŸ“°


---
## Defenses

#### ğŸ“‘PapersğŸ“‘


#### ğŸ’»Presentations & TalksğŸ’»


#### ğŸ“–Tutorials & WorkshopsğŸ“–

- 23.10 - GitHub - [awesome-LLM-safety-paper-list](https://github.com/ydyjya/awesome-LLM-safety-paper-list)
  - **Repo**&**Tutorials**

#### ğŸ“°News & ArticlesğŸ“°

---
## Dataset & Benchmark

#### ğŸ“‘PapersğŸ“‘
- 20.09 - EMNLP2020(findings) - [RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/abs/2009.11462)
  - **Toxicity**
- 21.09 - ACL2022 - [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)
  - **Hallucination**
- 22.03 - ACL2022 - [ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/abs/2203.09509)
  - **Toxicity**
#### ğŸ“šResourceğŸ“š
- 20.09 - URL - [RealToxicityPrompts Dataset](https://toxicdegeneration.allenai.org/)
  - **Toxicity**
- 21.09 - URL - [TruthfulQA Dataset](https://github.com/sylinrl/TruthfulQA)
  - **Hallucination**

---
## Author


**ğŸ¤—If you have any questions, please contact our authors!ğŸ¤—**

âœ‰ï¸: [ydyjya](https://github.com/ydyjya) â¡ï¸ zhouzhenhong@bupt.edu.cn
