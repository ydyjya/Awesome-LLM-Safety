# ğŸ›¡ï¸Awesome LLM-SafetyğŸ›¡ï¸[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

<p align="center">
<a href=""> <img src="https://img.shields.io/github/stars/ydyjya/Awesome-LLM-Safety?style=flat-square&logo=github" alt="GitHub stars"></a>
<a href=""> <img src="https://img.shields.io/github/forks/ydyjya/Awesome-LLM-Safety?style=flat-square&logo=github" alt="GitHub forks"></a>
<a href=""> <img src="https://img.shields.io/github/issues/ydyjya/Awesome-LLM-Safety?style=flat-square&logo=github" alt="GitHub issues"></a>
<a href=""> <img src="https://img.shields.io/github/last-commit/ydyjya/Awesome-LLM-Safety?style=flat-square&logo=github" alt="GitHub Last commit"></a>
</p>
<div align="center">

English | [ä¸­æ–‡](README_cn.md)

</div>

## ğŸ¤—Introduction


**Welcome to our Awesome-llm-safety repository!** ğŸ¥°ğŸ¥°ğŸ¥°

**ğŸ”¥ News**

- 2024.05 update NAACL 2024 Papers Collection, thanks @[zhrli324](https://github.com/zhrli324), @[feqHe](https://github.com/feqHe)!

**ğŸ§‘â€ğŸ’» Our Work**

We've curated a collection of the latest ğŸ˜‹, most comprehensive ğŸ˜, and most valuable ğŸ¤© resources on large language model safety (llm-safety). 
But we don't stop there; included are also relevant talks, tutorials, conferences, news, and articles. 
Our repository is constantly updated to ensure you have the most current information at your fingertips.

> If a resource is relevant to multiple subcategories, we place it under each applicable section. For instance, the "Awesome-LLM-Safety" repository will be listed under each subcategory to which it pertainsğŸ¤©!.

**âœ”ï¸ Perfect for Majority**
- For beginners curious about llm-safety, our repository serves as a compass for grasping the big picture and diving into the details. 
Classic or influential papers retained in the README provide a beginner-friendly navigation through interesting directions in the field;
- For seasoned researchers, this repository is a tool to keep you informed and fill any gaps in your knowledge. 
Within each subtopic, we are diligently updating all the latest content and continuously backfilling with previous work. 
Our thorough compilation and careful selection are time-savers for you.

**ğŸ§­ How to Use this Guide**
- Quick Start: In the README, users can find a curated list of select information sorted by date, along with links to various consultations.
- In-Depth Exploration: If you have a special interest in a particular subtopic, delve into the "subtopic" folder for more. 
Each item, be it an article or piece of news, comes with a brief introduction, allowing researchers to swiftly zero in on relevant content.

**ğŸ’¼ How to Contribution**

If you have completed an insightful work or carefully compiled conference papers, we would love to add your work to the repository.
- For **individual papers**, you can raise an issue, and we will quickly add your paper under the corresponding subtopic.
- If you have **compiled a collection of papers for a conference**, you are welcome to submit a pull request directly. 
We would greatly appreciate your contribution. 
Please note that these pull requests need to be consistent with our existing format.

**ğŸ“œAdvertisement**

ğŸŒ± If you would like more people to read your recent insightful work, please contact me via [email](zhouzhenhong@bupt.edu.cn). 
I can offer you a promotional spot here for up to one month.

**Letâ€™s start LLM Safety tutorial!**

---



## ğŸš€Table of Contents

- [ğŸ›¡ï¸Awesome LLM-SafetyğŸ›¡ï¸](#ï¸awesome-llm-safetyï¸)
  - [ğŸ¤—Introduction](#introduction)
  - [ğŸš€Table of Contents](#table-of-contents)
  - [ğŸ”Security & Discussion](#security & discussion)
    - [ğŸ“‘Papers](#papers)
    - [ğŸ“–Tutorials, Articles, Presentations and Talks](#tutorials-articles-presentations-and-talks)
    - [Other](#other)
  - [ğŸ”Privacy](#privacy)
    - [ğŸ“‘Papers](#papers-1)
    - [ğŸ“–Tutorials, Articles, Presentations and Talks](#tutorials-articles-presentations-and-talks-1)
    - [Other](#other-1)
  - [ğŸ“°Truthfulness \& Misinformation](#truthfulness--misinformation)
    - [ğŸ“‘Papers](#papers-2)
    - [ğŸ“–Tutorials, Articles, Presentations and Talks](#tutorials-articles-presentations-and-talks-2)
    - [Other](#other-2)
  - [ğŸ˜ˆJailBreak \& Attacks](#jailbreak--attacks)
    - [ğŸ“‘Papers](#papers-3)
    - [ğŸ“–Tutorials, Articles, Presentations and Talks](#tutorials-articles-presentations-and-talks-3)
    - [Other](#other-3)
  - [ğŸ›¡ï¸Defenses & Mitigation](#ï¸defenses & mitigation)
    - [ğŸ“–Tutorials, Articles, Presentations and Talks](#tutorials-articles-presentations-and-talks-4)
    - [Other](#other-4)
  - [ğŸ’¯Datasets \& Benchmark](#datasets--benchmark)
    - [ğŸ“‘Papers](#papers-4)
    - [ğŸ“–Tutorials, Articles, Presentations and Talks](#tutorials-articles-presentations-and-talks-5)
    - [ğŸ“šResourceğŸ“š](#resource)
    - [Other](#other-5)
  - [ğŸ§‘â€ğŸ« Scholars ğŸ‘©â€ğŸ«](#-scholars-)
  - [ğŸ§‘â€ğŸ“Author](#author)



---

## ğŸ¤”AI Safety & Security Discussions
|   Date    |                                                   Link                                                   |                                                                                                                                                                                       Publication                                                                                                                                                                                        | Authors |
|:---------:|:--------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------:|
| 2024/5/20 | [Managing extreme AI risks amid rapid progress](https://www.science.org/doi/abs/10.1126/science.adn0117) | Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Trevor Darrell, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian Hadfield, Jeff Clune, Tegan Maharaj, Frank Hutter, AtÄ±lÄ±m GÃ¼neÅŸ Baydin, Sheila McIlraith, Qiqi Gao, Ashwin Acharya, David Krueger, Anca Dragan, Philip Torr, Stuart Russell, Daniel Kahneman, Jan Brauner, SÃ¶ren Mindermann | Science |

---

## ğŸ”Security & Discussion

### ğŸ“‘Papers
| Date  |      Institute       | Publication |                                                                                            Paper                                                                                            |
|:-----:|:--------------------:|:-----------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
| 20.10 | Facebook AI Research |    arxiv    |                                                       [Recipes for Safety in Open-domain Chatbots](https://arxiv.org/abs/2010.07079)                                                        |
| 22.03 |        OpenAI        |  NIPS2022   | [Training language models to follow instructions with human feedback](https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html) |
| 23.07 |     UC Berkeley      |  NIPS2023   |                                                     [Jailbroken: How Does LLM Safety Training Fail?](https://arxiv.org/abs/2307.02483)                                                      |
| 23.12 |        OpenAI        |   Open AI   |                                 [Practices for Governing Agentic AI Systems](https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf)                                  |

### ğŸ“–Tutorials, Articles, Presentations and Talks

| Date  |          Type          |        Title         |                                          URL                                          |
|:-----:|:----------------------:|:--------------------:|:-------------------------------------------------------------------------------------:|
| 22.02 | Toxicity Detection API |   Perspective API    | [link](https://www.perspectiveapi.com/)<br/>[paper](https://arxiv.org/abs/2202.11176) |
| 23.07 |       Repository       | Awesome LLM Security |               [link](https://github.com/corca-ai/awesome-llm-security)                |
| 23.10 |       Tutorials        |  Awesome-LLM-Safety  |                 [link](https://github.com/ydyjya/Awesome-LLM-Safety)                  |
| 24.01 |       Tutorials        |    Awesome-LM-SSP    |                  [link](https://github.com/ThuCCSLab/Awesome-LM-SSP)                  |
| 24.XX |       Resource         | OWASP AI Exchange    |                  [link](https://owaspai.org/)                                         |

### Other

ğŸ‘‰[Latest&Comprehensive Security Paper](.//subtopic/Security&Discussion.md)

---
## ğŸ”Privacy 


### ğŸ“‘Papers
| Date  |    Institute    | Publication |                                                            Paper                                                             |
|:-----:|:---------------:|:-----------:|:----------------------------------------------------------------------------------------------------------------------------:|
| 19.12 |    Microsoft    |   CCS2020   |  [Analyzing Information Leakage of Updates to Natural Language Models](https://dl.acm.org/doi/abs/10.1145/3372297.3417880)   |
| 21.07 | Google Research |   ACL2022   |           [Deduplicating Training Data Makes Language Models Better](https://aclanthology.org/2022.acl-long.577/)            |
| 21.10 |    Stanford     |  ICLR2022   |      [Large language models can be strong differentially private learners](https://openreview.net/forum?id=bVuP3ltATMz)      |
| 22.02 | Google Research |  ICLR2023   |             [Quantifying Memorization Across Neural Language Models](https://openreview.net/forum?id=TatRHT_1cK)             |
| 22.02 | UNC Chapel Hill |  ICML2022   | [Deduplicating Training Data Mitigates Privacy Risks in Language Models](https://proceedings.mlr.press/v162/kandpal22a.html) |

### ğŸ“–Tutorials, Articles, Presentations and Talks

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |
| 24.01 | Tutorials |   Awesome-LM-SSP   | [link](https://github.com/ThuCCSLab/Awesome-LM-SSP)  |

### Other

ğŸ‘‰[Latest&Comprehensive Privacy Paper](.//subtopic/Privacy.md)

---
## ğŸ“°Truthfulness & Misinformation 


### ğŸ“‘Papers
| Date  |           Institute            | Publication |                                                                    Paper                                                                     |
|:-----:|:------------------------------:|:-----------:|:--------------------------------------------------------------------------------------------------------------------------------------------:|
| 21.09 |      University of Oxford      |   ACL2022   |                         [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)                          |
| 23.11 | Harbin Institute of Technology |    arxiv    | [A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/abs/2311.05232) |
| 23.11 |    Arizona State University    |    arxiv    |                      [Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey](https://arxiv.org/abs/2311.07914)                      |

### ğŸ“–Tutorials, Articles, Presentations and Talks

| Date  |    Type    |          Title           |                                URL                                |
|:-----:|:----------:|:------------------------:|:-----------------------------------------------------------------:|
| 23.07 | Repository | llm-hallucination-survey | [link](https://github.com/HillZhang1999/llm-hallucination-survey) |
| 23.10 | Repository |  LLM-Factuality-Survey   |   [link](https://github.com/wangcunxiang/LLM-Factuality-Survey)   |
| 23.10 | Tutorials  |    Awesome-LLM-Safety    |       [link](https://github.com/ydyjya/Awesome-LLM-Safety)        |

### Other

ğŸ‘‰[Latest&Comprehensive Truthfulness&Misinformation Paper](./subtopic/Truthfulness&Misinformation.md)

---
## ğŸ˜ˆJailBreak & Attacks 

### ğŸ“‘Papers
| Date  |            Institute            |         Publication          |                                                                   Paper                                                                   |
|:-----:|:-------------------------------:|:----------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------:|
| 20.12 |             Google              |     USENIX Security 2021     | [Extracting Training Data from Large Language Models](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting) |
| 22.11 |            AE Studio            | NIPS2022(ML Safety Workshop) |                     [Ignore Previous Prompt: Attack Techniques For Language Models](https://arxiv.org/abs/2211.09527)                     |
| 23.06 |             Google              |            arxiv             |                          [Are aligned neural networks adversarially aligned?](https://arxiv.org/abs/2306.15447)                           |
| 23.07 |               CMU               |            arxiv             |               [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)               |
| 23.10 |   University of Pennsylvania    |            arxiv             |                    [Jailbreaking Black Box Large Language Models in Twenty Queries](https://arxiv.org/abs/2310.08419)                     |

### ğŸ“–Tutorials, Articles, Presentations and Talks

| Date  |        Type        |                                       Title                                       |                                    URL                                     |
|:-----:|:------------------:|:---------------------------------------------------------------------------------:|:--------------------------------------------------------------------------:|
| 23.01 |     Community      |                              Reddit/ChatGPTJailbrek                               |             [link](https://www.reddit.com/r/ChatGPTJailbreak)              |
| 23.02 | Resource&Tutorials |                              Latest Jailbreak Prompts                            |  [link](https://repello.ai/blog/latest-claude-3-5-chatgpt-jailbreak-prompts-2024) |
| 23.10 |     Tutorials      |                                Awesome-LLM-Safety                                 |            [link](https://github.com/ydyjya/Awesome-LLM-Safety)            |
| 23.10 |      Article       |                 Adversarial Attacks on LLMs(Author: Lilian Weng)                  |   [link](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/)    |
| 23.11 |       Video        | [1hr Talk] Intro to Large Language Models<br/>From 45:45(Author: Andrej Karpathy) |            [link](https://www.youtube.com/watch?v=zjkBMFhNj_g)             |
| 24.09 |        Repo        |                      awesome_LLM-harmful-fine-tuning-papers                       | [link](https://github.com/git-disl/awesome_LLM-harmful-fine-tuning-papers) |
| 12.10 |        Resource    |                      Jailbreak Commuinities                                       | [link](https://repello.ai/blog/top-11-ai-jailbreak-communities-to-explore) |
| 12.10 |        Article    |                      Jailbreak Techniques and Safeguards                           | [link](https://repello.ai/blog/understanding-ai-jailbreaking-techniques-and-safeguards-against-prompt-exploits) |

### Other

ğŸ‘‰[Latest&Comprehensive JailBreak & Attacks Paper](./subtopic/Jailbreaks&Attack.md)

---
## ğŸ›¡ï¸Defenses & Mitigation

### ğŸ“‘Papers
| Date  |    Institute    | Publication |                                                             Paper                                                             |
|:-----:|:---------------:|:-----------:|:-----------------------------------------------------------------------------------------------------------------------------:|
| 21.07 | Google Research |   ACL2022   |            [Deduplicating Training Data Makes Language Models Better](https://aclanthology.org/2022.acl-long.577/)            |
| 22.04 |    Anthropic    |    arxiv    | [Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2204.05862) |



### ğŸ“–Tutorials, Articles, Presentations and Talks

| Date  |    Type    |         Title         |                              URL                              |
|:-----:|:----------:|:---------------------:|:-------------------------------------------------------------:|
| 23.10 | Tutorials  |  Awesome-LLM-Safety   |     [link](https://github.com/ydyjya/Awesome-LLM-Safety)      |

### Other

ğŸ‘‰[Latest&Comprehensive Defenses Paper](./subtopic/Defense&Mitigation)


--- 
## ğŸ’¯Datasets & Benchmark

### ğŸ“‘Papers
| Date  |        Institute         |     Publication     |                                                                  Paper                                                                   |
|:-----:|:------------------------:|:-------------------:|:----------------------------------------------------------------------------------------------------------------------------------------:|
| 20.09 | University of Washington | EMNLP2020(findings) |             [RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/abs/2009.11462)             |
| 21.09 |   University of Oxford   |       ACL2022       |                       [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)                        |
| 22.03 |           MIT            |       ACL2022       | [ToxiGen: A Large-Scale Machine-Generated datasets for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/abs/2203.09509) |


### ğŸ“–Tutorials, Articles, Presentations and Talks

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

### ğŸ“šResourceğŸ“š
- Toxicity - [RealToxicityPrompts datasets](https://toxicdegeneration.allenai.org/)
- Truthfulness - [TruthfulQA datasets](https://github.com/sylinrl/TruthfulQA)

### Other
ğŸ‘‰[Latest&Comprehensive datasets & Benchmark Paper](./subtopic/Datasets&Benchmark.md)

---
## ğŸ§‘â€ğŸ“Author


**ğŸ¤—If you have any questions, please contact our authors!ğŸ¤—**

âœ‰ï¸: [ydyjya](https://github.com/ydyjya) â¡ï¸ zhouzhenhong@bupt.edu.cn

ğŸ’¬: **LLM Safety Discussion**

<div align="center">

[Wechat Group](./resource/wechat.png) | [My Wechat](./resource/wechat.png)

</div>


---

[![Star History Chart](https://api.star-history.com/svg?repos=ydyjya/Awesome-LLM-Safety&type=Date)](https://star-history.com/#ydyjya/Awesome-LLM-Safety&Date)

**[â¬† Back to ToC](#table-of-contents)**
