# Datasets & Benchmark


## ðŸ“‘Papers

| Date  |                                                                                                      Institute                                                                                                       |       Publication       |                                                                                Paper                                                                                 |                                     Keywords                                      |
|:-----:|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------:|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------:|
| 20.09 |                                                                                               University of Washington                                                                                               |   EMNLP2020(findings)   |                           [RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/abs/2009.11462)                           |                                   **Toxicity**                                    |
| 21.09 |                                                                                                 University of Oxford                                                                                                 |         ACL2022         |                                     [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)                                      |                                 **Truthfulness**                                  |
| 22.03 |                                                                                                         MIT                                                                                                          |         ACL2022         |               [ToxiGen: A Large-Scale Machine-Generated datasets for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/abs/2203.09509)               |                                   **Toxicity**                                    |
| 23.07 |                                                                            Zhejiang University; School of Engineering Westlake University                                                                            |          arxiv          |             [Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models](https://arxiv.org/abs/2307.08487)              |                  **Text Safety**&**Benchmark**&**Jailbreaking**                   |
| 23.09 |                                                                                     LibrAI, MBZUAI, The University of Melbourne                                                                                      |          arxiv          |                                    [Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs](https://arxiv.org/abs/2308.13387)                                    |                       **Safety Evaluation**&**Safeguards**                        |
| 23.11 |                                                                                                   Fudan University                                                                                                   |          arxiv          |                                   [JADE: A Linguistic-based Safety Evaluation Platform for LLM](https://arxiv.org/abs/2311.00286)                                    |                               **Safety Benchmarks**                               |
| 23.11 |                                                                                                   UNC-Chapel Hill                                                                                                    |          arxiv          |                      [Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges](https://arxiv.org/abs/2311.03287)                       |                  **Hallucination**&**Benchmark**&**Multimodal**                   |
| 23.11 |                                                                                                   IBM Research AI                                                                                                    | EMNLP2023(GEM workshop) |                                    [Unveiling Safety Vulnerabilities of Large Language Models](https://arxiv.org/abs/2311.04124)                                     |       **Adversarial Examples**&**Clustering**&**Automatically Identifying**       |
| 23.11 |                                                                                  The Hong Kong University of Science and Technology                                                                                  |          arxiv          |                             [P-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models](https://arxiv.org/abs/2311.04044)                              |                  **Differential Privacy**&**Privacy Evaluation**                  |
| 23.11 |                                                                                                     UC Berkeley                                                                                                      |          arxiv          |                                                   [CAN LLMS FOLLOW SIMPLE RULES](https://arxiv.org/abs/2311.04235)                                                   |                       **Evaluation**&**Attack Strategies**                        |
| 23.11 |                                                                                            University of Central Florida                                                                                             |          arxiv          |                                 [THOS: A Benchmark Dataset for Targeted Hate and Offensive Speech](https://arxiv.org/abs/2311.06446)                                 |                 **Hate Speech**&**Offensive Speech**&**Dataset**                  |
| 23.11 |                                                                       Beijing Jiaotong University; DAMO Academy, Alibaba Group, Peng Cheng Lab                                                                       |          arXiv          |                        [AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation](https://arxiv.org/abs/2311.07397)                         |             Multi-modal Large Language Models&Hallucination&Benchmark             |
| 23.11 |                                                                                Patronus AI, University of Oxford, Bocconi University                                                                                 |          arxiv          |                  [SIMPLESAFETYTESTS: a Test Suite for Identifying Critical Safety Risks in Large Language Models](https://arxiv.org/abs/2311.08370)                  |                  **Safety Risks**&**Test Suite**&**Evaluation**                   |
| 23.11 |                                                            University of Southern California, University of Pennsylvania, University of California Davis                                                             |          arxiv          |                  [Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?](https://arxiv.org/abs/2311.09702)                  |            **Hallucinations**&**Semantic Associations**&**Benchmark**             |
| 23.11 |                                                          Seoul National University, Chung-Ang University, NAVER AI Lab, NAVER Cloud, University of Richmond                                                          |          arxiv          |                                       [LifeTox: Unveiling Implicit Toxicity in Life Advice](https://arxiv.org/abs/2311.09585)                                        |       **LifeTox Dataset**&**Toxicity Detection**&**Social Media Analysis**        |
| 23.11 |                                                                                   School of Information Renmin University of China                                                                                   |          arxiv          |              [UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation](https://arxiv.org/abs/2311.15296)               |                    **Hallucination**&**Evaluation Benchmark**                     |
| 23.11 |                                                                                            UC Santa Cruz, UNC-Chapel Hill                                                                                            |          arxiv          |                            [How Many Are in This Image? A Safety Evaluation Benchmark for Vision LLMs](https://arxiv.org/abs/2311.16101)                             | **Vision Large Language Models**&**Safety Evaluation**&A**dversarial Robustness** |
| 23.11 |                                  Institute of Information Engineering, Chinese Academy of Sciences; School of Cyber Security, University of Chinese Academy of Sciences; Baidu Inc.                                  |          arxiv          |                   [FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality Fairness Toxicity](https://arxiv.org/abs/2311.18580)                   |                            **Harmlessness Evaluation**                            |
| 23.12 |                                                                                                         Meta                                                                                                         |          arxiv          |                             [Purple Llama CYBERSECEVAL: A Secure Coding Benchmark for Language Models](https://arxiv.org/abs/2312.04724)                             |             **Safety**&**Cybersecurity**&**Code Security Benchmark**              |
| 23.12 |                                           University of Illinois Chicago, Bosch Research North America & Bosch Center for Artificial Intelligence (BCAI), UNC Chapel-Hill                                            |          arxiv          |                            [DELUCIONQA: Detecting Hallucinations in Domain-specific Question Answering](https://arxiv.org/abs/2312.05200)                            |  **Hallucination Detection**&**Domain-specific QA**&**Retrieval-augmented LLMs**  |
| 23.12 |                                                       University of Science and Technology of China, Hong Kong University of Science and Technology, Microsoft                                                       |          arxiv          |                  [Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models](https://arxiv.org/abs/2312.14197)                   |       **Indirect Prompt Injection Attacks**&**BIPIA Benchmark**&**Defense**       |
| 24.01 |                                                                                  NewsBreak, University of Illinois Urbana-Champaign                                                                                  |          arxiv          |                 [RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models](https://arxiv.org/abs/2401.00396)                  |    **Retrieval-Augmented Generation**&**Hallucination Detection**&**Dataset**     |
| 24.01 | Lehigh University, Illinois Institute of Technology, Institut Polytechnique de Paris, William & Mary, Texas A&M University, University of Georgia, Samsung Research America, Stanford University(Major contribution) |          arxiv          |                                        [TRUSTLLM: TRUSTWORTHINESS IN LARGE LANGUAGE MODELS](https://arxiv.org/abs/2401.05561)                                        |                   **Trustworthiness**&**Benchmark Evaluation**                    |
| 24.01 |                                                                                              University College London                                                                                               |          arxiv          |                                   [Hallucination Benchmark in Medical Visual Question Answering](https://arxiv.org/abs/2401.05827)                                   |         **Medical Visual Question Answering**&**Hallucination Benchmark**         |
| 24.01 |                                                                                              Carnegie Mellon University                                                                                              |          arxiv          |                                          [TOFU: A Task of Fictitious Unlearning for LLMs](https://arxiv.org/abs/2401.06121)                                          |               **Data Privacy**&**Ethical Concerns**&**Unlearning**                |
| 24.01 |                                                                                 IRLab CITIC Research Centre, Universidade da CoruÃ±a                                                                                  |          arxiv          |                                [MetaHate: A Dataset for Unifying Efforts on Hate Speech Detection](https://arxiv.org/abs/2401.06526)                                 |                    **Hate Speech Detection**&**Social Media**                     |
| 24.01 |                                                              Northwestern University, New York University, University of Liverpool, Rutgers University                                                               |          arxiv          |                  [AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models](https://arxiv.org/abs/2401.09002)                   |      **Jailbreak Attack**&**Evaluation Frameworks**&**Ground Truth Dataset**      |
| 24.01 |                                                                                            Shanghai Jiao Tong University                                                                                             |          arxiv          |                                    [R-Judge: Benchmarking Safety Risk Awareness for LLM Agents](https://arxiv.org/abs/2401.10019)                                    |              **LLM Agents**&**Safety Risk Awareness**&**Benchmark**               |
| 24.02 |                                                  University of Illinois Urbana-Champaign, Center for AI Safety, Carnegie Mellon University, UC Berkeley, Microsoft                                                   |          arxiv          |                   [HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal](https://arxiv.org/abs/2402.04249)                    |                   **Automated Red Teaming**&**Robust Refusal**                    |
| 24.02 |                                    Shanghai Artificial Intelligence Laboratory, Harbin Institute of Technology, Beijing Institute of Technology, Chinese University of Hong Kong                                     |          arxiv          |                     [SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models](https://arxiv.org/abs/2402.05044)                     |        **Safety Benchmark**&Safety Evaluation**&**Hierarchical Taxonomy**         |
| 24.02 |                                                                                           Middle East Technical University                                                                                           |          arxiv          |                      [HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs](https://arxiv.org/abs/2402.16211)                      |                    **Hallucination**&**Benchmarking Dataset**                     |
| 24.02 |                                                                                       Indian Institute of Technology Kharagpur                                                                                       |          arxiv          | [How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries](https://arxiv.org/abs/2402.15302) |           **Instruction-centric Responses**&**Ethical Vulnerabilities**           |
| 24.03 |                                                                                             East China Normal University                                                                                             |          arxiv          |                      [DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2403.00896)                      |  **Dialogue-level Hallucination**&**Benchmarking**&**Human-machine Interaction**  |
| 24.03 |                                               Tianjin University, Tianjin University, Zhengzhou University, China Academy of Information and Communications Technology                                               |          arxiv          |                           [OpenEval: Benchmarking Chinese LLMs across Capability, Alignment, and Safety](https://arxiv.org/abs/2403.12316)                           |                   **Chinese LLMs**&**Benchmarking**&**Safety**                    |
| 24.04 |                                                                                University of Pennsylvania, ETH Zurich, EPFL, Sony AI                                                                                 |          arxiv          |                       [JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models](https://arxiv.org/abs/2404.01318)                        |                 **Jailbreaking Attacks**&**Robustness Benchmark**                 |
| 24.04 |                                                                         Vector Institute for Artificial Intelligence, University of Limerick                                                                         |          arxiv          |                        [Developing Safe and Responsible Large Language Models - A Comprehensive Framework](https://arxiv.org/abs/2404.01399)                         |                **Responsible AI**&**AI Safety**&**Generative AI**                 |
| 24.04 |                                                      LMU Munich, University of Oxford, Siemens AG, Munich Center for Machine Learning (MCML), Wuhan University                                                       |          arxiv          |                          [RED TEAMING GPT-4V: ARE GPT-4V SAFE AGAINST UNI/MULTI-MODAL JAILBREAK ATTACKS?](https://arxiv.org/abs/2404.03411)                          |     **Jailbreak Attacks**&**GPT-4V**&**Evaluation Benchmark**&**Robustness**      |
| 24.04 |                                                                                       Bocconi University, University of Oxford                                                                                       |          arxiv          |           [SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and Improving Large Language Model Safety](https://arxiv.org/abs/2404.05399)           |              **LLM Safety**&**Open Datasets**&**Systematic Review**               |
| 24.04 |                                                                                    University of Alberta&The University of Tokyo                                                                                     |          arxiv          |                         [Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward](https://arxiv.org/abs/2404.08517)                          |              **LLM Safety**&**Online Safety Analysis**&**Benchmark**              |


## ðŸ“šResource

- Toxicity - [RealToxicityPrompts datasets](https://toxicdegeneration.allenai.org/)
- Truthfulness - [TruthfulQA datasets](https://github.com/sylinrl/TruthfulQA)
- TRUSTLLM - [TRUSTLLM](https://trustllmbenchmark.github.io/TrustLLM-Website/)