# Datasets & Benchmark


## ðŸ“‘Papers

| Date  |                                                                                              Institute                                                                                              |           Publication           |                                                                                Paper                                                                                 |                                           Keywords                                           |
|:-----:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------------------:|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------------------:|
| 20.09 |                                                                                      University of Washington                                                                                       |       EMNLP2020(findings)       |                           [RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/abs/2009.11462)                           |                                         **Toxicity**                                         |
| 21.09 |                                                                                        University of Oxford                                                                                         |             ACL2022             |                                     [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)                                      |                                       **Truthfulness**                                       |
| 22.03 |                                                                                                 MIT                                                                                                 |             ACL2022             |               [ToxiGen: A Large-Scale Machine-Generated datasets for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/abs/2203.09509)               |                                         **Toxicity**                                         |
| 23.07 |                                                                   Zhejiang University; School of Engineering Westlake University                                                                    |              arxiv              |             [Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models](https://arxiv.org/abs/2307.08487)              |                        **Text Safety**&**Benchmark**&**Jailbreaking**                        |
| 23.07 |                                                                                   Stevens Institute of Technology                                                                                   |       NAACL2024(findings)       |                         [HateModerate: Testing Hate Speech Detectors against Content Moderation Policies](https://arxiv.org/abs/2307.12418)                          |            **Hate Speech Detection**&**Content Moderation**&**Machine Learning**             |
| 23.08 |                                                                                          Meta Reality Labs                                                                                          |            NAACL2024            |                                [Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)?](https://arxiv.org/abs/2308.10168)                                 |            **Large Language Models**&**Knowledge Graphs**&**Question Answering**             |
| 23.08 |                                                                                         Bocconi University                                                                                          |            NAACL2024            |                   [XSTEST: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models](https://arxiv.org/abs/2308.01263)                    |                **Large Language Models**&**Safety Behaviours**&**Test Suite**                |
| 23.09 |                                                                             LibrAI, MBZUAI, The University of Melbourne                                                                             |              arxiv              |                                    [Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs](https://arxiv.org/abs/2308.13387)                                    |                             **Safety Evaluation**&**Safeguards**                             |
| 23.10 |                                                                       University of Edinburgh, Huawei Technologies Co., Ltd.                                                                        |            NAACL2024            |                                   [Assessing the Reliability of Large Language Model Knowledge](https://arxiv.org/abs/2310.09820)                                    |            **Large Language Models**&**Factual Knowledge**&**Knowledge Probing**             |
| 23.10 |                                                                                     University of Pennsylvania                                                                                      |       NAACL2024(findings)       |            [Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks](https://arxiv.org/abs/2310.12516)             |        **Hallucination Assessment**&**Adversarial Attacks**&**Large Language Models**        |
| 23.11 |                                                                                          Fudan University                                                                                           |              arxiv              |                                   [JADE: A Linguistic-based Safety Evaluation Platform for LLM](https://arxiv.org/abs/2311.00286)                                    |                                    **Safety Benchmarks**                                     |
| 23.11 |                                                                                           UNC-Chapel Hill                                                                                           |              arxiv              |                      [Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges](https://arxiv.org/abs/2311.03287)                       |                        **Hallucination**&**Benchmark**&**Multimodal**                        |
| 23.11 |                                                                                           IBM Research AI                                                                                           |     EMNLP2023(GEM workshop)     |                                    [Unveiling Safety Vulnerabilities of Large Language Models](https://arxiv.org/abs/2311.04124)                                     |            **Adversarial Examples**&**Clustering**&**Automatically Identifying**             |
| 23.11 |                                                                         The Hong Kong University of Science and Technology                                                                          |              arxiv              |                             [P-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models](https://arxiv.org/abs/2311.04044)                              |                       **Differential Privacy**&**Privacy Evaluation**                        |
| 23.11 |                                                                                             UC Berkeley                                                                                             |              arxiv              |                                                   [CAN LLMS FOLLOW SIMPLE RULES](https://arxiv.org/abs/2311.04235)                                                   |                             **Evaluation**&**Attack Strategies**                             |
| 23.11 |                                                                                    University of Central Florida                                                                                    |              arxiv              |                                 [THOS: A Benchmark Dataset for Targeted Hate and Offensive Speech](https://arxiv.org/abs/2311.06446)                                 |                       **Hate Speech**&**Offensive Speech**&**Dataset**                       |
| 23.11 |                                                              Beijing Jiaotong University; DAMO Academy, Alibaba Group, Peng Cheng Lab                                                               |              arXiv              |                        [AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation](https://arxiv.org/abs/2311.07397)                         |                  Multi-modal Large Language Models&Hallucination&Benchmark                   |
| 23.11 |                                                                        Patronus AI, University of Oxford, Bocconi University                                                                        |              arxiv              |                  [SIMPLESAFETYTESTS: a Test Suite for Identifying Critical Safety Risks in Large Language Models](https://arxiv.org/abs/2311.08370)                  |                        **Safety Risks**&**Test Suite**&**Evaluation**                        |
| 23.11 |                                                    University of Southern California, University of Pennsylvania, University of California Davis                                                    |              arxiv              |                  [Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?](https://arxiv.org/abs/2311.09702)                  |                  **Hallucinations**&**Semantic Associations**&**Benchmark**                  |
| 23.11 |                                                 Seoul National University, Chung-Ang University, NAVER AI Lab, NAVER Cloud, University of Richmond                                                  |              arxiv              |                                       [LifeTox: Unveiling Implicit Toxicity in Life Advice](https://arxiv.org/abs/2311.09585)                                        |             **LifeTox Dataset**&**Toxicity Detection**&**Social Media Analysis**             |
| 23.11 |                                                                          School of Information Renmin University of China                                                                           |              arxiv              |              [UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation](https://arxiv.org/abs/2311.15296)               |                          **Hallucination**&**Evaluation Benchmark**                          |
| 23.11 |                                                                                   UC Santa Cruz, UNC-Chapel Hill                                                                                    |              arxiv              |                            [How Many Are in This Image? A Safety Evaluation Benchmark for Vision LLMs](https://arxiv.org/abs/2311.16101)                             |      **Vision Large Language Models**&**Safety Evaluation**&A**dversarial Robustness**       |
| 23.11 |                         Institute of Information Engineering, Chinese Academy of Sciences; School of Cyber Security, University of Chinese Academy of Sciences; Baidu Inc.                          |              arxiv              |                   [FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality Fairness Toxicity](https://arxiv.org/abs/2311.18580)                   |                                 **Harmlessness Evaluation**                                  |
| 23.11 |                                                                    Fudan University&Shanghai Artificial Intelligence Laboratory                                                                     |            NAACL2024            |                                          [Fake Alignment: Are LLMs Really Aligned Well?](https://arxiv.org/abs/2311.05915)                                           |              **Large Language Models**&**Safety Evaluation**&**Fake Alignment**              |
| 23.11 |                                                                                     Kahlert School of Computing                                                                                     |            NAACL2024            |                                   [Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness](https://arxiv.org/abs/2311.09694)                                   |          **NLP Robustness**&**Out-of-Domain Evaluation**&**Adversarial Evaluation**          |
| 23.11 |                                                                                    Shanghai Jiao Tong University                                                                                    |       NAACL2024(findings)       |                                [CLEANâ€“EVAL: Clean Evaluation on Contaminated Large Language Models](https://arxiv.org/abs/2311.09154)                                |            **Clean Evaluation**&**Data Contamination**&**Large Language Models**             |
| 23.12 |                                                                                                Meta                                                                                                 |              arxiv              |                             [Purple Llama CYBERSECEVAL: A Secure Coding Benchmark for Language Models](https://arxiv.org/abs/2312.04724)                             |                   **Safety**&**Cybersecurity**&**Code Security Benchmark**                   |
| 23.12 |                                   University of Illinois Chicago, Bosch Research North America & Bosch Center for Artificial Intelligence (BCAI), UNC Chapel-Hill                                   |              arxiv              |                            [DELUCIONQA: Detecting Hallucinations in Domain-specific Question Answering](https://arxiv.org/abs/2312.05200)                            |       **Hallucination Detection**&**Domain-specific QA**&**Retrieval-augmented LLMs**        |
| 23.12 |                                              University of Science and Technology of China, Hong Kong University of Science and Technology, Microsoft                                               |              arxiv              |                  [Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models](https://arxiv.org/abs/2312.14197)                   |            **Indirect Prompt Injection Attacks**&**BIPIA Benchmark**&**Defense**             |
| 24.01 |                                                                         NewsBreak, University of Illinois Urbana-Champaign                                                                          |              arxiv              |                 [RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models](https://arxiv.org/abs/2401.00396)                  |          **Retrieval-Augmented Generation**&**Hallucination Detection**&**Dataset**          |
| 24.01 | University of Notre Dame, Lehigh University, Illinois Institute of Technology, Institut Polytechnique de Paris, William & Mary, Texas A&M University, Samsung Research America, Stanford University |            ICML 2024            |                                [TRUSTLLM: TRUSTWORTHINESS IN LARGE LANGUAGE MODELS](https://proceedings.mlr.press/v235/huang24x.html)                                |                         **Trustworthiness**&**Benchmark Evaluation**                         |
| 24.01 |                                                                                      University College London                                                                                      |              arxiv              |                                   [Hallucination Benchmark in Medical Visual Question Answering](https://arxiv.org/abs/2401.05827)                                   |              **Medical Visual Question Answering**&**Hallucination Benchmark**               |
| 24.01 |                                                                                     Carnegie Mellon University                                                                                      |              arxiv              |                                          [TOFU: A Task of Fictitious Unlearning for LLMs](https://arxiv.org/abs/2401.06121)                                          |                     **Data Privacy**&**Ethical Concerns**&**Unlearning**                     |
| 24.01 |                                                                         IRLab CITIC Research Centre, Universidade da CoruÃ±a                                                                         |              arxiv              |                                [MetaHate: A Dataset for Unifying Efforts on Hate Speech Detection](https://arxiv.org/abs/2401.06526)                                 |                          **Hate Speech Detection**&**Social Media**                          |
| 24.01 |                                                      Northwestern University, New York University, University of Liverpool, Rutgers University                                                      |              arxiv              |                  [AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models](https://arxiv.org/abs/2401.09002)                   |           **Jailbreak Attack**&**Evaluation Frameworks**&**Ground Truth Dataset**            |
| 24.01 |                                                                                    Shanghai Jiao Tong University                                                                                    |              arxiv              |                                    [R-Judge: Benchmarking Safety Risk Awareness for LLM Agents](https://arxiv.org/abs/2401.10019)                                    |                    **LLM Agents**&**Safety Risk Awareness**&**Benchmark**                    |
| 24.02 |                                          University of Illinois Urbana-Champaign, Center for AI Safety, Carnegie Mellon University, UC Berkeley, Microsoft                                          |              arxiv              |                   [HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal](https://arxiv.org/abs/2402.04249)                    |                         **Automated Red Teaming**&**Robust Refusal**                         |
| 24.02 |                            Shanghai Artificial Intelligence Laboratory, Harbin Institute of Technology, Beijing Institute of Technology, Chinese University of Hong Kong                            |              arxiv              |                     [SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models](https://arxiv.org/abs/2402.05044)                     |              **Safety Benchmark**&Safety Evaluation**&**Hierarchical Taxonomy**              |
| 24.02 |                                                                                  Middle East Technical University                                                                                   |              arxiv              |                      [HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs](https://arxiv.org/abs/2402.16211)                      |                          **Hallucination**&**Benchmarking Dataset**                          |
| 24.02 |                                                                              Indian Institute of Technology Kharagpur                                                                               |              arxiv              | [How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries](https://arxiv.org/abs/2402.15302) |                **Instruction-centric Responses**&**Ethical Vulnerabilities**                 |
| 24.03 |                                                                                    East China Normal University                                                                                     |              arxiv              |                      [DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2403.00896)                      |       **Dialogue-level Hallucination**&**Benchmarking**&**Human-machine Interaction**        |
| 24.03 |                                      Tianjin University, Tianjin University, Zhengzhou University, China Academy of Information and Communications Technology                                       |              arxiv              |                           [OpenEval: Benchmarking Chinese LLMs across Capability, Alignment, and Safety](https://arxiv.org/abs/2403.12316)                           |                         **Chinese LLMs**&**Benchmarking**&**Safety**                         |
| 24.04 |                                                                        University of Pennsylvania, ETH Zurich, EPFL, Sony AI                                                                        |              arxiv              |                       [JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models](https://arxiv.org/abs/2404.01318)                        |                      **Jailbreaking Attacks**&**Robustness Benchmark**                       |
| 24.04 |                                                                Vector Institute for Artificial Intelligence, University of Limerick                                                                 |              arxiv              |                        [Developing Safe and Responsible Large Language Models - A Comprehensive Framework](https://arxiv.org/abs/2404.01399)                         |                      **Responsible AI**&**AI Safety**&**Generative AI**                      |
| 24.04 |                                              LMU Munich, University of Oxford, Siemens AG, Munich Center for Machine Learning (MCML), Wuhan University                                              |              arxiv              |                          [RED TEAMING GPT-4V: ARE GPT-4V SAFE AGAINST UNI/MULTI-MODAL JAILBREAK ATTACKS?](https://arxiv.org/abs/2404.03411)                          |           **Jailbreak Attacks**&**GPT-4V**&**Evaluation Benchmark**&**Robustness**           |
| 24.04 |                                                                              Bocconi University, University of Oxford                                                                               |              arxiv              |           [SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and Improving Large Language Model Safety](https://arxiv.org/abs/2404.05399)           |                    **LLM Safety**&**Open Datasets**&**Systematic Review**                    |
| 24.04 |                                                                            University of Alberta&The University of Tokyo                                                                            |              arxiv              |                         [Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward](https://arxiv.org/abs/2404.08517)                          |                   **LLM Safety**&**Online Safety Analysis**&**Benchmark**                    |
| 24.04 |                                                                     Technion â€“ Israel Institute of Technology, Google Research                                                                      |              arxiv              |                          [Constructing Benchmarks and Interventions for Combating Hallucinations in LLMs](https://arxiv.org/abs/2404.09971)                          |                              **Hallucinations**&**Benchmarks**                               |
| 24.05 |                                                                                     Carnegie Mellon University                                                                                      |              arxiv              |              [PolygloToxicityPrompts: Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models](https://arxiv.org/abs/2405.09373)               |                           **Multilingual Evaluation**&**Datasets*                            |
| 24.05 |                                                                       Paul G. Allen School of Computer Science & Engineering                                                                        |              arxiv              |            [MASSIVE Multilingual Abstract Meaning Representation: A Dataset and Baselines for Hallucination Detection](https://arxiv.org/abs/2405.19285)             |                 **Hallucination Detection**&**Multilingual AMR**&**Dataset**                 |
| 24.05 |                                                                                 University of California, Riverside                                                                                 |              arxiv              |                                  [Cross-Task Defense: Instruction-Tuning LLMs for Content Safety](https://arxiv.org/abs/2405.15202)                                  |                   **Instruction-Tuning**&**LLM Safety**&**Content Safety**                   |
| 24.06 |                                                                                       University of Waterloo                                                                                        |              arxiv              |                                [TruthEval: A Dataset to Evaluate LLM Truthfulness and Reliability](https://arxiv.org/abs/2406.01855)                                 |                               **Truthfulness**&**Reliability**                               |
| 24.06 |                                                                                         Rutgers University                                                                                          |              arxiv              |                                               [MoralBench: Moral Evaluation of LLMs](https://arxiv.org/abs/2406.04428)                                               |                             **Moral Evaluation**&**MoralBench**                              |
| 24.06 |                                                                                         Tsinghua University                                                                                         |              arxiv              |                     [Benchmarking Trustworthiness of Multimodal Large Language Models: A Comprehensive Study](https://arxiv.org/abs/2406.07057)                      |                         **Trustworthiness**&**MLLMs**&**Benchmark**                          |
| 24.06 |                                                                             Beijing Academy of Artificial Intelligence                                                                              |              arxiv              |                     [HalluDial: A Large-Scale Benchmark for Automatic Dialogue-Level Hallucination Evaluation](https://arxiv.org/abs/2406.07070)                     |                **Hallucination Evaluation**&**Dialogue-Level**&**HalluDial**                 |
| 24.06 |                                                                                         Sichuan University                                                                                          |              arxiv              |                 [LEGEND: Leveraging Representation Engineering to Annotate Safety Margin for Preference Datasets](https://arxiv.org/abs/2406.08124)                  |           **Safety Margin**&**Preference Datasets**&**Representation Engineering**           |
| 24.06 |                                                                   The Hong Kong University of Science and Technology (Guangzhou)                                                                    |              arxiv              |                                     [Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs](https://arxiv.org/abs/2406.09324)                                     |                            **Jailbreak Attacks**&**Benchmarking**                            |
| 24.06 |                                                                                 AI Innovation Center, China Unicom                                                                                  |              arxiv              |                        [CHiSafetyBench: A Chinese Hierarchical Safety Benchmark for Large Language Models](https://arxiv.org/abs/2406.10311)                         | **Chinese Hierarchical Safety Benchmark**&**Large Language Models**&**Automatic Evaluation** |
| 24.06 |                                                                                               Google                                                                                                |              arxiv              |                    [Supporting Human Raters with the Detection of Harmful Content using Large Language Models](https://arxiv.org/abs/2406.12800)                     |                        **Harmful Content Detection**&**Hate Speech**                         |
| 24.06 |                                                  South China University of Technology, Pazhou Laboratory, University of Maryland, Baltimore County                                                  |              arxiv              |                      [GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large Language Models](https://arxiv.org/abs/2406.13925)                       |             **Gender Bias Mitigation**&**Alignment Dataset**&**Bias Categories**             |
| 24.06 |                                                              Center for AI Safety and Governance, Institute for AI, Peking University                                                               |              arxiv              |                    [SAFESORA: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset](https://arxiv.org/abs/2406.14477)                    |                        **Safety Alignment**&**Text2Video Generation**                        |
| 24.06 |                                                                                          Fudan University                                                                                           |              arxiv              |                                                 [Cross-Modality Safety Alignment](https://arxiv.org/abs/2406.15279)                                                  |          **Multimodal Safety**&**Large Vision-Language Models**&**SIUO Benchmark**           |
| 24.06 |                                                                                                KAIST                                                                                                |              arxiv              |                          [CSRT: Evaluation and Analysis of LLMs using Code-Switching Red-Teaming Dataset](https://arxiv.org/abs/2406.15481)                          |                    **Code-Switching**&**Red-Teaming**&**Multilingualism**                    |
| 24.06 |                                                                                      University College London                                                                                      |              arxiv              |                        [JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models](https://arxiv.org/abs/2406.15484)                         |                       **Gender Bias**&**Hiring Bias**&**Benchmarking**                       |
| 24.06 |                                                                                          Peking University                                                                                          |              arxiv              |                           [PKU-SafeRLHF: A Safety Alignment Preference Dataset for Llama Family Models](https://arxiv.org/abs/2406.15513)                            |                         **Safety Alignment**&**Preference Dataset**                          |
| 24.06 |                                                                                University of California, Los Angeles                                                                                |              arxiv              |                           [MOSSBench: Is Your Multimodal Language Model Oversensitive to Safe Queries?](https://arxiv.org/abs/2406.17806)                            |           **Multimodal Language Models**&**Oversensitivity**&**Safety Mechanisms**           |
| 24.06 |                                                                                       Allen Institute for AI                                                                                        |              arxiv              |                   [WILDGUARD: Open One-stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs](https://arxiv.org/abs/2406.18495)                   |               **Safety Moderation**&**Jailbreak Attacks**&**Moderation Tools**               |
| 24.06 |                                                                                      University of Washington                                                                                       |              arxiv              |                    [WILDTEAMING at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models](https://arxiv.org/abs/2406.18510)                    |                 **Jailbreaking**&**Safety Training**&**Adversarial Attacks**                 |
| 24.07 |                                                                                     Beijing Jiaotong University                                                                                     |              arxiv              |              [KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge Graph-based False Premise Questions](https://arxiv.org/abs/2407.05868)              |         **Factuality Hallucination**&**Knowledge Graph**&**False Premise Questions**         |
| 24.07 |                                                                                     Chinese Academy of Sciences                                                                                     |              arxiv              |                             [T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models](https://arxiv.org/abs/2407.05965)                             |           **Text-to-Video Generation**&**Safety Evaluation**&**Generative Models**           |
| 24.07 |                                                                                             Patronus AI                                                                                             |              arxiv              |                                       [Lynx: An Open Source Hallucination Evaluation Model](https://arxiv.org/abs/2407.08488)                                        |                   **Hallucination Detection**&**RAG**&**Evaluation Model**                   |
| 24.07 |                                                                                            Virginia Tech                                                                                            |              arxiv              |                    [AIR-BENCH 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies](https://arxiv.org/abs/2407.17436)                     |                **AI Safety**&**Regulations**&**Policies**&**Risk Categories**                |
| 24.07 |                                                                                         Columbia University                                                                                         |            ECCV 2024            |                           [HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning](https://arxiv.org/abs/2407.15680)                           |                  **Hallucination**&**Vision-Language Models**&**Datasets**                   |
| 24.07 |                                                                                        Center for AI Safety                                                                                         |              arxiv              |                             [Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?](https://arxiv.org/abs/2407.21792)                             |                                 **AI Safety**&**Benchmarks**                                 |
| 24.08 |                                                                                           Walled AI Labs                                                                                            |              arxiv              |                         [WALLEDEVAL: A Comprehensive Safety Evaluation Toolkit for Large Language Models](https://arxiv.org/abs/2408.03837)                          |                              **AI Safety**&**Prompt Injection**                              |
| 24.08 |                                                                                       ShanghaiTech University                                                                                       |              arxiv              |                  [MMJ-Bench: A Comprehensive Study on Jailbreak Attacks and Defenses for Vision Language Models](https://arxiv.org/abs/2408.08464)                   |                **Jailbreak Attacks**&**Vision-Language Models**&**Security**                 |
| 24.08 |                                                                                         Stanford University                                                                                         |              arxiv              |                    [Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risk of Language Models](https://arxiv.org/abs/2408.08926)                    |                            **Cybersecurity**&**Capture the Flag**                            |
| 24.08 |                                                                                         Zhejiang University                                                                                         |              arxiv              |                         [Characterizing and Evaluating the Reliability of LLMs against Jailbreak Attacks](https://arxiv.org/abs/2408.09326)                          |              **Jailbreak Attacks**&**LLM Reliability**&**Evaluation Framework**              |
| 24.08 |                                                                                             Enkrypt AI                                                                                              |              arxiv              |                        [SAGE-RT: Synthetic Alignment Data Generation for Safety Evaluation and Red Teaming](https://arxiv.org/abs/2408.11851)                        |             **Synthetic Data Generation**&**Safety Evaluation**&**Red Teaming**              |
| 24.08 |                                                                                         Tianjin University                                                                                          |      Findings of ACL 2024       |                            [CMoralEval: A Moral Evaluation Benchmark for Chinese Large Language Models](https://arxiv.org/abs/2408.09819)                            |                            **Moral Evaluation**&**Moral Dilemma**                            |
| 24.08 |                                                                                        University of Surrey                                                                                         |           IJCAI 2024            |                              [CodeMirage: Hallucinations in Code Generated by Large Language Models](https://arxiv.org/abs/2408.08333)                               |                        **Code Hallucinations**&**CodeMirage Dataset**                        |
| 24.08 |                                                                                  Chalmers University of Technology                                                                                  |              arxiv              |                                  [LLMSecCode: Evaluating Large Language Models for Secure Coding](https://arxiv.org/abs/2408.16100)                                  |                          **Secure Coding**&**Evaluation Framework**                          |
| 24.09 |                                                                                 The Chinese University of Hong Kong                                                                                 |              arxiv              |                     [Testing and Evaluation of Large Language Models: Correctness, Non-Toxicity, and Fairness](https://arxiv.org/abs/2409.00551)                     |                        **Correctness**&**Non-Toxicity**&**Fairness**                         |
| 24.09 |                                                                                                KAIST                                                                                                |              arxiv              |                        [Evaluating Image Hallucination in Text-to-Image Generation with Question-Answering](https://arxiv.org/abs/2409.12784)                        |         **Image Hallucination**&**Text-to-Image Generation**&**Question-Answering**          |
| 24.09 |                                                                                         Zhejiang University                                                                                         |              arxiv              |             [GenTel-Safe: A Unified Benchmark and Shielding Framework for Defending Against Prompt Injection Attacks](https://arxiv.org/abs/2409.19521)              |                     **Prompt Injection**&**LLM Safety**&**Benchmarking**                     |
| 24.10 |                                                                                         Zhejiang University                                                                                         |              arxiv              |                [AGENT SECURITY BENCH (ASB): FORMALIZING AND BENCHMARKING ATTACKS AND DEFENSES IN LLM-BASED AGENTS](https://arxiv.org/abs/2410.02644)                 |             **LLM-based Agents**&**Security Benchmarks**&**Adversarial Attacks**             |
| 24.10 |                                                                                Zhejiang University, Duke University                                                                                 |              arxiv              |             [SCISAFEEVAL: A Comprehensive Benchmark for Safety Alignment of Large Language Models in Scientific Tasks](https://arxiv.org/abs/2410.03769)             |                          **Safety Alignment**&**Scientific Tasks**                           |
| 24.10 |                                                                         The Chinese University of Hong Kong, Tencent AI Lab                                                                         |              arxiv              |                          [Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step](https://arxiv.org/abs/2410.03869)                          |                **Chain-of-Jailbreak**&**Image Generation Models**&**Safety**                 |
| 24.10 |                                                              University of California, Santa Cruz, University of California, Berkeley                                                               |              arxiv              |                               [Multimodal Situational Safety: A Benchmark for Large Language Models](https://arxiv.org/abs/2410.06172)                               |               **Multimodal Situational Safety**&**MLLMs**&**Safety Benchmark**               |
| 24.10 |                                                                                            IBM Research                                                                                             |              arxiv              |                      [ST-WebAgentBench: A Benchmark for Evaluating Safety and Trustworthiness in Web Agents](https://arxiv.org/abs/2410.06703)                       |                        **Web Agents**&**Safety**&**Trustworthiness**                         |
| 24.10 |                                                Renmin University of China, Anthropic, University of Oxford, University of Edinburgh, Mila, Tangentic                                                |              arxiv              |                           [POISONBENCH: Assessing Large Language Model Vulnerability to Data Poisoning](https://arxiv.org/abs/2410.08811)                            |               **Data poisoning**&**LLM vulnerability**&**Preference learning**               |
| 24.10 |                                                                                Gray Swan AI, UK AI Safety Institute                                                                                 |              arxiv              |                                  [AGENTHARM: A Benchmark for Measuring Harmfulness of LLM Agents](https://arxiv.org/abs/2410.09024)                                  |                   **Jailbreaking**&**LLM agents**&**Harmful agent tasks**                    |
| 24.10 |                                                                                          Purdue University                                                                                          |              arxiv              |                          [COLLU-BENCH: A Benchmark for Predicting Language Model Hallucinations in Code](https://arxiv.org/abs/2410.09997)                           |           **Code hallucinations**&**Code generation**&**Automated program repair**           |
| 24.10 |                                                The Hong Kong University of Science and Technology (Guangzhou), University of Birmingham, Baidu Inc.                                                 |              arxiv              |         [JAILJUDGE: A Comprehensive Jailbreak Judge Benchmark with Multi-Agent Enhanced Explanation Evaluation Framework](https://arxiv.org/abs/2410.12855)          |                        **Jailbreak judge**&**Multi-agent framework**                         |
| 24.10 |                                                                               University of Notre Dame, IBM Research                                                                                |              arxiv              |                                     [BenchmarkCards: Large Language Model and Risk Reporting](https://arxiv.org/abs/2410.12974)                                      |                           **BenchmarkCards**&**Bias**&**Fairness**                           |
| 24.10 |                  Vectara, Inc., Iowa State University, University of Southern California, Entropy Technologies, University of Waterloo, Funix.io, University of Wisconsin, Madison                  |              arxiv              |                          [FaithBench: A Diverse Hallucination Benchmark for Summarization by Modern LLMs](https://arxiv.org/abs/2410.13210)                          |          **Hallucination detection**&**Human-annotated benchmark**&**Faithfulness**          |
| 24.10 |                                                                            Southern University of Science and Technology                                                                            |              arxiv              |                         [ChineseSafe: A Chinese Benchmark for Evaluating Safety in Large Language Models](https://arxiv.org/abs/2410.18491)                          |                    **ChineseSafe**&**Content Safety**&**LLM Evaluation**                     |
| 24.10 |                                                                                         Beihang University                                                                                          |              arxiv              |                          [SafeBench: A Safety Evaluation Framework for Multimodal Large Language Models](https://arxiv.org/abs/2410.18927)                           |   **Multimodal Large Language Models**&**Safety Evaluation Framework**&**Risk Assessment**   |
| 24.10 |                                                                                  University of Washington-Madison                                                                                   |              arxiv              |                                 [CFSafety: Comprehensive Fine-grained Safety Assessment for LLMs](https://arxiv.org/abs/2410.21695)                                  |               **Safety Assessment**&**LLM Evaluation**&**Instruction Attacks**               |
| 24.10 |                                                                                     University of Pennsylvania                                                                                      |              arxiv              |                                  [Benchmarking LLM Guardrails in Handling Multilingual Toxicity](https://arxiv.org/abs/2410.22153)                                   |         **Multilingual Toxicity Detection**&**Guardrails**&**Jailbreaking Attacks**          |
| 24.10 |                                                                                   University of Wisconsin-Madison                                                                                   |              arxiv              |                    [InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection Guardrail Models](https://arxiv.org/abs/2410.22770)                     |         **Prompt Injection Defense**&**Over-defense Detection**&**Guardrail Models**         |
| 24.10 |                                                          National Engineering Research Center for Software Engineering, Peking University                                                           |          NeurIPS 2024           |                    [SG-Bench: Evaluating LLM Safety Generalization Across Diverse Tasks and Prompt Types](https://github.com/MurrayTom/SG-Bench)                     |                 **LLM Safety**&**Prompt Engineering**&**Jailbreak Attacks**                  |
| 24.11 |                                                                                          Fudan University                                                                                           |              arXiv              |                                  [LONGSAFETYBENCH: LONG-CONTEXT LLMS STRUGGLE WITH SAFETY ISSUES](https://arxiv.org/abs/2411.06899)                                  |                **Long-Context Models**&**Safety Evaluation**&**Benchmarking**                |
| 24.11 |                                                                                              Anthropic                                                                                              |              arXiv              |                                  [Rapid Response: Mitigating LLM Jailbreaks with a Few Examples](https://arxiv.org/abs/2411.07494)                                   |                           **Jailbreak Defense**&**Rapid Response**                           |
| 24.11 |                                                                                        Texas A&M University                                                                                         |              arXiv              |           [Responsible AI in Construction Safety: Systematic Evaluation of Large Language Models and Prompt Engineering](https://arxiv.org/abs/2411.08320)           |              **Construction Safety**&**Prompt Engineering**&**LLM Evaluation**               |
| 24.11 |                                                                                         IBM Research Europe                                                                                         | NeurIPS 2024 SafeGenAI Workshop |                [HarmLevelBench: Evaluating Harm-Level Compliance and the Impact of Quantization on Model Alignment](https://arxiv.org/abs/2411.06835)                |          **Jailbreaking Techniques**&**LLM Vulnerability**&**Quantization Impact**           |
| 24.11 |                                                                                          Peking University                                                                                          |              arxiv              |                                   [ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain](https://arxiv.org/abs/2411.16736)                                   |                     **LLM Safety**&**Chemistry Domain**&**Benchmarking**                     |
| 24.11 |                                                             New York University, JPMorgan Chase, Cornell Tech, Northeastern University                                                              |              arxiv              |                                    [Assessment of LLM Responses to End-user Security Questions](https://arxiv.org/abs/2411.14571)                                    |              **LLM Evaluation**&**End-user Security**&**Information Integrity**              |
| 24.11 |                                                National Library of Medicine, NIH&University of Maryland&University of Virginia&Universidad de Chile                                                 |              arxiv              |                       [Ensuring Safety and Trust: Analyzing the Risks of Large Language Models in Medicine](https://arxiv.org/abs/2411.14487)                        |                     **Medical AI**&**LLM Safety**&**MedGuard Benchmark**                     |
| 24.11 |                                                                              European Commission Joint Research Centre                                                                              |           EMNLP 2024            |                            [GuardBench: A Large-Scale Benchmark for Guardrail Models](https://aclanthology.org/2024.emnlp-main.1022.pdf)                             |                      **guardrail models**&**benchmark**&**evaluation**                       |
| 24.12 |                                                                                           Vizuara AI Labs                                                                                           |              arxiv              |                           [CBEVAL: A Framework for Evaluating and Interpreting Cognitive Biases in LLMs](https://arxiv.org/abs/2412.03605)                           |              **Cognitive Biases**&**LLM Evaluation**&**Reasoning Limitations**               |
| 24.12 |                                                                         Beijing Institute of Technology, Beihang University                                                                         |              arxiv              |                           [REFF: Reinforcing Format Faithfulness in Language Models across Varied Tasks](https://arxiv.org/abs/2412.09173)                           |                            **Format Faithfulness**&**Benchmark**                             |
| 24.12 |                                                                                    UCLA, Salesforce AI Research                                                                                     |          NeurIPS 2024           |                                          [SAFEWORLD: Geo-Diverse Safety Alignment](https://github.com/PlusLabNLP/SafeWorld)                                          |             **Geo-Diverse Alignment**&**Safety Evaluation**&**Legal Compliance**             |
| 24.12 |                                                                                    Shanghai Jiao Tong University                                                                                    |              arxiv              |                            [SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents](https://arxiv.org/abs/2412.13178)                             |         **Safety-Aware Task Planning**&**Embodied LLM Agents**&**Hazard Mitigation**         |
| 24.12 |                                                                                         Tsinghua University                                                                                         |              arxiv              |                                      [AGENT-SAFETYBENCH: Evaluating the Safety of LLM Agents](https://arxiv.org/abs/2412.14470)                                      |                **Agent Safety**&**Risk Awareness**&**Interactive Evaluation**                |
| 24.12 |                                                                                            TU Darmstadt                                                                                             |              arxiv              |                             [LLMs Lost in Translation: M-ALERT Uncovers Cross-Linguistic Safety Gaps](https://arxiv.org/abs/2412.15035)                              |           **Cross-Linguistic Safety**&**Multilingual Benchmark**&**LLM Alignment**           |
| 24.12 |                                                                 Alibaba, China Academy of Information and Communications Technology                                                                 |              arxiv              |                       [Chinese SafetyQA: A Safety Short-form Factuality Benchmark for Large Language Models](https://arxiv.org/abs/2412.15265)                       |                        **Safety Benchmark**&**Factuality Evaluation**                        |
| 24.12 |                                                                             University of Warwick, Cranfield University                                                                             |              arxiv              |                    [MedHallBench: A New Benchmark for Assessing Hallucination in Medical Large Language Models](https://arxiv.org/abs/2412.18947)                    |                      **Medical Hallucinations**&**Benchmark**&**RLHF**                       |
| 24.12 |                                                                                The Hong Kong Polytechnic University                                                                                 |              arxiv              |                    [SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for LLMs in Cybersecurity](https://arxiv.org/abs/2412.20787)                    |         **Cybersecurity Benchmark**&**Large Language Models**&**Dataset Evaluation**         |
| 25.01 |                                                                                  KTH Royal Institute of Technology                                                                                  |              arxiv              |           [CySecBench: Generative AI-based CyberSecurity-focused Prompt Dataset for Benchmarking Large Language Models](https://arxiv.org/abs/2501.01335)            |               **Cybersecurity Benchmark**&**Jailbreaking**&**Prompt Dataset**                |
| 25.01 |                                                                           Shahjalal University of Science and Technology                                                                            |              arxiv              |                 [From Scarcity to Capability: Empowering Fake News Detection in Low-Resource Languages with LLMs](https://arxiv.org/abs/2501.09604)                  |                **Fake News Detection**&**Bangla**&**Low-Resource Languages**                 |
| 25.01 |                                                                                               NVIDIA                                                                                                |              arxiv              |                     [AEGIS2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails](https://arxiv.org/abs/2501.09004)                     |              **AI Safety**&**Content Moderation Dataset**&**LLM Risk Taxonomy**              |
| 25.01 |                                                                                   Georgia Institute of Technology                                                                                   |              arxiv              |               [On The Origin of Cultural Biases in Language Models: From Pre-training Data to Linguistic Phenomena](https://arxiv.org/abs/2501.04662)                |    **Cultural Bias in LLMs**&**Cross-Linguistic Analysis**&**Arabic-English Benchmarks**     |
| 25.01 |                                                                                         Bocconi University                                                                                          |              arxiv              |                                 [MSTS: A Multimodal Safety Test Suite for Vision-Language Models](https://arxiv.org/abs/2501.10057)                                  |                       **Multimodal Safety**&**Vision-Language Models**                       |
| 25.01 |                                                                                          Fudan University                                                                                           |              arxiv              |               [You Can't Eat Your Cake and Have It Too: The Performance Degradation of LLMs with Jailbreak Defense](https://arxiv.org/abs/2501.12210)                |                    **Jailbreak Defense**&**LLM Performance**&**USEBench**                    |
| 25.01 |                                                                                          McGill University                                                                                          |              arxiv              |                [OnionEval: A Unified Evaluation of Fact-conflicting Hallucination for Small-Large Language Models](https://arxiv.org/abs/2501.12975)                 |   **Fact-conflicting Hallucination**&**Small-Large Language Models (SLLMs)**&**Benchmark**   |
| 25.01 |                                                                                                HKUST                                                                                                |              arxiv              |        [Tune In, Act Up: Exploring the Impact of Audio Modality-Specific Edits on Large Audio Language Models in Jailbreak](https://arxiv.org/abs/2501.13772)        |       **Audio Language Models**&**Jailbreak Vulnerabilities**&**Audio Modality Edits**       |
| 25.01 |                                                                                       University of Cambridge                                                                                       |              arxiv              |                         [CASE-BENCH: Context-Aware Safety Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2501.14940)                          |                 **LLM Safety**&**Context-Aware Evaluation**&**Over-Refusal**                 |
| 25.01 |                                                                           CISPA Helmholtz Center for Information Security                                                                           |      USENIX Security 2025       |                    [HATEBENCH: Benchmarking Hate Speech Detectors on LLM-Generated Content and Hate Campaigns](https://arxiv.org/abs/2501.16750)                     |            **Hate Speech Detection**&**LLM-Generated Content**&**Hate Campaigns**            |
| 25.01 |                                                                   Shanghai Artificial Intelligence Laboratory, Tianjin University                                                                   |              arxiv              |                             [Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models](https://arxiv.org/abs/2501.18533v1)                             |                 **Vision-Language Models (VLMs)**&**Chain-of-Thought (CoT)**                 |
| 25.01 |                                                                             Independent Research Team â€œAnnyeong! Ludaâ€                                                                              |            PACLIC 38            |                                [RICoTA: Red-teaming of In-the-wild Conversation with Test Attempts](https://arxiv.org/abs/2501.17715)                                |               **Jailbreaking**&**Conversational AI**&**User Intent Detection**               |
| 25.01 |                                                                                     Renmin University of China                                                                                      |              arxiv              |                    [SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2501.18636)                     |     **Retrieval-Augmented Generation**&**Security Benchmarking**&**Adversarial Attacks**     |


## ðŸ“šResource

- Toxicity - [RealToxicityPrompts datasets](https://toxicdegeneration.allenai.org/)
- Truthfulness - [TruthfulQA datasets](https://github.com/sylinrl/TruthfulQA)
- TRUSTLLM - [TRUSTLLM](https://trustllmbenchmark.github.io/TrustLLM-Website/)
