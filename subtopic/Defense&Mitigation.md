# Defense

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"


## üìëPapers

| Date  |                                                                                                                 Institute                                                                                                                  |                          Publication                           |                                                                                  Paper                                                                                  |                                                                Keywords                                                                 |
|:-----:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------------------------:|
| 21.07 |                                                                                                              Google Research                                                                                                               |                            ACL2022                             |                                 [Deduplicating Training Data Makes Language Models Better](https://aclanthology.org/2022.acl-long.577/)                                 |                                        **Privacy Protected**&**Deduplication**&**Memorization**                                         |
| 23.05 |                                                                                                         UC Davis, USC, UW-Madison                                                                                                          |                           NAACL2024                            |                                   [From Shortcuts to Triggers: Backdoor Defense with Denoised PoE](https://arxiv.org/abs/2305.14910)                                    |                                **Backdoor Attacks**&**Defense Methods**&**Natural Language Processing**                                 |
| 23.08 |                                                                                                          Georgia Tech, Intel Labs                                                                                                          |                             arxiv                              |                               [LLM Self Defense: By Self Examination LLMs Know They Are Being Tricked](https://arxiv.org/abs/2308.07308)                                |                                 **Adversarial Attacks**&**Self Defense**&**Harmful Content Detection**                                  |
| 23.08 |                                                                                                           University of Michigan                                                                                                           |                             arxiv                              |                                         [DETECTING LANGUAGE MODEL ATTACKS WITH PERPLEXITY](https://arxiv.org/abs/2308.14132v3)                                          |                                      **Adversarial Suffixes**&**Perplexity**&**Attack Detection**                                       |
| 23.09 |                                                                                                           University of Maryland                                                                                                           |                             arxiv                              |                                         [Certifying LLM Safety against Adversarial Prompting](https://arxiv.org/abs/2309.02705)                                         |                                                **Safety Filter**&**Adversarial Prompts**                                                |
| 23.09 |                                                                                                           University of Maryland                                                                                                           |                             arxiv                              |                              [BASELINE DEFENSES FOR ADVERSARIAL ATTACKS AGAINST ALIGNED LANGUAGE MODELS](https://arxiv.org/abs/2309.00614)                              |                                     **Perplexity**&**Input Preprocessing**&**Adversarial Training**                                     |
| 23.09 |                                                                                                     The Pennsylvania State University                                                                                                      |                             arxiv                              |                                [DEFENDING AGAINST ALIGNMENT-BREAKING ATTACKS VIA ROBUSTLY ALIGNED LLM](https://arxiv.org/abs/2309.14348)                                |                             **Alignment-Breaking Attacks**&**Adversarial Prompts**&**Jailbreaking Prompts**                             |
| 23.10 |                                                                                                         University of Pennsylvania                                                                                                         |                             arxiv                              |                               [SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks](https://arxiv.org/abs/2310.03684)                               |                                          **Jailbreak**&**Adversarial Attack**&**Perturbation**                                          |
| 23.10 |                                                                                                         Michigan State University                                                                                                          |                             arXiv                              |                                [Jailbreaker in Jail: Moving Target Defense for Large Language Models](https://arxiv.org/abs/2310.02417)                                 |                             **Dialogue System**&**Trustworthy Machine Learning**&**Moving Target Defense**                              |
| 23.10 |                                                                                                             Peking University                                                                                                              |                             arxiv                              |                         [Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations](https://arxiv.org/abs/2310.06387)                         |                              **In-Context Learning**&**Adversarial Attacks**&**In-Context Demonstrations**                              |
| 23.10 |                                                                                               The Chinese University of Hong Kong&Microsoft                                                                                                |                           NAACL2024                            |                                           [SELF-GUARD: Empower the LLM to Safeguard Itself](https://arxiv.org/abs/2310.15851)                                           |                                  **Large Language Models**&**Jailbreak Attacks**&**Safety Mechanisms**                                  |
| 23.11 |                                                                                                      University of California Irvine                                                                                                       |                             arxiv                              |                            [Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield](https://arxiv.org/abs/2311.00172)                            |                                           **Adversarial Prompt Shield**&**Safety Classifier**                                           |
| 23.11 |                                                                                                      Child Health Evaluative Sciences                                                                                                      |                             arxiv                              |                                [Pyclipse, a library for deidentification of free-text clinical notes](https://arxiv.org/abs/2311.02748)                                 |                                               **Clinical Text Data**&**Deidentification**                                               |
| 23.11 |                                                                                                            Tsinghua University                                                                                                             |                             arxiv                              |                      [Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization](https://arxiv.org/abs/2311.09096)                       |                                       **Jailbreaking Attacks**&**Goal Prioritization**&**Safety**                                       |
| 23.11 |                                                           University of Southern California, Harvard University, University of California Davis, University of Wisconsin-Madison                                                           |                             arxiv                              |                   [Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations](https://arxiv.org/abs/2311.09763)                   |                                 **Backdoor Attacks**&**Defensive Demonstrations**&**Test-Time Defense**                                 |
| 23.11 |                                                                                                    University of Maryland College Park                                                                                                     |                             arxiv                              |                  [Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information](https://arxiv.org/abs/2311.11509)                   |                            **Adversarial Prompt Detection**&**Perplexity Measures**&**Token-level Analysis**                            |
| 23.12 |                                                                                         Rensselaer Polytechnic Institute, Northeastern University                                                                                          |                             arxiv                              |                            [Combating Adversarial Attacks through a Conscience-Based Alignment Framework](https://arxiv.org/abs/2312.00029)                             |                                    **Adversarial Attacks**&**Conscience-Based Alignment**&**Safety**                                    |
| 23.12 |                                                                                             Azure Research, Microsoft Security Response Center                                                                                             |                             arXiv                              |                                  [Maatphor: Automated Variant Analysis for Prompt Injection Attacks](https://arxiv.org/abs/2312.11513)                                  |                                       **Prompt Injection Attacks**&**Automated Variant Analysis**                                       |
| 23.12 |                                                                 University of Massachusetts Amherst, Columbia University, Google, Stanford University, New York University                                                                 |                             arxiv                              |                                  [Learning and Forgetting Unsafe Examples in Large Language Models](https://arxiv.org/abs/2312.12736)                                   |                                     **Safety Issues**&**ForgetFilter Algorithm**&**Unsafe Content**                                     |
| 23.12 |                                                                                        UC Berkeley, King Abdulaziz City for Science and Technology                                                                                         |                             arXiv                              |                                     [Jatmo: Prompt Injection Defense by Task-Specific Finetuning](https://arxiv.org/abs/2312.17673)                                     |                                                      Prompt Injection&LLM Security                                                      |
| 24.01 |                                                                                                          Arizona State University                                                                                                          |                             arxiv                              |        [The Art of Defending: A Systematic Evaluation and Analysis of LLM Defense Strategies on Safety and Over-Defensiveness](https://arxiv.org/abs/2401.00287)        |                                        **Safety**&**Over-Defensiveness**&**Defense Strategies**                                         |
| 24.01 |                                                                                           Logistic and Supply Chain MultiTech R&D Centre (LSCM)                                                                                            |                             arxiv                              |                 [Detection and Defense Against Prominent Attacks on Preconditioned LLM-Integrated Virtual Assistants](https://arxiv.org/abs/2401.00994)                 |                                                 **Preconditioning**&**Cyber Security**                                                  |
| 24.01 |                                                    The Hong Kong University of Science and Technology, University of Illinois at Urbana-Champaign, The Hong Kong Polytechnic University                                                    |                             arxiv                              |                                 [MLLM-Protector: Ensuring MLLM‚Äôs Safety without Hurting Performance](https://arxiv.org/abs/2401.02906)                                  |                              **Multimodal Large Language Models (MLLMs)**&**Safety**&**Malicious Attacks**                              |
| 24.01 |                                                                                                         Carnegie Mellon University                                                                                                         |                             arxiv                              |                                           [TOFU: A Task of Fictitious Unlearning for LLMs](https://arxiv.org/abs/2401.06121)                                            |                                          **Data Privacy**&**Ethical Concerns**&**Unlearning**                                           |
| 24.01 |                                                                                                 Wuhan University, The University of Sydney                                                                                                 |                             arxiv                              |                         [Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender](https://arxiv.org/abs/2401.06561)                          |                                         **Intention Analysis**&**Jailbreak Defense**&**Safety**                                         |
| 24.01 |                                                                                                    The Hong Kong Polytechnic University                                                                                                    |                             arxiv                              |                [Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks Against LLM-Integrated Applications](https://arxiv.org/abs/2401.07612)                |                                              **AI Security**&**Prompt Injection Attacks**                                               |
| 24.01 |                                                                                     University of Illinois at Urbana-Champaign, University of Chicago                                                                                      |                             arxiv                              |                     [Robust Prompt Optimization for Defending Large Language Models Against Jailbreaking Attacks](https://arxiv.org/abs/2401.17263)                     |                                    **AI Alignment**&**Jailbreaking**&**Robust Prompt Optimization**                                     |
| 24.02 |                                                                                                          Arizona State University                                                                                                          |                             arxiv                              |                             [Adversarial Text Purification: A Large Language Model Approach for Defense](https://arxiv.org/abs/2402.06655)                              |                                      **Textual Adversarial Defenses**&**Adversarial Purification**                                      |
| 24.02 |                                                                                                    Peking University, Wuhan University                                                                                                     |                             arxiv                              |                                    [Fight Back Against Jailbreaking via Prompt Adversarial Tuning](https://arxiv.org/abs/2402.06255)                                    |                              **Jailbreaking Attacks**&**Prompt Adversarial Tuning**&**Defense Mechanisms**                              |
| 24.02 |                                                                            University of Washington, The Pennsylvania State University, Allen Institute for AI                                                                             |                             arxiv                              |                             [SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding](https://arxiv.org/abs/2402.08983)                             |                                             **Jailbreak Attacks**&**Safety-Aware Decoding**                                             |
| 24.02 |                                                                                                Shanghai Artificial Intelligence Laboratory                                                                                                 |                             arxiv                              |                               [Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey](https://arxiv.org/abs/2402.09283)                               |                                          **LLM Conversation Safety**&**Attacks**&**Defenses**                                           |
| 24.02 |                                                                             University of Notre Dame, INRIA&King Abdullah University of Science and Technology                                                                             |                             arxiv                              |                                     [Defending Jailbreak Prompts via In-Context Adversarial Game](https://arxiv.org/abs/2402.13148)                                     |                                             **Adversarial Training**&**Jailbreak Defense**                                              |
| 24.02 |                                                     University of New South Wales Australia, Delft University of Technology The Netherlands&Nanyang Technological University Singapore                                                     |                             arxiv                              |                               [LLM Jailbreak Attack versus Defense Techniques - A Comprehensive Study](https://arxiv.org/abs/2402.13457)                                |                                              **Jailbreak Attacks**&**Defense Techniques**                                               |
| 24.02 |                                                                                    The Hong Kong University of Science and Technology, Duke University                                                                                     |                             arxiv                              |                          [GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis](https://arxiv.org/abs/2402.13494)                          |                                    **Safety-Critical Gradient Analysis**&**Unsafe Prompt Detection**                                    |
| 24.02 |                                                                                                        The University of Melbourne                                                                                                         |                             arxiv                              |                          [Round Trip Translation Defence against Large Language Model Jailbreaking Attacks](https://arxiv.org/abs/2402.13517)                           |                                        **Social-Engineered Attacks**&**Round Trip Translation**                                         |
| 24.02 |                                                                                                      Nanyang Technological University                                                                                                      |                             arxiv                              |                        [LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper](https://arxiv.org/abs/2402.15727)                        |                                                **Jailbreaking Attacks**&**Self-Defense**                                                |
| 24.02 |                                                                                                              Ajou University                                                                                                               |                             arxiv                              |                      [Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement](https://arxiv.org/abs/2402.15180)                      |                                                **Jailbreak Attacks**&**Self-Refinement**                                                |
| 24.02 |                                                                                                                    UCLA                                                                                                                    |                             arxiv                              |                                   [Defending LLMs against Jailbreaking Attacks via Backtranslation](https://arxiv.org/abs/2402.16459)                                   |                                              **Backtranslation**&**Jailbreaking Attacks**                                               |
| 24.02 |                                                                                                   University of California Santa Barbara                                                                                                   |                             arxiv                              |                          [Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing](https://arxiv.org/abs/2402.16192)                           |                                              **Semantic Smoothing**&**Jailbreak Attacks**                                               |
| 24.02 |                                                                                                      University of Wisconsin-Madison                                                                                                       |                             arxiv                              |                              [Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment](https://arxiv.org/abs/2402.14968)                               |                                    **Fine-tuning Attack**&**Backdoor Alignment**&**Safety Examples**                                    |
| 24.02 |                                                                                                            University of Exeter                                                                                                            |                             arxiv                              |                           [Is the System Message Really Important to Jailbreaks in Large Language Models?](https://arxiv.org/abs/2402.14857)                            |                                                    **Jailbreak**&**System Messages**                                                    |
| 24.03 |                                                                                             The Chinese University of Hong Kong, IBM Research                                                                                              |                             arxiv                              |              [Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes](https://arxiv.org/abs/2403.00867)               |                                        **Jailbreak Attacks**&**Refusal Loss**&**Gradient Cuff**                                         |
| 24.03 |                                                                  Oregon State University, Pennsylvania State University, CISPA Helmholtz Center for Information Security                                                                   |                             arxiv                              |                                   [AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks](https://arxiv.org/abs/2403.04783)                                    |                                              **Defense Mechanisms**&**Jailbreak Attacks**                                               |
| 24.03 |                                                         Peking University, University of Wisconsin‚ÄìMadison, International Digital Economy Academy, University of California Davis                                                          |                             arxiv                              |         [AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting](https://arxiv.org/abs/2403.09513)          |                                    **Multimodal Large Language Models Safety**&**Defense Strategy**                                     |
| 24.03 |                                                Southern University of Science and Technology, Hong Kong University of Science and Technology, Huawei Noah‚Äôs Ark Lab, Peng Cheng Laboratory                                                 |                             arxiv                              |                         [Eyes Closed Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation](https://arxiv.org/abs/2403.09572)                          |                                                     **Multimodal LLMs**&**Safety**                                                      |
| 24.03 |                                                                           UIUC, Virginia Tech, Salesforce Research, University of California Berkeley, UChicago                                                                            |                             arxiv                              |                         [RIGORLLM: RESILIENT GUARDRAILS FOR LARGE LANGUAGE MODELS AGAINST UNDESIRED CONTENT](https://arxiv.org/abs/2403.13031)                          |                                         **Biases**&**Harmful Content**&**Resilient Guardrails**                                         |
| 24.03 |                                                                                                                 Microsoft                                                                                                                  |                             arxiv                              |                                [Defending Against Indirect Prompt Injection Attacks With Spotlighting](https://arxiv.org/abs/2403.14720)                                |                                             **Indirect Prompt Injection**&**Spotlighting**                                              |
| 24.03 |                                                            XiaMen University, Yanshan University, IDEA Research, Inner Mongolia University, Microsoft, Microsoft Research Asia                                                             |                           NAACL2024                            |                      [Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models](https://arxiv.org/abs/2403.11838)                       |                                      **Language Models**&**Safety Guidelines**&**Model Alignment**                                      |
| 24.03 |                                                                                                                 IIT Delhi                                                                                                                  |                           NAACL2024                            |              [Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF](https://arxiv.org/abs/2403.10088)               |           **Counterspeech Generation**&**Multi-Task Instruction Tuning**&**Reinforcement Learning from AI Feedback (RLAIF)**            |
| 24.03 |                                                                                                           Stony Brook University                                                                                                           |                      NAACL2024(findings)                       |                                     [Task-Agnostic Detector for Insertion-Based Backdoor Attacks](https://arxiv.org/abs/2403.17155)                                     |                                       **Backdoor Detection**&**Logit Features**&**NLP Security**                                        |
| 24.03 |                                                                                                            Chung-Ang University                                                                                                            |                      NAACL2024(findings)                       |             [Don‚Äôt be a Fool: Pooling Strategies in Offensive Language Detection from User-Intended Adversarial Attacks](https://arxiv.org/abs/2403.15467)              |                             **Offensive Language Detection**&**Adversarial Attacks**&**Pooling Strategies**                             |
| 24.04 |                                                                                           South China University of Technology&Pazhou Laboratory                                                                                           |                             arxiv                              |                       [Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge](https://arxiv.org/abs/2404.05880)                        |                                                     **Jailbreaking**&**Unlearning**                                                     |
| 24.04 |                                                                                               Zhejiang University, Johns Hopkins University                                                                                                |                             arxiv                              |                                [SAFEGEN: Mitigating Unsafe Content Generation in Text-to-Image Models](https://arxiv.org/abs/2404.06666)                                |                                   **Text-to-Image Models**&**Unsafe Content**&**Content Mitigation**                                    |
| 24.04 |                                                                                    Hong Kong University of Science and Technology, University of Oxford                                                                                    |                             arxiv                              |                                    [Latent Guard: A Safety Framework for Text-to-Image Generation](https://arxiv.org/abs/2404.08031)                                    |                                     **Text-to-Image Models**&**Safety Framework**&**Latent Guard**                                      |
| 24.04 |                                        Nanjing University, Microsoft Research Asia, Tsinghua University, Queen Mary University of London, Pennsylvania State University,  NEC Laboratories America                                         |                             arxiv                              |                                          [Protecting Your LLMs with Information Bottleneck](https://arxiv.org/abs/2404.13968)                                           |                                           **Information Bottleneck**&**Adversarial Defense**                                            |
| 24.04 |                                                                             Centre for Software Excellence Huawei, University of Manitoba, Queen‚Äôs University                                                                              |                             arxiv                              |                         [A Framework for Real-time Safeguarding the Text Generation of Large Language Models](https://arxiv.org/abs/2404.19048)                         |                                          **Text Generation Safety**&**Real-time Safeguarding**                                          |
| 24.04 |                                                                                                     Princeton University&UC Davis&USC                                                                                                      |                           NAACL2024                            |                        [Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors](https://arxiv.org/abs/2404.02356)                         |                                  **Nested Product of Experts**&**Data Poisoning**&**Backdoor Defense**                                  |
| 24.05 |                                                                                                            Tsinghua University                                                                                                             |                             arxiv                              |                    [Learnable Linguistic Watermarks for Tracing Model Extraction Attacks on Large Language Models](https://arxiv.org/abs/2405.01509)                    |                                              **Model Extraction Attacks**&**Watermarking**                                              |
| 24.05 |                                                                                                            Tsinghua University                                                                                                             |                             arxiv                              |                                    [Adaptive and Robust Watermark against Model Extraction Attack](https://arxiv.org/abs/2405.02365)                                    |                                              **Model Extraction Attacks**&**Watermarking**                                              |
| 24.05 |                                                                                                          Johns Hopkins University                                                                                                          |                           ICML 2024                            |                              [PARDEN: Can You Repeat That? Defending against Jailbreaks via Repetition](https://arxiv.org/abs/2405.07932)                               |                                                       **Jailbreaks**&**Defense**                                                        |
| 24.05 |                                                                                                          University of Edinburgh                                                                                                           |                             arxiv                              |                                 [Spectral Editing of Activations for Large Language Model Alignment](https://arxiv.org/abs/2405.09719)                                  |                                  **Bias Mitigation**&**Truthfulness Enhancement**&**Spectral Editing**                                  |
| 24.05 |                                                                                                        East China Normal University                                                                                                        |                             arxiv                              |                     [A Safety Realignment Framework via Subspace-Oriented Model Fusion for Large Language Models](https://arxiv.org/abs/2405.09055)                     |                                                 **Model Fusion**&**Safeguard Strategy**                                                 |
| 24.05 |                                                                                             The Hong Kong University of Science and Technology                                                                                             |                             arxiv                              |                                        [Backdoor Removal for Generative Large Language Models](https://arxiv.org/abs/2405.07667)                                        |                                     **Backdoor Attacks**&**Generative Models**&**Safety Training**                                      |
| 24.05 |                                                                                                           Stony Brook University                                                                                                           |                             arxiv                              |                            [Robustifying Safety-Aligned Large Language Models through Clean Data Curation](https://arxiv.org/abs/2405.19358)                            |                                **Jailbreaking Attacks**&**Clean Data Curation**&**LLM Safety Alignment**                                |
| 24.05 |                                                                                                         University of Pennsylvania                                                                                                         |                             arxiv                              |                             [One-Shot Safety Alignment for Large Language Models via Optimal Dualization](https://arxiv.org/abs/2405.19544)                             |                                    **Safety Alignment**&**Constrained RLHF**&**Optimal Dualization**                                    |
| 24.05 |                                                                                                                   Naver                                                                                                                    |                             arxiv                              |                                  [SLM as Guardian: Pioneering AI Safety with Small Language Models](https://arxiv.org/abs/2405.19795)                                   |                                   **AI Safety**&**Small Language Models**&**Harmful Query Detection**                                   |
| 24.05 |                                                                                                    The Chinese University of Hong Kong                                                                                                     |                             arxiv                              |                    [Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks](https://arxiv.org/abs/2405.20099)                     |                                    **Jailbreak Attacks**&**Defensive Prompt Patch**&**LLM Security**                                    |
| 24.05 |                                                                                               Tokyo University of Agriculture and Technology                                                                                               |                             arxiv                              |                            [Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent](https://arxiv.org/abs/2405.20770)                             |                                      **Adversarial Robustness**&**LLM Agent**&**Textual Attacks**                                       |
| 24.06 |                                                                                                    University of California, Riverside                                                                                                     |                             arxiv                              |                                  [Cross-Modal Safety Alignment: Is Textual Unlearning All You Need?](https://arxiv.org/abs/2406.02575)                                  |                          **Cross-Modality Safety Alignment**&**Textual Unlearning**&**Vision-Language Models**                          |
| 24.06 |                                                                                                          University of Liverpool                                                                                                           | IEEE Transactions on Pattern Analysis and Machine Intelligence |                                            [Safeguarding Large Language Models: A Survey](https://arxiv.org/abs/2406.02622)                                             |                                              **Survey**&**Safeguards**&**Trustworthy AI**                                               |
| 24.06 |                                                                                                          Oregon State University                                                                                                           |                             arxiv                              |                      [Defending Large Language Models Against Attacks With Residual Stream Activation Analysis](https://arxiv.org/abs/2406.03230)                       |                                          **Adversarial Machine Learning**&**Machine Learning**                                          |
| 24.06 |                                                                                               Huazhong University of Science and Technology                                                                                                |                             arxiv                              |                          [AutoJailbreak: Exploring Jailbreak Attacks and Defenses through a Dependency Lens](https://arxiv.org/abs/2406.03805)                          |                                              **Jailbreak Attacks**&**Dependency Analysis**                                              |
| 24.06 |                                                                                             The Hong Kong University of Science and Technology                                                                                             |                             arxiv                              |                          [SELFDEFEND: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner](https://arxiv.org/abs/2406.05498)                          |                                                 **Jailbreaking Defense**&**SELFDEFEND**                                                 |
| 24.06 |                                                                                                            Princeton University                                                                                                            |                             arxiv                              |                                  [Safety Alignment Should Be Made More Than Just a Few Tokens Deep](https://arxiv.org/abs/2406.05946)                                   |                                              **Safety Alignment**&**Adversarial Attacks**                                               |
| 24.06 |                                                                                                  The University of Alabama at Birmingham                                                                                                   |                             arxiv                              |                               [Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models](https://arxiv.org/abs/2406.05948)                               |                                               **Backdoor Attacks**&**Chain-of-Scrutiny**                                                |
| 24.06 |                                                                                       The Hong Kong University of Science and Technology (Guangzhou)                                                                                       |                             arxiv                              |                                  [Adversarial Tuning: Defending Against Jailbreak Attacks for LLMs](https://arxiv.org/abs/2406.06622)                                   |                                              **Jailbreak Attacks**&**Adversarial Tuning**                                               |
| 24.06 |                                                                                                                Komorebi AI                                                                                                                 |                             arxiv                              |                                      [Merging Improves Self-Critique Against Jailbreak Attacks](https://arxiv.org/abs/2406.07188)                                       |                                                 **Jailbreak Attacks**&**Self-Critique**                                                 |
| 24.06 |                                                                                              Tsinghua Shenzhen International Graduate School                                                                                               |                             arxiv                              |                     [MLLMGUARD: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models](https://arxiv.org/abs/2406.07594)                     |                                          **Safety Evaluation**&**MLLMs**&**Multi-dimensional**                                          |
| 24.06 |                                                                                                             Purdue University                                                                                                              |                             arxiv                              |                         [RL-JACK: Reinforcement Learning-powered Black-box Jailbreaking Attack against LLMs](https://arxiv.org/abs/2406.08725)                          |                                           **Jailbreaking Attacks**&**Reinforcement Learning**                                           |
| 24.06 |                                                                                          Mohamed bin Zayed University of Artificial Intelligence                                                                                           |                             arxiv                              |                                [MirrorCheck: Efficient Adversarial Defense for Vision-Language Models](https://arxiv.org/abs/2406.09250)                                |                                   **Adversarial Defense**&**Vision-Language Models**&**MirrorCheck**                                    |
| 24.06 |                                                                                                            Teesside University                                                                                                             |                             arxiv                              |                       [Threat Modelling and Risk Analysis for Large Language Model (LLM)-Powered Applications](https://arxiv.org/abs/2406.11007)                        |                                   **Threat Modelling**&**Risk Analysis**&**LLM-Powered Applications**                                   |
| 24.06 |                                                                                                             NVIDIA Corporation                                                                                                             |                             arxiv                              |                                    [garak: A Framework for Security Probing Large Language Models](https://arxiv.org/abs/2406.11036)                                    |                                                     **garak**&**Security Probing**                                                      |
| 24.06 |                                                                                               University of Science and Technology of China                                                                                                |                             arxiv                              |                     [Self and Cross-Model Distillation for LLMs: Effective Methods for Refusal Pattern Alignment](https://arxiv.org/abs/2406.11285)                     |                            **Self Distillation**&**Cross-Model Distillation**&**Refusal Pattern Alignment**                             |
| 24.06 |                                                                                                          University of Washington                                                                                                          |                             arxiv                              |                         [CLEANGEN: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models](https://arxiv.org/abs/2406.12257)                         |                                         **CLEANGEN**&**Backdoor Attacks**&**Generation Tasks**                                          |
| 24.06 |                                                                                                            Columbia University                                                                                                             |                             arxiv                              |                                   [Defending Against Social Engineering Attacks in the Age of LLMs](https://arxiv.org/abs/2406.12263)                                   |                                                **Social Engineering**&**CSE Detection**                                                 |
| 24.06 |                                                                                                  Indian Institute of Technology Kharagpur                                                                                                  |                             arxiv                              |                        [SafeInfer: Context Adaptive Decoding Time Safety Alignment for Large Language Models](https://arxiv.org/abs/2406.12274)                         |                                    **SafeInfer**&**Context Adaptive Decoding**&**Safety Alignment**                                     |
| 24.06 |                                                                                                        Chinese Academy of Sciences                                                                                                         |                             arxiv                              |        [Adversarial Contrastive Decoding: Boosting Safety Alignment of Large Language Models via Opposite Prompt Optimization](https://arxiv.org/abs/2406.16743)        |                                              **Safety Alignment**&**Contrastive Decoding**                                              |
| 24.07 |                                                                                                           University of Toronto                                                                                                            |                             arxiv                              |                             [A False Sense of Safety: Unsafe Information Leakage in ‚ÄòSafe‚Äô AI Responses](https://arxiv.org/abs/2407.02551)                              |                                              **Jailbreak Attacks**&**Information Leakage**                                              |
| 24.07 |                                                                                                            Tsinghua University                                                                                                             |                             arxiv                              |              [Safe Unlearning: A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks](https://arxiv.org/abs/2407.02855)               |                                                  **Jailbreak Attacks**&**Unlearning**                                                   |
| 24.07 |                                                                                                      National University of Singapore                                                                                                      |                             arxiv                              |                                  [Self-Evaluation as a Defense Against Adversarial Attacks on LLMs](https://arxiv.org/abs/2407.03234)                                   |                                               **Adversarial Attacks**&**Self-Evaluation**                                               |
| 24.07 |                                                                                                             Tianjin University                                                                                                             |                             arxiv                              |                                     [DART: Deep Adversarial Automated Red Teaming for LLM Safety](https://arxiv.org/abs/2407.03876)                                     |                                    **Automated Red Teaming**&**Adversarial Training**&**LLM Safety**                                    |
| 24.07 |                                                                                                         Seoul National University                                                                                                          |                       ACL 2024 Workshop                        |                              [Safe-Embed: Unveiling the Safety-Critical Knowledge of Sentence Encoders](https://arxiv.org/abs/2407.06851)                               |                                 **Sentence Encoders**&**Safety-Critical Knowledge**&**Unsafe Prompts**                                  |
| 24.07 |                                                                                                              Duke University                                                                                                               |                             arxiv                              |                                     [Refusing Safe Prompts for Multi-modal Large Language Models](https://arxiv.org/abs/2407.09050)                                     |                                   **Multimodal LLMs**&**Safe Prompt Refusal**&**Adversarial Attacks**                                   |
| 24.07 |                                                                                                    The Chinese University of Hong Kong                                                                                                     |                             arxiv                              |                      [Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training](https://arxiv.org/abs/2407.09121)                       |                              **Safety Training**&**Refusal Position Bias**&**Decoupled Refusal Training**                               |
| 24.07 |                                                                                                             GovTech Singapore                                                                                                              |                             arxiv                              |                    [LionGuard: Building a Contextualized Moderation Classifier to Tackle Localized Unsafe Content](https://arxiv.org/abs/2407.10995)                    |                                       **Moderation Classifier**&**Localized Content**&**Safety**                                        |
| 24.07 |                                                                                                      Georgia Institute of Technology                                                                                                       |                             arxiv                              |                  [Targeted Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs](https://arxiv.org/abs/2407.15549)                   |                                             **Latent Adversarial Training**&**Robustness**                                              |
| 24.07 |                                                                                                      Georgia Institute of Technology                                                                                                       |                             arxiv                              |                 [A Voter-Based Stochastic Rejection-Method Framework for Asymptotically Safe Language Model Outputs](https://arxiv.org/abs/2407.16994)                  |                                               **Stochastic Rejection-Method**&**Safety**                                                |
| 24.07 |                                                                                                       Shanghai Jiao Tong University                                                                                                        |                             arxiv                              |                                              [SAFETY-J: Evaluating Safety with Critique](https://arxiv.org/abs/2407.17075)                                              |                                **Safety Evaluation**&**Critique-based Judgment**&**Bilingual Evaluator**                                |
| 24.07 |                                                                                                                 Dynamo AI                                                                                                                  |                       ICML 2024 Workshop                       |                                    [PrimeGuard: Safe and Helpful LLMs through Tuning-Free Routing](https://arxiv.org/abs/2407.16318)                                    |                                        **Inference-Time Guardrails**&**Safety**&**Helpfulness**                                         |
| 24.07 |                                                                                                          ShanghaiTech University                                                                                                           |                             arxiv                              |                             [Defending Jailbreak Attack in VLMs via Cross-modality Information Detector](https://arxiv.org/abs/2407.21659)                              |                         **Jailbreak Attacks**&**Vision Language Models (VLMs)**&**Cross-modality Information**                          |
| 24.08 |                                                                                                  University of Illinois Urbana-Champaign                                                                                                   |                             arxiv                              |                                          [Tamper-Resistant Safeguards for Open-Weight LLMs](https://arxiv.org/abs/2408.00761)                                           |                                       **Tamper Resistance**&**Security**&**Adversarial Attacks**                                        |
| 24.08 |                                                                                             Beijing University of Posts and Telecommunications                                                                                             |                             arxiv                              |                            [SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models](https://arxiv.org/abs/2408.02632)                            |                                           **Self-Evolving Framework**&**Adversarial Safety**                                            |
| 24.08 |                                                                                                     University of Texas at San Antonio                                                                                                     |                       KDD 2024 AI4Cyber                        |                       [Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models?](https://arxiv.org/abs/2408.02651)                        |                                                 &**Adversarial Attacks**&**Alignment**                                                  |
| 24.08 |                                                                                                      University of Texas at Arlington                                                                                                      |                             arxiv                              |                  [Utilizing Large Language Models to Optimize the Detection and Explainability of Phishing Websites](https://arxiv.org/abs/2408.05667)                  |                                                **Phishing Detection**&**Explainability**                                                |
| 24.08 |                                                                                                            New York University                                                                                                             |                             arxiv                              |                          [Towards Robust and Cost-Efficient Knowledge Unlearning for Large Language Models](https://arxiv.org/abs/2408.06621)                           |                                              **Knowledge Unlearning**&**Cost-Efficiency**                                               |
| 24.08 |                                                                                             Beijing University of Posts and Telecommunications                                                                                             |                             arxiv                              |                [Alignment-Enhanced Decoding: Defending via Token-Level Adaptive Refining of Probability Distributions](https://arxiv.org/abs/2408.07663)                |                                          **Alignment-Enhanced Decoding**&**Jailbreak Defense**                                          |
| 24.08 |                                                                                               University of Science and Technology of China                                                                                                |                             arxiv                              |                   [Prefix Guidance: A Steering Wheel for Large Language Models to Defend Against Jailbreak Attacks](https://arxiv.org/abs/2408.08924)                   |                                                **Jailbreak Defense**&**Prefix Guidance**                                                |
| 24.08 |                                                                                                          University of Liverpool                                                                                                           |                             arxiv                              |                      [Adaptive Guardrails for Large Language Models via Trust Modeling and In-Context Learning](https://arxiv.org/abs/2408.08959)                       |                                   **Adaptive Guardrails**&**Trust Modeling**&**In-Context Learning**                                    |
| 24.08 |                                                                                                      National University of Singapore                                                                                                      |                             arxiv                              | [BaThe: Defense against the Jailbreak Attack in Multimodal Large Language Models by Treating Harmful Instruction as Backdoor Trigger](https://arxiv.org/abs/2408.09093) |                             **Jailbreak Defense**&**Multimodal Large Language Models**&**Backdoor Trigger**                             |
| 24.08 |                                                                                                      Georgia Institute of Technology                                                                                                       |                             arxiv                              |                  [Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning](https://arxiv.org/abs/2408.09600)                  |                                **Safety Alignment**&**Harmful Fine-tuning**&**Post-fine-tuning Defense**                                |
| 24.08 |                                                                                                               LG Electronics                                                                                                               |                             arxiv                              |                                   [ATHENA: Safe Autonomous Agents with Verbal Contrastive Learning](https://arxiv.org/abs/2408.11021)                                   |                               **Autonomous Agents**&**Verbal Contrastive Learning**&**Safety Evaluation**                               |
| 24.08 |                                                                                                          Duke Kunshan University                                                                                                           |                             arxiv                              |                  [EEG-Defender: Defending against Jailbreak through Early Exit Generation of Large Language Models](https://arxiv.org/abs/2408.11308)                   |                                             **Jailbreak Defense**&**Early Exit Generation**                                             |
| 24.08 |                                                                                               Hong Kong University of Science and Technology                                                                                               |                             arxiv                              |                        [AgentMonitor: A Plug-and-Play Framework for Predictive and Secure Multi-Agent Systems](https://arxiv.org/abs/2408.14972)                        |                                    **Multi-Agent Systems**&**Predictive Modeling**&**AgentMonitor**                                     |
| 24.09 |                                                                                                    University of Maryland, College Park                                                                                                    |                             CoLM24                             |                  [Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models](https://arxiv.org/abs/2409.00598)                  |                                   **False Refusals**&**Pseudo-Harmful Prompts**&**Safety Alignment**                                    |
| 24.09 |                                                                                                      Georgia Institute of Technology                                                                                                       |                             arxiv                              |                [BOOSTER: Tackling Harmful Fine-Tuning for Large Language Models via Attenuating Harmful Perturbation](https://arxiv.org/abs/2409.01586)                 |                                 **Harmful Fine-Tuning**&**LLM Alignment**&**Perturbation Attenuation**                                  |
| 24.09 |                                                                                                 University of Chinese Academy of Sciences                                                                                                  |                             arxiv                              |                              [Recent Advances in Attack and Defense Approaches of Large Language Models](https://arxiv.org/abs/2409.03274)                              |                                              **Attack Approaches**&**Defense Mechanisms**                                               |
| 24.09 |                                                            Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing, Institute of Artificial Intelligence                                                            |                             arxiv                              |                                [HSF: Defending against Jailbreak Attacks with Hidden State Filtering](https://arxiv.org/abs/2409.03788)                                 |                                 **Hidden State Filtering**&**Jailbreak Attacks**&**Defense Mechanism**                                  |
| 24.09 |                                                                                                  Mahindra International School, SuperAGI                                                                                                   |                             arxiv                              |                                [Safeguarding AI Agents: Developing and Analyzing Safety Architectures](https://arxiv.org/abs/2409.03793)                                |                                       **AI Agents**&**Safety Architectures**&**Risk Mitigation**                                        |
| 24.09 |                                                                                                        Southern Illinois University                                                                                                        |                             arxiv                              |                   [Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks](https://arxiv.org/abs/2409.07353)                   |                              **Vision-Language Models**&**Jailbreak Attacks**&**Adversarial Fine-Tuning**                               |
| 24.09 |                                                                                                               Noah's Ark Lab                                                                                                               |                           COLM 2024                            |                [CoCA: Regaining Safety-awareness of Multimodal Large Language Models with Constitutional Calibration](https://arxiv.org/abs/2409.11365)                 |                    **Multimodal Large Language Models (MLLMs)**&**Safety Awareness**&**Constitutional Calibration**                     |
| 24.09 |                                                                                                             Rutgers University                                                                                                             |                             arxiv                              |                                   [Data-centric NLP Backdoor Defense from the Lens of Memorization](https://arxiv.org/abs/2409.14200)                                   |                                   **NLP Backdoor Defense**&**Memorization**&**Data-centric Defense**                                    |
| 24.09 |                                                                                                          Northwestern University                                                                                                           |                             arxiv                              |                      [PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs](https://arxiv.org/abs/2409.14729)                       |                                            **Prompt Injection**&**Fuzzing**&**LLM Security**                                            |
| 24.09 |                                                                                                          University of Minnesota                                                                                                           |                             arxiv                              |           [On the Vulnerability of Applying Retrieval-Augmented Generation within Knowledge-Intensive Application Domains](https://arxiv.org/abs/2409.17275)            |                      **Retrieval-Augmented Generation**&**Poisoning Attacks**&**Knowledge-Intensive Applications**                      |
| 24.09 |                                                                                                            IBM Research Europe                                                                                                             |                             arxiv                              |                      [MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks](https://arxiv.org/abs/2409.17699)                      |                                  **Jailbreak Attacks**&**Mixture of Experts**&**Tabular Classifiers**                                   |
| 24.09 |                                                                                     Institute of Information Engineering, Chinese Academy of Sciences                                                                                      |                           EMNLP 2024                           |      [Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction](https://arxiv.org/abs/2409.16783)      |                                  **Red Teaming**&**Multi-turn Interaction**&**Automated Red Teaming**                                   |
| 24.09 |                                                                                                                 Chegg Inc.                                                                                                                 |                             arxiv                              |                                         [Overriding Safety Protections of Open-Source Models](https://arxiv.org/abs/2409.19476)                                         |                                        **Harmfulness**&**Knowledge Drift**&**Model Uncertainty**                                        |
| 24.09 |                                                                                                           University of Toronto                                                                                                            |                             arxiv                              |                                  [ROBUST LLM SAFEGUARDING VIA REFUSAL FEATURE ADVERSARIAL TRAINING](https://arxiv.org/abs/2409.20089)                                   |                                    **Adversarial Training**&**LLM Safeguarding**&**Refusal Feature**                                    |
| 24.10 |                                                                                                           University of A Coru√±a                                                                                                           |                             arxiv                              |                                 [Decoding Hate: Exploring Language Models' Reactions to Hate Speech](https://arxiv.org/abs/2410.00775)                                  |                                           **Hate Speech**&**LLMs**&**Mitigation Strategies**                                            |
| 24.10 |                                                                                                            Tel Aviv University                                                                                                             |                             arxiv                              |                                 [MITIGATING COPY BIAS IN IN-CONTEXT LEARNING THROUGH NEURON PRUNING](https://arxiv.org/abs/2410.01288)                                  |                                        **Copy Bias**&**In-Context Learning**&**Neuron Pruning**                                         |
| 24.10 |                                                                                                        Chinese Academy of Sciences                                                                                                         |                             arxiv                              |          [JAILBREAK ANTIDOTE: RUNTIME SAFETY-UTILITY BALANCE VIA SPARSE REPRESENTATION ADJUSTMENT IN LARGE LANGUAGE MODELS](https://arxiv.org/abs/2410.02298)           |                                        **Jailbreaking**&**LLM Safety**&**Sparse Representation**                                        |
| 24.10 |                                                                                                      CAS Key Laboratory of AI Safety                                                                                                       |                             arxiv                              |                          [HIDDENGUARD: FINE-GRAINED SAFE GENERATION WITH SPECIALIZED REPRESENTATION ROUTER](https://arxiv.org/abs/2410.02684)                           |                                **Safe Generation**&**Representation Router**&**Token-level Moderation**                                 |
| 24.10 |                                                                                                                 HydroX AI                                                                                                                  |                             arxiv                              |                               [Precision Knowledge Editing: Enhancing Safety in Large Language Models](https://arxiv.org/abs/2410.03772)                                |                                                    **Knowledge Editing**&**Safety**                                                     |
| 24.10 |                                                                                                             IBM Research, MIT                                                                                                              |                             arxiv                              |                                        [Large Language Models Can Be Strong Self-Detoxifiers](https://arxiv.org/abs/2410.03818)                                         |                                             **Toxicity Reduction**&**Self-Detoxification**                                              |
| 24.10 |                                                                                                          UC Berkeley, Meta, FAIR                                                                                                           |                             arxiv                              |                                         [Aligning LLMs to Be Robust Against Prompt Injection](https://arxiv.org/abs/2410.05451)                                         |                                                    **Prompt Injection**&**Security**                                                    |
| 24.10 |                                                                                                             Purdue University                                                                                                              |                             arxiv                              |                          [ETA: Evaluating Then Aligning Safety of Vision Language Models at Inference Time](https://arxiv.org/abs/2410.06625)                           |                                   **Vision Language Models**&**Safety Alignment**&**Inference Time**                                    |
| 24.10 |                                                                                   Aerospace Information Research Institute, Chinese Academy of Sciences                                                                                    |                             arxiv                              |                                [Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level](https://arxiv.org/abs/2410.06809)                                |                                           **Decoding-Level Defense**&**Jailbreak Prevention**                                           |
| 24.10 |                                                                                               Rensselaer Polytechnic Institute, IBM Research                                                                                               |                             arxiv                              |                              [SEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection](https://arxiv.org/abs/2410.07471)                               |                                **Safety-enhanced Aligned LLM**&**Fine-tuning**&**Bilevel Optimization**                                 |
| 24.10 |                                                                                                          KAIST AI, Naver Cloud AI                                                                                                          |                             arxiv                              |                          [HOW DOES VISION-LANGUAGE ADAPTATION IMPACT THE SAFETY OF VISION LANGUAGE MODELS?](https://arxiv.org/abs/2410.07571)                           |                                           **Vision-Language Adaptation**&**Safety**&**LVLMs**                                           |
| 24.10 |                                                                                        Johns Hopkins University, Microsoft Responsible AI Research                                                                                         |                             arxiv                              |                       [Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements](https://arxiv.org/abs/2410.08968)                       |                               **Safety alignment**&**Inference-time adaptation**&**LLMs controllability**                               |
| 24.10 |                                                                                              Princeton University, Zoom Video Communications                                                                                               |                             arxiv                              |                          [Instructional Segment Embedding: Improving LLM Safety with Instruction Hierarchy](https://arxiv.org/abs/2410.09102)                           |                              **Instruction hierarchy**&**LLM safety**&**Instructional segment embedding**                               |
| 24.10 |                                                                                        Zhejiang University, Westlake University, Beihang University                                                                                        |                             arxiv                              |                                               [LOCKING DOWN THE FINETUNED LLMS SAFETY](https://arxiv.org/abs/2410.10343)                                                |                                      **Safety alignment**&**Fine-tuned LLMs**&**Meta-SafetyLock**                                       |
| 24.10 |                                                                                                         Carnegie Mellon University                                                                                                         |                             arxiv                              |                                             [Data Defenses Against Large Language Models](https://arxiv.org/abs/2410.13138)                                             |                               **Data defenses**&**Adversarial prompt injections**&**Privacy protection**                                |
| 24.10 |                                                                                                      University of Wisconsin-Madison                                                                                                       |            NeurIPS 2024 Safe Generative AI Workshop            |                                          [Safety-Aware Fine-Tuning of Large Language Models](https://arxiv.org/abs/2410.10014)                                          |                             **Safety-aware fine-tuning**&**Harmful data detection**&**Data contamination**                              |
| 24.10 |                                                                                                            Zhejiang University                                                                                                             |                             arxiv                              |                 [CoreGuard: Safeguarding Foundational Capabilities of LLMs Against Model Stealing in Edge Deployment](https://arxiv.org/abs/2410.13903)                 |                                                 **Model Stealing**&**Edge Deployment**                                                  |
| 24.10 |                                                                                                      Nanyang Technological University                                                                                                      |                             arxiv                              |                           [Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation](https://arxiv.org/abs/2410.14425)                           |                           **Backdoor Defense**&**Knowledge Distillation**&**Parameter-Efficient Fine-Tuning**                           |
| 24.10 |                                                                                                           Stony Brook University                                                                                                           |                             arxiv                              |                         [RobustKV: Defending Large Language Models Against Jailbreak Attacks via KV Eviction](https://arxiv.org/abs/2410.19937)                         |                                     **Jailbreak Attacks**&**LLM Defense**&**KV Cache Optimization**                                     |
| 24.10 |                                                                                                                 UW-Madison                                                                                                                 |                             arxiv                              |                       [FATH: Authentication-based Test-time Defense against Indirect Prompt Injection Attacks](https://arxiv.org/abs/2410.21492)                        |                                 **Prompt Injection Defense**&**LLM Security**&**Authentication System**                                 |
| 24.10 |                                                                                                    Vanderbilt University Medical Center                                                                                                    |                             arxiv                              |                                   [Embedding-based Classifiers Can Detect Prompt Injection Attacks](https://arxiv.org/abs/2410.22284)                                   |                        **Prompt Injection Detection**&**Embedding-based Classification**&**Adversarial Attacks**                        |
| 24.10 |                                                                                                      University of Wisconsin-Madison                                                                                                       |                             arxiv                              |                      [InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection Guardrail Models](https://arxiv.org/abs/2410.22770)                      |                              **Prompt Injection Defense**&**Over-defense Detection**&**Guardrail Models**                               |
| 24.11 |                                                                                                         National Taiwan University                                                                                                         |                             arxiv                              |                                    [Attention Tracker: Detecting Prompt Injection Attacks in LLMs](https://arxiv.org/abs/2411.00348)                                    |                                              **Prompt Injection**&**Attention Mechanism**                                               |
| 24.11 |                                                                                                      National University of Singapore                                                                                                      |                             arxiv                              |                               [Defense Against Prompt Injection Attack by Leveraging Attack Techniques](https://arxiv.org/abs/2411.00459)                               |                                           **Prompt Injection Defense**&**Attack Techniques**                                            |
| 24.11 |                                                                                                      Georgia Institute of Technology                                                                                                       |                             arxiv                              |               [UNIGUARD: Towards Universal Safety Guardrails for Jailbreak Attacks on Multimodal Large Language Models](https://arxiv.org/abs/2411.01703)               |                            **Jailbreak Attacks**&**Multimodal Safety Guardrails**&**Adversarial Robustness**                            |
| 24.11 |                                                                                                         Mila ‚Äì Quebec AI Institute                                                                                                         |                             arXiv                              |                    [Combining Domain and Alignment Vectors to Achieve Better Knowledge-Safety Trade-offs in LLMs](https://arxiv.org/abs/2411.06824)                     |                                       **Domain Expertise**&**Safety Alignment**&**Model Merging**                                       |
| 24.11 |                                                                                                                 Anthropic                                                                                                                  |                             arXiv                              |                                    [Rapid Response: Mitigating LLM Jailbreaks with a Few Examples](https://arxiv.org/abs/2411.07494)                                    |                                                **Jailbreak Defense**&**Rapid Response**                                                 |
| 24.11 |                                                                                                      National University of Singapore                                                                                                      |                             arXiv                              |                                 [The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense](https://arxiv.org/abs/2411.08410)                                  |                                  **Vision-Language Models**&**Jailbreak Defense**&**Model Alignment**                                   |
| 24.11 |                                                                                             Beijing University of Posts and Telecommunications                                                                                             |                             arXiv                              |                            [Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey](https://arxiv.org/abs/2411.09259)                            |                             **Jailbreak Attacks**&**Multimodal Generative Models**&**Security Challenges**                              |
| 24.11 |                                                                                                      Singapore Management University                                                                                                       |                             arxiv                              |                   [CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization](https://arxiv.org/abs/2411.12768)                    |                                           **Backdoor Defense**&**Consistency Regularization**                                           |
| 24.11 |                                                                                                                UC Berkeley                                                                                                                 |                          COLING 2025                           |                                   [Lightweight Safety Guardrails Using Fine-tuned BERT Embeddings](https://arxiv.org/abs/2411.14398)                                    |                                     **Safety Guardrails**&**Fine-tuned BERT**&**Prompt Filtering**                                      |
| 24.11 |                                                                            The Hong Kong University of Science and Technology (Guangzhou), Tsinghua University                                                                             |                             arxiv                              |                            [PEFTGuard: Detecting Backdoor Attacks Against Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2411.17453)                            |                                                **Backdoor Detection**&**PEFT**&**LoRA**                                                 |
| 24.12 |                                                                                                           MIT,Speechmatics,MATS                                                                                                            |                  NeurIPS 2024 SoLaR workshops                  |           [Jailbreak Defense in a Narrow Domain: Limitations of Existing Methods and a New Transcript-Classifier Approach](https://arxiv.org/abs/2412.02159)            |                                    **Jailbreak Defense**&**LLM Security**&**Transcript Classifier**                                     |
| 24.12 |                                                                                                       Shanghai Jiao Tong University                                                                                                        |                          COLING 2025                           |                    [Gracefully Filtering Backdoor Samples for Generative Large Language Models without Retraining](https://arxiv.org/abs/2412.02454)                    |                                      **Backdoor Attacks**&**Generative LLMs**&**Frequency Space**                                       |
| 24.12 |                                                                                                             NVIDIA Corporation                                                                                                             |                             arxiv                              |                             [Improved Large Language Model Jailbreak Detection via Pretrained Embeddings](https://arxiv.org/abs/2412.01547)                             |                                   **Jailbreak Detection**&**Pretrained Embeddings**&**LLM Security**                                    |
| 24.11 |                                                                                             Fudan University, Worcester Polytechnic Institute                                                                                              |                             arxiv                              |                      [Knowledge Database or Poison Base? Detecting RAG Poisoning Attack through LLM Activations](https://arxiv.org/abs/2411.18948)                      |                                        **RAG Poisoning Attack**&**LLM Activations**&**Security**                                        |
| 24.11 |                                                                      University of Maryland-College Park, Indian Institute of Technology Bombay, Princeton University                                                                      |                             arxiv                              |                    [Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via Inference-Time Alignment](https://arxiv.org/abs/2411.18688)                     |                                 **Safety Alignment**&**Jailbreak Attack**&**Inference-Time Alignment**                                  |
| 24.12 |                                                                                                               Not specified                                                                                                                |                             arxiv                              |                                       [Enhancing Adversarial Resistance in LLMs with Recursion](https://arxiv.org/abs/2412.06181)                                       |                                  **Adversarial Resistance**&**LLM Security**&**Prompt Simplification**                                  |
| 24.12 |                                                                                          University of Maryland, Capital One, New York University                                                                                          |                             arxiv                              |                             [Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language Models](https://arxiv.org/abs/2412.06748)                             |                                                **Refusal Tokens**&**Model Calibration**                                                 |
| 24.12 |                                                                                                            Dalhousie University                                                                                                            |                             arxiv                              |                                               [Classifier-free Guidance in LLMs Safety](https://arxiv.org/abs/2412.06846)                                               |                                 **Unlearning**&**Reinforcement Learning**&**Classifier-free Guidance**                                  |
| 24.12 |                                                                                                        Princeton University, Google                                                                                                        |                             arxiv                              |                                    [Evaluating the Durability of Safeguards for Open-Weight LLMs](https://arxiv.org/abs/2412.07097)                                     |                                    **LLM Safeguards**&**Open-Weight Models**&**Adversarial Defense**                                    |
| 24.12 |                                                                                          Michigan State University, University of Hawaii at Manoa                                                                                          |                             arxiv                              |             [FlexLLM: Exploring LLM Customization for Moving Target Defense on Black-Box LLMs Against Jailbreak Attacks](https://arxiv.org/abs/2412.07672)              |                                  **LLM Customization**&**Moving Target Defense**&**Jailbreak Attacks**                                  |
| 24.12 |                                                                                                                  Neudesic                                                                                                                  |                             arxiv                              |                                   [Lightweight Safety Classification Using Pruned Language Models](https://arxiv.org/abs/2412.13435)                                    |                                   **Model Pruning**&**Content Safety**&**Prompt Injection Detection**                                   |
| 24.12 |                                                                                                            Asan Medical Center                                                                                                             |                             arxiv                              |                             [Mitigating Adversarial Attacks in LLMs through Defensive Suffix Generation](https://arxiv.org/abs/2412.13705)                              |                              **Adversarial Defense**&**Gradient-Based Optimization**&**Defensive Suffix**                               |
| 24.12 |                                                                                             The Chinese University of Hong Kong, IBM Research                                                                                              |                           AAAI 2025                            |                      [Token Highlighter: Inspecting and Mitigating Jailbreak Prompts for Large Language Models](https://arxiv.org/abs/2412.18171)                       |                                   **Jailbreak Attacks**&**Affirmation Loss**&**Token-Level Defense**                                    |
| 24.12 |                                                                              MBZUAI, Huazhong University of Science and Technology, University of Notre Dame                                                                               |                             arxiv                              |               [Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models](https://arxiv.org/abs/2412.17034)                |                              **Jailbreak Attacks**&**Activation Boundary Defense (ABD)**&**LLM Security**                               |
| 24.12 |                                                                                                     The Pennsylvania State University                                                                                                      |                             arxiv                              |                 [The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents](https://arxiv.org/abs/2412.16682)                 |                                    **Indirect Prompt Injection**&**Task Alignment**&**LLM Security**                                    |
| 24.12 |                                                                                                                   OpenAI                                                                                                                   |                             arxiv                              |                                   [Deliberative Alignment: Reasoning Enables Safer Language Models](https://arxiv.org/abs/2412.16339)                                   |                                 **Safety Alignment**&**Reasoning-Based Training**&**Chain-of-Thought**                                  |
| 24.12 |                                                                                                 MMLab, The Chinese University of Hong Kong                                                                                                 |                             arxiv                              |                   [RapGuard: Safeguarding Multimodal Large Language Models via Rationale-aware Defensive Prompting](https://arxiv.org/abs/2412.18826)                   |                           **Multimodal Large Language Models**&**Defensive Prompting**&**Safety Mechanisms**                            |
| 24.12 |                                                                                                         National Taiwan University                                                                                                         |                             arxiv                              |                                [Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging](https://arxiv.org/abs/2412.19512)                                 |                                                 **Safety Alignment**&**Model Merging**                                                  |
| 24.12 |                                                                              MBZUAI, Huazhong University of Science and Technology, University of Notre Dame                                                                               |                            ACL 2025                            |               [Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models](https://arxiv.org/abs/2412.17034)                |                                               **Jailbreak**&**LLMs**&**Safety Boundary**                                                |
| 25.01 |                                                                                                                   Lakera                                                                                                                   |                             arxiv                              |                                             [Gandalf the Red: Adaptive Security for LLMs](https://arxiv.org/abs/2501.07927)                                             |                                        **Adaptive Security**&**Prompt Attacks**&**Red-Teaming**                                         |
| 25.01 |                                                                                                               IIIT Hyderabad                                                                                                               |                             arxiv                              |                                     [Enhancing AI Safety Through the Fusion of Low-Rank Adapters](https://arxiv.org/abs/2501.06208)                                     |                                   **Low-Rank Adapters**&**Safety Alignment**&**Jailbreak Mitigation**                                   |
| 25.01 |                                                                                                             Xidian University                                                                                                              |                           AAAI 2025                            |                      [Backdoor Token Unlearning: Exposing and Defending Backdoors in Pretrained Language Models](https://arxiv.org/abs/2501.03272)                      |                                                **Backdoor Defense**&**Token Unlearning**                                                |
| 25.01 |                                                                                                      Case Western Reserve University                                                                                                       |                             arxiv                              |                         [Navigating the Designs of Privacy-Preserving Fine-tuning for Large Language Models](https://arxiv.org/abs/2501.04323)                          |                       **Privacy-Preserving Fine-tuning**&**Data Reconstruction Attack Defense**&**GuardedTuning**                       |
| 25.01 |                                                                                                      North Carolina State University                                                                                                       |                             arxiv                              |                   [Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for Jailbreak Attack Defense](https://arxiv.org/abs/2501.02629)                    |                                 **Layer-Specific Editing**&**Machine Unlearning**&**Jailbreak Defense**                                 |
| 25.01 |                                                                                                         Xi‚Äôan Jiaotong University                                                                                                          |                             arxiv                              |                    [Spot Risks Before Speaking! Unraveling Safety Attention Heads in Large Vision-Language Models](https://arxiv.org/abs/2501.02029)                    |                                  **Vision-Language Models**&**Safety Mechanisms**&**Attention Heads**                                   |
| 25.01 |                                                                                                            New York University                                                                                                             |                             arxiv                              |                     [Safeguarding Large Language Models in Real-time with Tunable Safety-Performance Trade-offs](https://arxiv.org/abs/2501.02018)                      |                       **Large Language Model Safeguards**&**Controlled Text Generation**&**Jailbreak Mitigation**                       |
| 25.01 |                                                                                                        East China Normal University                                                                                                        |                             arxiv                              |     [Latent-space adversarial training with post-aware calibration for defending large language models against jailbreak attacks](https://arxiv.org/abs/2501.10639)     |                                             **Jailbreak Attacks**&**Adversarial Training**                                              |
| 25.01 |                                                                                                                Capital One                                                                                                                 |                             arxiv                              |          [Refining Input Guardrails: Enhancing LLM-as-a-Judge Efficiency Through Chain-of-Thought Fine-Tuning and Alignment](https://arxiv.org/abs/2501.13080)          |                                  **Input Guardrails**&**Chain-of-Thought Fine-Tuning**&**LLM Defense**                                  |
| 25.01 |                                                                                                             Xidian University                                                                                                              |                             arxiv                              |                              [HumorReject: Decoupling LLM Safety from Refusal Prefix via A Little Humor](https://arxiv.org/abs/2501.13677)                              |                                  **LLM Safety**&**Prefix Injection Defense**&**Humor-based Alignment**                                  |
| 25.01 |                                                                                                             Hainan University                                                                                                              |                             arxiv                              |             [FDLLM: A Text Fingerprint Detection Method for LLMs in Multi-Language, Multi-Domain Black-Box Environments](https://arxiv.org/abs/2501.16029)              |                              **LLM Fingerprinting**&**Black-Box Detection**&**Multi-Language AI Security**                              |
| 25.01 |                                                                                                      University of Wisconsin-Madison                                                                                                       |                             arXiv                              |                                 [Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs](https://arxiv.org/abs/2501.16534)                                  |                                     **LLM Alignment**&**Jailbreak Attacks**&**Safety Classifiers**                                      |
| 25.01 |                                                                                                                  AIShield                                                                                                                  |                             arxiv                              |           [CHALLENGES IN ENSURING AI SAFETY IN DEEPSEEK-R1 MODELS: THE SHORTCOMINGS OF REINFORCEMENT LEARNING STRATEGIES](https://arxiv.org/abs/2501.17030v1)           |                                   **AI Safety**&**Supervised Fine-Tuning**&**Harmlessness Reduction**                                   |
| 25.01 |                                                                                                Mondragon University, University of Seville                                                                                                 |                             arxiv                              |                                     [ASTRAL: Automated Safety Testing of Large Language Models](https://arxiv.org/abs/2501.17132v1)                                     |                                             **LLM Safety Testing**&**Automated Evaluation**                                             |
| 25.01 |                                                                                                  UK AI Safety Institute, Redwood Research                                                                                                  |                             arxiv                              |                                               [A sketch of an AI control safety case](https://arxiv.org/abs/2501.17315v1)                                               |                                         **Safety Case**&**Data Exfiltration**&**LLM Security**                                          |
| 25.01 |                                                               Tsinghua University, Shenzhen Campus of Sun Yat-sen University, Didichuxing, Nanyang Technological University                                                                |                             arxiv                              |                [Panacea: Mitigating Harmful Fine-tuning for Large Language Models via Post-fine-tuning Perturbation](https://arxiv.org/abs/2501.18100v1)                |                                         **Harmful Fine-tuning Attack**&**Defensive Alignment**                                          |
| 25.01 |                                                            National University of Singapore, University of Chinese Academy of Sciences, Peking University, Westlake University                                                             |                             arxiv                              |                                       [GuardReasoner: Towards Reasoning-based LLM Safeguards](https://arxiv.org/abs/2501.18492v1)                                       |                          **LLM Safeguards**&**Reasoning-based Moderation**&**Direct Preference Optimization**                           |
| 25.01 |                                                                                                                 Anthropic                                                                                                                  |                             arxiv                              |             [Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming](https://arxiv.org/abs/2501.18837)             |                                 **Universal Jailbreaks**&**LLM Security**&**Classifier-based Defense**                                  |
| 25.01 |                                                                                                      National University of Singapore                                                                                                      |                             arxiv                              |                             [Enhancing Model Defense Against Jailbreaks with Proactive Safety Reasoning](https://arxiv.org/abs/2501.19180)                              |                                   **Jailbreak Defense**&**Safety Chain-of-Thought**&**LLM Security**                                    |
| 25.02 |                                                                                                                 Aligned AI                                                                                                                 |                             arxiv                              |                     [Defense Against the Dark Prompts: Mitigating Best-of-N Jailbreaking with Prompt Evaluation](https://arxiv.org/abs/2502.00580)                      |                                     **Jailbreaking Defense**&**Prompt Evaluation**&**LLM Security**                                     |
| 25.02 |                                                                                                     The Pennsylvania State University                                                                                                      |                             arxiv                              |                              [Towards Robust Multimodal Large Language Models Against Jailbreak Attacks](https://arxiv.org/abs/2502.00653)                              |                                   **Multimodal LLMs**&**Jailbreak Defense**&**Adversarial Training**                                    |
| 25.02 |                                                                                                             Purdue University                                                                                                              |                             arxiv                              |                                      [LLM Safety Alignment is Divergence Estimation in Disguise](https://arxiv.org/abs/2502.00657)                                      |                                     **LLM Alignment**&**Divergence Estimation**&**Safety Training**                                     |
| 25.02 |                                                                                                            University of Oxford                                                                                                            |                             arxiv                              |                               [AgentBreeder: Mitigating the AI Safety Impact of Multi-Agent Scaffolds](https://arxiv.org/abs/2502.00757)                                |                                  **Multi-Agent Systems**&**LLM Safety**&**Evolutionary Optimization**                                   |
| 25.02 |                                                                                                           Huawei Noah‚Äôs Ark Lab                                                                                                            |                             arxiv                              |                               [Almost Surely Safe Alignment of Large Language Models at Inference-Time](https://arxiv.org/abs/2502.01208)                               |                              **LLM Safety Alignment**&**Inference-Time Optimization**&**Constrained MDPs**                              |
| 25.02 |                                                                                                            Tsinghua University                                                                                                             |                             arxiv                              |                                   [STAIR: Improving Safety Alignment with Introspective Reasoning](https://arxiv.org/abs/2502.02384)                                    |                            **LLM Safety Alignment**&**Introspective Reasoning**&**Monte Carlo Tree Search**                             |
| 25.02 |                                                                                                    The Hong Kong Polytechnic University                                                                                                    |                             arxiv                              |                   ["It Warned Me Just at the Right Moment": Exploring LLM-based Real-time Detection of Phone Scams](https://arxiv.org/abs/2502.03964)                   |                               **Phone Scam Detection**&**LLM-based Security**&**Real-time Intervention**                                |
| 25.02 |                                                                                                            Tsinghua University                                                                                                             |                             arxiv                              |                 [Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment](https://arxiv.org/abs/2502.04040)                 |                      **LLM Safety Alignment**&**Reasoning-based Training**&**Out-of-Distribution Generalization**                       |
| 25.02 |                                                                                             King Abdullah University of Science and Technology                                                                                             |                             arxiv                              |      ["Short-length" Adversarial Training Helps LLMs Defend "Long-length" Jailbreak Attacks: Theoretical and Empirical Evidence](https://arxiv.org/abs/2502.04204)      |                                     **Adversarial Training**&**Jailbreak Attacks**&**LLM Security**                                     |
| 25.02 |                                                                                                   University of California, Los Angeles                                                                                                    |                             arxiv                              |                             [DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails](https://arxiv.org/abs/2502.05163)                              |                               **Multilingual LLM Safety**&**Reinforcement Learning**&**Guardrail Models**                               |
| 25.02 |                                                                                                  University of California, Santa Barbara                                                                                                   |                             arxiv                              |                        [MELON: Indirect Prompt Injection Defense via Masked Re-execution and Tool Comparison](https://arxiv.org/abs/2502.05174)                         |                           **Indirect Prompt Injection Defense**&**Large Language Models (LLMs)**&**Security**                           |
| 25.02 |                                                                                                      University of California, Irvine                                                                                                      |                             arxiv                              |                            [Bridging the Safety Gap: A Guardrail Pipeline for Trustworthy LLM Inferences](https://arxiv.org/abs/2502.08142)                             |                                    **LLM Safety**&**Guardrail Pipeline**&**Hallucination Detection**                                    |
| 25.02 |                                                                                             Beijing University of Posts and Telecommunications                                                                                             |                             arxiv                              |             [Refining Positive and Toxic Samples for Dual Safety Self-Alignment of LLMs with Minimal Human Interventions](https://arxiv.org/abs/2502.08657)             |                                  **Safety Alignment**&**Self-Supervised Learning**&**LLM Robustness**                                   |
| 25.02 |                                                                                                         Carnegie Mellon University                                                                                                         |                             arxiv                              |                              [RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage](https://arxiv.org/abs/2502.08966)                               |                                  **Prompt Injection**&**Privacy Leakage**&**Tool-Based Agent Systems**                                  |
| 25.02 |                                                                                                      Georgia Institute of Technology                                                                                                       |                             arxiv                              |                      [AGENTGUARD: Repurposing Agentic Orchestrator for Safety Evaluation of Tool Orchestration](https://arxiv.org/abs/2502.09809)                       |                                           **LLM Agents**&**Tool Orchestration**&**AI Safety**                                           |
| 25.02 |                                                                                                       Shanghai Jiao Tong University                                                                                                        |                             arxiv                              |       [X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from Multi-Turn Jailbreaks without Compromising Usability](https://arxiv.org/abs/2502.09990)       |                                     **Jailbreaking Defense**&**Multi-Turn Attacks**&**LLM Safety**                                      |
| 25.02 |                                                                                                                  KuaiShou                                                                                                                  |                             arxiv                              |                                     [MM-RLHF: The Next Step Forward in Multimodal LLM Alignment](https://arxiv.org/abs/2502.10391)                                      |                                         **Multimodal RLHF**&**LLM Alignment**&**Reward Models**                                         |
| 25.02 |                                                                                                  Indian Institute of Technology Kharagpur                                                                                                  |                             arxiv                              |                     [Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment](https://arxiv.org/abs/2502.11244)                      |                                        **Multilingual Safety**&**Functional Parameter Steering**                                        |
| 25.02 |                                                                                                         The Ohio State University                                                                                                          |                             arxiv                              |                           [AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety Detection](https://arxiv.org/abs/2502.11448)                           |                                     **LLM Agent Safety**&**Risk Detection**&**Adaptive Guardrails**                                     |
| 25.02 |                                                                                                               Alibaba Group                                                                                                                |                             arxiv                              |                      [Equilibrate RLHF: Towards Balancing Helpfulness-Safety Trade-off in Large Language Models](https://arxiv.org/abs/2502.11555)                      |                                   **RLHF**&**Helpfulness-Safety Trade-off**&**Large Language Models**                                   |
| 25.02 |                                                                                                          ShanghaiTech University                                                                                                           |                             arxiv                              |                        [DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with Model Editing](https://arxiv.org/abs/2502.11647)                         |                                        **Jailbreak Defense**&**Model Editing**&**LLM Security**                                         |
| 25.02 |                                                                                                                   KAIST                                                                                                                    |                             arxiv                              |              [SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models](https://arxiv.org/abs/2502.12464)              |                                            **LLM Safety**&**Model Selection**&**Guardrails**                                            |
| 25.02 |                                                                                                             GovTech Singapore                                                                                                              |                             arxiv                              |        [Safe at the Margins: A General Approach to Safety Alignment in Low-Resource English Languages ‚Äì A Singlish Case Study](https://arxiv.org/abs/2502.12485)        |                                             **Safety Alignment**&**Low-Resource Languages**                                             |
| 25.02 |                                                                                                       Beihang University, Baidu Inc.                                                                                                       |                             arxiv                              |                   [Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking](https://arxiv.org/abs/2502.12970)                    |                                  **Jailbreaking Defense**&**LLM Safety**&**Reasoning-based Security**                                   |
| 25.02 |                                                                                                     The Pennsylvania State University                                                                                                      |                             arXiv                              |                                  [Understanding and Rectifying Safety Perception Distortion in VLMs](https://arxiv.org/abs/2502.13095)                                  |                                  **Vision-Language Models**&**Safety Alignment**&**Activation Shift**                                   |
| 25.02 |                                                                                                     Rochester Institute of Technology                                                                                                      |                             arXiv                              |  [UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models](https://arxiv.org/abs/2502.13141)   |                                    **Prompt Injection**&**Backdoor Attacks**&**Adversarial Attacks**                                    |
| 25.02 |                                                                                            Institute of Automation, Chinese Academy of Sciences                                                                                            |                             arXiv                              |                                 [ShieldLearner: A New Paradigm for Jailbreak Attack Defense in LLMs](https://arxiv.org/abs/2502.13162)                                  |                                              **Jailbreak Attack**&**Adversarial Defense**                                               |
| 25.02 |                                                                                                    The Hong Kong Polytechnic University                                                                                                    |                             arXiv                              |   [Why Safeguarded Ships Run Aground? Aligned Large Language Models‚Äô Safety Mechanisms Tend to Be Anchored in The Template Region](https://arxiv.org/abs/2502.13946)    |                                                  **Safety Alignment**&**Jailbreaking**                                                  |
| 25.02 |                                                                                                              Fudan University                                                                                                              |                             arXiv                              |                                [How Jailbreak Defenses Work and Ensemble? A Mechanistic Investigation](https://arxiv.org/abs/2502.14486)                                |                             **Jailbreak Defenses**&**Large Vision-Language Models**&**Ensemble Strategies**                             |
| 25.02 |                                                                                                    The Chinese University of Hong Kong                                                                                                     |                             arXiv                              |             [HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States](https://arxiv.org/abs/2502.14744)             |                          **Jailbreak Detection**&**Large Vision-Language Models**&**Hidden States Monitoring**                          |
| 25.02 |                                                                                                              Wuhan University                                                                                                              |                      USENIX Security 2025                      |        [JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and Manipulation](https://arxiv.org/abs/2502.07557)         |                                       **Jailbreak Defense**&**LLM Security**&**Concept Analysis**                                       |
| 25.02 |                                                                                                           North South University                                                                                                           |                             arxiv                              |                        [Guardians of the Agentic System: Preventing Many-Shots Jailbreak with Agentic System](https://arxiv.org/abs/2502.16750)                         |                                         **AI Agents**&**Jailbreaking**&**Adversarial Attacks**                                          |
| 25.02 |                                                                                                      Warsaw University of Technology                                                                                                       |                             arxiv                              |                        [MAYBE I SHOULD NOT ANSWER THAT, BUT... DO LLMs UNDERSTAND THE SAFETY OF THEIR INPUTS?](https://arxiv.org/abs/2502.16174)                        |                                   **LLM Safety**&**Prompt Classification**&**Adversarial Detection**                                    |
| 25.02 |                                                                                                             Sichuan University                                                                                                             |                             arxiv                              |                [Beyond Surface-Level Patterns: An Essence-Driven Defense Framework Against Jailbreak Attacks in LLMs](https://arxiv.org/abs/2502.19041)                 |                                      **Jailbreak Attacks**&**LLM Security**&**Defense Framework**                                       |
| 25.02 |                                                                                                            Samsung Electronics                                                                                                             |                             arxiv                              |                [Toward Responsible Federated Large Language Models: Leveraging a Safety Filter and Constitutional AI](https://arxiv.org/abs/2502.16691)                 |                                       **Federated Learning**&**LLM Safety**&**Constitutional AI**                                       |
| 25.02 |                                                                                                        Mila, Universit√© de Montr√©al                                                                                                        |                             arxiv                              |                           [A Generative Approach to LLM Harmfulness Detection with Special Red Flag Tokens](https://arxiv.org/abs/2502.16366)                           |                                   **Harmfulness Detection**&**Safety Fine-Tuning**&**Red Flag Token**                                   |
| 25.02 |                                                                                                                 LMU Munich                                                                                                                 |                             arxiv                              |                            [Soft Token Attacks Cannot Reliably Audit Unlearning in Large Language Models](https://arxiv.org/abs/2502.15836)                             |                                    **Unlearning in LLMs**&**Soft Token Attacks**&**Model Auditing**                                     |
| 25.02 |                                                                                                                 PRISM Eval                                                                                                                 |                           AAAI 2025                            |                [Multi-Agent Security Tax: Trading Off Security and Collaboration Capabilities in Multi-Agent Systems](https://arxiv.org/abs/2502.19145)                 |                                  **Multi-Agent Systems**&**Security Trade-Offs**&**Jailbreak Defense**                                  |
| 25.02 |                                                                                                          NEC Laboratories America                                                                                                          |                GenAI4Health Workshop AAAI 2025                 |           [Reducing Hallucinations of Medical Multimodal Large Language Models with Visual Retrieval-Augmented Generation](https://arxiv.org/abs/2502.15040)            |                    **Hallucination Reduction**&**Medical Multimodal LLMs**&**Visual Retrieval-Augmented Generation**                    |
| 25.02 |                                                                                                  √âcole Polytechnique F√©d√©rale de Lausanne                                                                                                  |                           TMLR 2025                            |                           [Single-pass Detection of Jailbreaking Input in Large Language Models](https://openreview.net/forum?id=42v6I5Ut9a)                            |                             **Jailbreak Detection**&**Single-pass Defense**&**Logit-based Classification**                              |
| 25.02 |                                                                                            Harbin Institute of Technology, Shenzhen, Baidu Inc.                                                                                            |                             arXiv                              |                           [The Rise of Darkness: Safety-Utility Trade-Offs in Role-Playing Dialogue Agents](https://arxiv.org/abs/2502.20757)                           |                   **Role-Playing Dialogue Agents**&**Safety-Utility Trade-Off**&**Adaptive Preference Optimization**                    |
| 25.02 |                                                                                      Harbin Institute of Technology & Singapore Management University                                                                                      |                             arXiv                              |                    [Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play Fine-Tuning of LLMs](https://arxiv.org/abs/2502.20968)                     |                                        **Role-Playing LLMs**&**AI Safety**&**Fine-Tuning Risks**                                        |
| 25.03 |                                                                                                      Singapore Management University                                                                                                       |                             arXiv                              |                          [Zero-Shot Defense Against Toxic Images via Inherent Multimodal Alignment in LVLMs](https://arxiv.org/abs/2503.00037)                          |                               **Toxic Image Detection**&**Vision-Language Models**&**Multimodal Safety**                                |
| 25.03 |                                                                                                    ShanghaiTech University, Quantstamp                                                                                                     |                             arXiv                              |               [Breaking the Loop: Detecting and Mitigating Denial-of-Service Vulnerabilities in Large Language Models](https://arxiv.org/abs/2503.00416)                |                                **Denial-of-Service**&**Large Language Models**&**Recurrent Generation**                                 |
| 25.03 |                                                                                                      Georgia Institute of Technology                                                                                                       |                             arXiv                              |                           [Safety Tax: Safety Alignment Makes Your Large Reasoning Models Less Reasonable](https://arxiv.org/abs/2503.00555)                            |                                 **Large Reasoning Models**&**Safety Alignment**&**Reasoning Trade-off**                                 |
| 25.03 |                                                                                               Appier AI Research, National Taiwan University                                                                                               |                             arXiv                              |                        [Answer, Refuse, or Guess? Investigating Risk-Aware Decision Making in Language Models](https://arxiv.org/abs/2503.01332)                        |                             **Risk-Aware Decision Making**&**Language Models**&**Uncertainty Calibration**                              |
| 25.03 |                                                                                        Technical University of Munich, Mila, Universit√© de Montr√©al                                                                                        |                             arXiv                              |                                               [LLM-Safety Evaluations Lack Robustness](https://arxiv.org/abs/2503.02574)                                                |                                      **LLM Safety Evaluation**&**Robustness**&**Bias Mitigation**                                       |
| 25.03 |                                                                                                             Peking University                                                                                                              |                             arXiv                              |                  [SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning](https://arxiv.org/abs/2503.03480)                  |                           **Vision-Language-Action Models**&**Safe Reinforcement Learning**&**Robot Safety**                            |
| 25.03 |                                                                                                     University of California, Berkeley                                                                                                     |                             arXiv                              |                                   [Improving LLM Safety Alignment with Dual-Objective Optimization](https://arxiv.org/abs/2503.03710)                                   |                             **LLM Safety Alignment**&**Dual-Objective Optimization**&**Jailbreak Defense**                              |
| 25.03 |                                                                              University of Science and Technology of China, Nanyang Technological University                                                                               |                             arXiv                              |               [AgentSafe: Safeguarding Large Language Model-based Multi-agent Systems via Hierarchical Data Management](https://arxiv.org/abs/2503.04392)               |                         **Multi-agent Systems Security**&**Hierarchical Data Management**&**Memory Protection**                         |
| 25.03 |                                                                                                Nanjing University of Science and Technology                                                                                                |                             arxiv                              |                         [Adversarial Training for Multimodal Large Language Models against Jailbreak Attacks](https://arxiv.org/abs/2503.04833)                         |                           **Multimodal Large Language Models**&**Adversarial Training**&**Jailbreak Defense**                           |
| 25.03 |                                                                                                         Carnegie Mellon University                                                                                                         |                             arxiv                              |                    [Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety](https://arxiv.org/abs/2503.05021)                    |                                     **LLM Safety**&**Reasoning Fine-Tuning**&**Jailbreak Defense**                                      |
| 25.03 |                                                                                                                   Google                                                                                                                   |                             arxiv                              |                                                 [BSAFE: (B)acktracking for (SAFE)ty](https://arxiv.org/abs/2503.08919)                                                  |                                     **LLM Safety**&**Backtracking Alignment**&**Jailbreak Defense**                                     |
| 25.03 |                                                                                                      National University of Singapore                                                                                                      |                            S&P 2025                            |                          [Prompt Inversion Attack against Collaborative Inference of Large Language Models](https://arxiv.org/abs/2503.09022)                           |                                 **Prompt Inversion**&**Collaborative Inference**&**LLM Privacy Attack**                                 |
| 25.03 |                                                                                     Institute of Information Engineering, Chinese Academy of Sciences                                                                                      |                             arxiv                              |                          [Align in Depth: Defending Jailbreak Attacks via Progressive Answer Detoxification](https://arxiv.org/abs/2503.11185)                          |                                     **Jailbreak Defense**&**Safety Fine-tuning**&**LLM Robustness**                                     |
| 25.03 |                                                                                                                   XCALLY                                                                                                                   |                             arxiv                              |                             [Prompt Injection Detection and Mitigation via AI Multi-Agent NLP Frameworks](https://arxiv.org/abs/2503.11517)                             |                                      **Prompt Injection**&**Multi-Agent Systems**&**AI Security**                                       |
| 25.03 |                                                                                                         Michigan State University                                                                                                          |                             arxiv                              |                              [Safety Mirage: How Spurious Correlations Undermine VLM Safety Fine-tuning](https://arxiv.org/abs/2503.11832)                              |                               **Vision-Language Models**&**Safety Fine-tuning**&**Spurious Correlation**                                |
| 25.03 |                                                                                             Beijing University of Posts and Telecommunications                                                                                             |                             arxiv                              |                         [MirrorGuard: Adaptive Defense Against Jailbreaks via Entropy-Guided Mirror Crafting](https://arxiv.org/abs/2503.12931)                         |                                    **Jailbreak Defense**&**Entropy Analysis**&**Prompt Calibration**                                    |
| 25.03 |                                                                                                    The Hong Kong Polytechnic University                                                                                                    |                             arxiv                              |                              [Towards Harmless Multimodal Assistants with Blind Preference Optimization](https://arxiv.org/abs/2503.14189)                              |                                  **Multimodal LLMs**&**Safety Alignment**&**Preference Optimization**                                   |
| 25.03 |                                                                                                   University of Modena and Reggio Emilia                                                                                                   |                           CVPR 2025                            |                                           [Hyperbolic Safety-Aware Vision-Language Models](https://arxiv.org/abs/2503.12127)                                            |                             **Safety-Aware Retrieval**&**Vision-Language Models**&**Hyperbolic Embedding**                              |
| 25.03 |                                                                                                                   OpenAI                                                                                                                   |                             arxiv                              |                                 [OpenAI‚Äôs Approach to External Red Teaming for AI Models and Systems](https://arxiv.org/abs/2503.16431)                                 |                                      **Red Teaming**&**AI Risk Assessment**&**Safety Evaluation**                                       |
| 25.03 |                                                                                                   Unicom Data Intelligence, China Unicom                                                                                                   |                             arxiv                              |                              [Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts](https://arxiv.org/abs/2503.16529)                               |                                         **Safety Evaluation**&**Distillation**&**Chinese LLMs**                                         |
| 25.03 |                                                                                                            Zhejiang University                                                                                                             |                             arxiv                              |                                      [Towards LLM Guardrails via Sparse Representation Steering](https://arxiv.org/abs/2503.16851)                                      |                                  **Representation Engineering**&**Sparse Autoencoder**&**LLM Safety**                                   |
| 25.03 |                                                                                   Technical University of Munich, Ludwig Maximilian University of Munich                                                                                   |                             arxiv                              |                    [THINK BEFORE REFUSAL: Triggering Safety Reflection in LLMs to Mitigate False Refusal Behavior](https://arxiv.org/abs/2503.17882)                    |                                        **False Refusal**&**Safety Alignment**&**LLM Reasoning**                                         |
| 25.03 |                                                                                             The Hong Kong University of Science and Technology                                                                                             |                             arxiv                              |                     [STShield: Single-Token Sentinel for Real-Time Jailbreak Detection in Large Language Models](https://arxiv.org/abs/2503.17932)                      |                                     **Jailbreak Detection**&**LLM Safety**&**Adversarial Training**                                     |
| 25.03 |                                                                                                       Zhejiang University, Ant Group                                                                                                       |                             arxiv                              |                                 [LookAhead Tuning: Safer Language Models via Partial Answer Previews](https://arxiv.org/abs/2503.19041)                                 |                                          **Safe Fine-tuning**&**Answer Prefix**&**LLM Safety**                                          |
| 25.03 |                                                                                               IBM Research, Rensselaer Polytechnic Institute                                                                                               |                             arxiv                              |                            [Fundamental Safety-Capability Trade-offs in Fine-tuning Large Language Models](https://arxiv.org/abs/2503.20807)                            |                            **Safety-Capability Trade-off**&**Fine-tuning Strategy**&**Theoretical Analysis**                            |
| 25.03 |                                                                                                 Technical University Munich, IBM Research                                                                                                  |                  ICLR 2025 Workshop on BTLMA                   |          [SAFEMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging](https://arxiv.org/abs/2503.17239)          |                                   **Safety Alignment**&**Model Merging**&**Post-Fine-Tuning Defense**                                   |
| 25.03 |                                                                                                                 HydroX AI                                                                                                                  |                             arxiv                              |                          [Optimizing Safe and Aligned Language Generation: A Multi-Objective GRPO Approach](https://arxiv.org/abs/2503.21819)                           |                  **Language Model Alignment**&**Multi-Objective Optimization**&**Group Relative Policy Optimization**                   |
| 25.04 |                                                                                               University of Science and Technology of China                                                                                                |                             arxiv                              |              [LightDefense: A Lightweight Uncertainty-Driven Defense against Jailbreaks via Shifted Token Distribution](https://arxiv.org/abs/2504.01533)               |                                 **LLM Safety**&**Jailbreak Defense**&**Token Distribution Adjustment**                                  |
| 25.04 |                                                                                                         Seoul National University                                                                                                          |                             arxiv                              |                                       [Representation Bending for Large Language Model Safety](https://arxiv.org/abs/2504.01550)                                        |                                **LLM Safety**&**Representation Engineering**&**Adversarial Robustness**                                 |
| 25.04 |                                                                                                         LG Toronto AI Research Lab                                                                                                         |                             arxiv                              |                       [The Hidden Space of Safety: Understanding Preference-Tuned LLMs in Multilingual Context](https://arxiv.org/abs/2504.02708)                       |                                  **Multilingual Alignment**&**LLM Safety**&**Representation Analysis**                                  |
| 25.04 |                                                                                                            Zhejiang University                                                                                                             |                             arxiv                              |                           [ERPO: Advancing Safety Alignment via Ex-Ante Reasoning Preference Optimization](https://arxiv.org/abs/2504.02725)                            |                               **LLM Safety Alignment**&**Ex-Ante Reasoning**&**Preference Optimization**                                |
| 25.04 |                                                                                                            Carleton University                                                                                                             |                             arxiv                              |                    [Safety Modulation: Enhancing Safety in Reinforcement Learning through Cost-Modulated Rewards](https://arxiv.org/abs/2504.03040)                     |                                **Safe Reinforcement Learning**&**Constrained MDP**&**Reward Modulation**                                |
| 25.04 |                                                                                                              VMware Research                                                                                                               |                             arxiv                              |                                           [Bypassing Safety Guardrails in LLMs Using Humor](https://arxiv.org/abs/2504.06577)                                           |                                      **LLM Jailbreaking**&**Prompt Engineering**&**Humor Attacks**                                      |
| 25.04 |                                                                                                           University of Warwick                                                                                                            |                             arxiv                              |                                    [Detecting Malicious AI Agents Through Simulated Interactions](https://arxiv.org/abs/2504.03726)                                     |                           **Malicious intent detection**&**Human-AI interaction**&**Manipulation techniques**                           |
| 25.04 |                                                                                                        Chinese Academy of Sciences                                                                                                         |                             arxiv                              |                        [SINCon: Mitigate LLM-Generated Malicious Message Injection Attack for Rumor Detection](https://arxiv.org/abs/2504.07135)                        |                                **Rumor Detection**&**Message Injection Attack**&**Contrastive Learning**                                |
| 25.04 |                                                                                                        University of South Carolina                                                                                                        |                             arxiv                              |             [SafeChat: A Framework for Building Trustworthy Collaborative Assistants and a Case Study of its Usefulness](https://arxiv.org/abs/2504.07995)              |                               **Collaborative Assistant**&**Trustworthy Chatbot**&**SafeChat Framework**                                |
| 25.04 |                                                                                                         Carnegie Mellon University                                                                                                         |                             arxiv                              |                 [SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs](https://arxiv.org/abs/2504.08192)                 |                                      **Machine Unlearning**&**Sparse Autoencoder**&**LLM Safety**                                       |
| 25.04 |                                                                                                       Harbin Institute of Technology                                                                                                       |                             arxiv                              |                               [AdaSteer: Your Aligned LLM is Inherently an Adaptive Jailbreak Defender](https://arxiv.org/abs/2504.09466)                               |                                    **Jailbreak Defense**&**Activation Steering**&**Adaptive Safety**                                    |
| 25.04 |                                                                                                        City University of Hong Kong                                                                                                        |                             arxiv                              |                                           [ControlNet: A Firewall for RAG-based LLM System](https://arxiv.org/abs/2504.09593)                                           |                                          **RAG Security**&**AI Firewall**&**Activation Shift**                                          |
| 25.04 |                                                                                                          Independent Researchers                                                                                                           |                             arxiv                              |                                                  [Mitigating Many-Shot Jailbreaking](https://arxiv.org/abs/2504.09604)                                                  |                                **Many-Shot Jailbreaking**&**Fine-Tuning Defense**&**Input Sanitization**                                |
| 25.04 |                                                                                                                Georgia Tech                                                                                                                |                             arxiv                              |                                            [The Structural Safety Generalization Problem](https://arxiv.org/abs/2504.09712)                                             |                                        **Safety Generalization**&**Jailbreaking**&**Guardrails**                                        |
| 25.04 |                                                                                                             University of Utah                                                                                                             |                             arxiv                              |                                     [Alleviating the Fear of Losing Alignment in LLM Fine-tuning](https://arxiv.org/abs/2504.09757)                                     |                              **LLM Alignment**&**Fine-tuning Recovery**&**Harmful Direction Restoration**                               |
| 25.04 |                                                                                                                UC Berkeley                                                                                                                 |                             arxiv                              |                                       [Progent: Programmable Privilege Control for LLM Agents](https://arxiv.org/abs/2504.11703)                                        |                                       **LLM Agents**&**Privilege Control**&**Policy Programming**                                       |
| 25.04 |                                                                                                                 Microsoft                                                                                                                  |                             arxiv                              |                [AttentionDefense: Leveraging System Prompt Attention for Explainable Defense Against Novel Jailbreaks](https://arxiv.org/abs/2504.12321)                |                                 **Jailbreak Detection**&**Explainable AI**&**System Prompt Attention**                                  |
| 25.04 |                                                                                                       Shanghai Jiao Tong University                                                                                                        |                             arxiv                              |                      [VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization](https://arxiv.org/abs/2504.12661)                      |                                 **Vision-Language Model**&**Safety Alignment**&**Multimodal Reasoning**                                 |
| 25.04 |                                                                                                                Sporo Health                                                                                                                |                             arxiv                              |                              [MCP Guardian: A Security-First Layer for Safeguarding MCP-Based AI System](https://arxiv.org/abs/2504.12757)                              |                                        **Model Context Protocol**&**Agentic AI**&**AI Security**                                        |
| 25.04 |                                                                                                 University of Chinese Academy of Sciences                                                                                                  |                           CVPR 2025                            |                 [Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?](https://arxiv.org/abs/2504.13052)                 |                                        **MLLM Safety**&**Rejection Tuning**&**Compliance Bias**                                         |
| 25.04 |                                                                                                              Fudan University                                                                                                              |                             arxiv                              |        [Concept Enhancement Engineering: A Lightweight and Efficient Robust Defense Against Jailbreak Attacks in Embodied AI](https://arxiv.org/abs/2504.13201)         |                             **Embodied Intelligence**&**Jailbreak Attacks**&**Representation Engineering**                              |
| 25.04 |                                                                                                             Tongji University                                                                                                              |                             arxiv                              |                         [DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention Modification](https://arxiv.org/abs/2504.13562)                         |                            **LLM Jailbreak Defense**&**Attention Modification**&**Inference-Time Strategy**                             |
| 25.04 |                                                                                                      Quebec University at Chicoutimi                                                                                                       |                             arxiv                              |               [A DATA-CENTRIC APPROACH FOR SAFE AND SECURE LARGE LANGUAGE MODELS AGAINST THREATENING AND TOXIC CONTENT](https://arxiv.org/abs/2504.16120)               |                                  **Toxicity Mitigation**&**Post-generation Correction**&**LLM Safety**                                  |
| 25.04 |                                                                                                                   Intuit                                                                                                                   |                             arxiv                              |                              [Building A Secure Agentic AI Application Leveraging Google‚Äôs A2A Protocol](https://arxiv.org/abs/2504.16902)                              |                                       **Agentic AI**&**A2A Protocol**&**MAESTRO Threat Modeling**                                       |
| 25.04 |                                                                                                            Zhejiang University                                                                                                             |                             arxiv                              |               [DUALBREACH: Efficient Dual-Jailbreaking via Target-Driven Initialization and Multi-Target Optimization](https://arxiv.org/abs/2504.18564)                |                                         **Jailbreaking**&**Guardrails**&**Adversarial Attack**                                          |
| 25.04 |                                                                                                              Microsoft India                                                                                                               |                             arxiv                              |                                         [SAGE: A Generic Framework for LLM Safety Evaluation](https://arxiv.org/abs/2504.19674)                                         |                                **LLM Safety Evaluation**&**Adversarial Testing**&**Multi-turn Dialogue**                                |
| 25.04 |                                                                                                            Chongqing University                                                                                                            |                             arxiv                              |                [Evaluate-and-Purify: Fortifying Code Language Models Against Adversarial Attacks Using LLM-as-a-Judge](https://arxiv.org/abs/2504.19730)                |                                    **Adversarial Attack**&**Code Language Model**&**LLM-as-a-Judge**                                    |
| 25.04 |                                                                                                      National University of Singapore                                                                                                      |                             arxiv                              |           [Robustness via Referencing: Defending against Prompt Injection Attacks by Referencing the Executed Instruction](https://arxiv.org/abs/2504.20472)            |                                   **Prompt Injection**&**Instruction Referencing**&**LLM Robustness**                                   |
| 25.04 |                                                                                                      Beijing Institute of Technology                                                                                                       |                           ACM MM‚Äô25                            |                          [AGATE: Stealthy Black-box Watermarking for Multimodal Model Copyright Protection](https://arxiv.org/abs/2504.21044)                           |                           **Black-box Watermarking**&**Model Copyright Protection**&**Watermarking Security**                           |
| 25.05 |                                                                                                          University of Cincinnati                                                                                                          |                             arxiv                              |                         [Jailbreak Detection in Clinical Training LLMs Using Feature-Based Predictive Models](https://arxiv.org/abs/2505.00010)                         |                                 **Jailbreak Detection**&**Clinical Simulation**&**Fuzzy Decision Tree**                                 |
| 25.05 |                                                                                                    University of California, Santa Cruz                                                                                                    |                             arxiv                              |                                        [Large Language Models are Autonomous Cyber Defenders](https://arxiv.org/abs/2505.04843)                                         |                                   **Autonomous Cyber Defense**&**LLM Agents**&**Multi-Agent Systems**                                   |
| 25.05 |                                                                                                             Nanjing University                                                                                                             |                             arxiv                              |                               [Defending against Indirect Prompt Injection by Instruction Detection](https://arxiv.org/abs/2505.06311v1)                                |                          **Indirect Prompt Injection**&**Instruction Detection**&**Behavioral State Analysis**                          |
| 25.05 |                                                                                                          University of Cambridge                                                                                                           |                             arxiv                              |                                     [Adversarial Suffix Filtering: a Defense Pipeline for LLMs](https://arxiv.org/abs/2505.09602v1)                                     |                                **Adversarial Suffix**&**Jailbreak Defense**&**Model-Agnostic Filtering**                                |
| 25.05 |                                                                                                            Saarland University                                                                                                             |                             arxiv                              |                               [ProxyPrompt: Securing System Prompts against Prompt Extraction Attacks](https://arxiv.org/abs/2505.11459)                                |                                   **Prompt Extraction**&**System Prompt Protection**&**LLM Security**                                   |
| 25.05 |                                                                                                              Fudan University                                                                                                              |                             arxiv                              |                                    [SAFEVID: Toward Safety Aligned Video Large Multimodal Models](https://arxiv.org/abs/2505.11926)                                     |                              **Video Multimodal Models**&**Safety Alignment**&**Preference Optimization**                               |
| 25.05 |                                                                                                          George Mason University                                                                                                           |                             arxiv                              |                          [Web IP at Risk: Prevent Unauthorized Real-Time Retrieval by Large Language Models](https://arxiv.org/abs/2505.12655)                          |                                    **Web IP Protection**&**LLM Anti-Retrieval**&**Semantic Defense**                                    |
| 25.05 |                                                                                                                ARIMLABS.AI                                                                                                                 |                             arxiv                              |                                              [The Hidden Dangers of Browsing AI Agents](https://arxiv.org/abs/2505.13076)                                               |                                    **Browsing Agents**&**Prompt Injection**&**Multi-Layer Security**                                    |
| 25.05 |                                                                                     Institute of Information Engineering, Chinese Academy of Sciences                                                                                      |                             arxiv                              |                      [EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.13506)                      |                            **RAG Security**&**Contextual Diversity Detection**&**Corpus Poisoning Defense**                             |
| 25.05 |                                                                                               Beijing Institute of AI Safety and Governance                                                                                                |                             arxiv                              |                            [PANDAGUARD: Systematic Evaluation of LLM Safety against Jailbreaking Attacks](https://arxiv.org/abs/2505.13862)                             |                                **Jailbreak Evaluation**&**Multi-Agent System**&**LLM Safety Benchmark**                                 |
| 25.05 |                                                                                                            University of Oxford                                                                                                            |                             arxiv                              |                     [SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors](https://arxiv.org/abs/2505.14300)                     |                             **Harmful Output Detection**&**Backdoor Behavior**&**Unsupervised Monitoring**                              |
| 25.05 |                                                                                                                   HKUST                                                                                                                    |                             arxiv                              |                                 [MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol](https://arxiv.org/abs/2505.14590)                                 |                                          **MCP**&**Contextual Integrity**&**Safety Benchmark**                                          |
| 25.05 |                                                                                                             Yonsei University                                                                                                              |                             arxiv                              |                           [SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment](https://arxiv.org/abs/2505.14667)                            |                               **Safety Alignment**&**Chain-of-Thought Reasoning**&**Early Intervention**                                |
| 25.05 |                                                                                                            Tsinghua University                                                                                                             |                             arxiv                              |                          [How Should We Enhance the Safety of Large Reasoning Models: An Empirical Study](https://arxiv.org/abs/2505.15404v1)                           |                                 **Large Reasoning Models**&**Safety Fine-Tuning**&**Chain-of-Thought**                                  |
| 25.05 |                                                                                                             Peking University                                                                                                              |                             arxiv                              |                                  [Advancing LLM Safe Alignment with Safety Representation Ranking](https://arxiv.org/abs/2505.15710v1)                                  |                                  **Safety Alignment**&**Representation Learning**&**Response Ranking**                                  |
| 25.05 |                                                                                                             Peking University                                                                                                              |                             arxiv                              |                      [Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval](https://arxiv.org/abs/2505.15753v1)                      |                                 **Jailbreaking Defense**&**Safety Context Retrieval**&**LLM Security**                                  |
| 25.05 |                                                                                                               UC Santa Cruz                                                                                                                |                             arxiv                              |                                   [SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning](https://arxiv.org/abs/2505.16186v1)                                    |                                     **Safety Alignment**&**Jailbreak Defense**&**Reasoning Models**                                     |
| 25.05 |                                                                                                             Nankai University                                                                                                              |                             arxiv                              |                    [CTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning](https://arxiv.org/abs/2505.16559v1)                     |                                  **Fine-tuning Security**&**Collapse Trap**&**LLM Alignment Defense**                                   |
| 25.05 |                                                                                                             Peking University                                                                                                              |                             arxiv                              |                            [Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization](https://arxiv.org/abs/2505.16737v1)                             |                                    **Fine-tuning Risk**&**Safety Optimization**&**Gradient Probing**                                    |
| 25.05 |                                                                                                              Wuhan University                                                                                                              |                             arxiv                              |                                  [Backdoor Cleaning without External Guidance in MLLM Fine-tuning](https://arxiv.org/abs/2505.16916v1)                                  |                                     **Backdoor Defense**&**Multimodal LLMs**&**Attention Entropy**                                      |
| 25.05 |                                                                                                        East China Normal University                                                                                                        |                       ACL 2025 Findings                        |             [Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models](https://arxiv.org/abs/2505.16104v1)             |                                        **Model Pruning**&**LVLM Safety**&**Neuron Restoration**                                         |
| 25.05 |                                                                                                             Nanjing University                                                                                                             |                       ACL 2025 Findings                        |                [Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement](https://arxiv.org/abs/2505.12060v1)                 |                                 **LLM Safety**&**Jailbreak Defense**&**Discrimination-Generation Gap**                                  |
| 25.05 |                                                                                                       Harbin Institute of Technology                                                                                                       |                            ACL 2025                            |                                  [MPO: Multilingual Safety Alignment via Reward Gap Optimization](https://arxiv.org/abs/2505.16869v1)                                   |                                  **Multilingual Safety**&**Reward Gap Optimization**&**LLM Alignment**                                  |
| 25.05 |                                                                                               Southern University of Science and Technology                                                                                                |                           ICML 2025                            |                       [Safe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on Diverse Datasets](https://arxiv.org/abs/2505.12038v1)                        |                                      **Safety Fine-Tuning**&**Delta Optimization**&**LLM Defense**                                      |
| 25.05 |                                                                                                      North Carolina State University                                                                                                       |                           ICML 2025                            |                                [Safety Alignment Can Be Not Superficial With Explicit Safety Signals](https://arxiv.org/abs/2505.17072)                                 |                                        **Safety Alignment**&**LLMs**&**Binaray Classification**                                         |
| 25.05 |                                                                                                                   Impel                                                                                                                    |                             arxiv                              |                           [Improving LLM Outputs Against Jailbreak Attacks with Expert Model Integration](https://arxiv.org/abs/2505.17066v1)                           |                                 **Jailbreak Attacks**&**Expert Model Integration**&**Prompt Injection**                                 |
| 25.05 |                                                                                                      Beijing Institute of Technology                                                                                                       |                             arxiv                              |                                [RRTL: Red Teaming Reasoning Large Language Models in Tool Learning](https://arxiv.org/abs/2505.17106v1)                                 |                                          **Red Teaming**&**Reasoning LLMs**&**Tool Learning**                                           |
| 25.05 |                                                                                                            Tsinghua University                                                                                                             |                             arxiv                              |                           [Does Chain-of-Thought Reasoning Really Reduce Harmfulness from Jailbreaking?](https://arxiv.org/abs/2505.17650v1)                            |                                **Chain-of-Thought Reasoning**&**Jailbreaking**&**Harmfulness Modeling**                                 |
| 25.05 |                                                                                                              Duke University                                                                                                               |                             arxiv                              |                                [A Critical Evaluation of Defenses against Prompt Injection Attacks](https://arxiv.org/abs/2505.18333v1)                                 |                                     **Prompt Injection**&**Defense Evaluation**&**Adaptive Attack**                                     |
| 25.05 |                                                                                                 Harbin Institute of Technology (Shenzhen)                                                                                                  |                             arxiv                              |                                        [Safety Alignment via Constrained Knowledge Unlearning](https://arxiv.org/abs/2505.18588)                                        |                                   **Safety Alignment**&**Knowledge Unlearning**&**Jailbreak Defense**                                   |
| 25.05 |                                                                                             Beijing University of Posts and Telecommunications                                                                                             |                             arxiv                              |      [PD3F: A Pluggable and Dynamic DoS-Defense Framework Against Resource Consumption Attacks Targeting Large Language Models](https://arxiv.org/abs/2505.18680)       |                                **DoS Defense**&**Resource Consumption Attack**&**Large Language Models**                                |
| 25.05 |                                                                                         King Abdullah University of Science and Technology (KAUST)                                                                                         |                             arxiv                              |                                 [An Embarrassingly Simple Defense Against LLM Abliteration Attacks](https://arxiv.org/abs/2505.19056v1)                                 |                                     **Abliteration Attack**&**Extended Refusal**&**LLM Alignment**                                      |
| 25.05 |                                                                                                           King‚Äôs College London                                                                                                            |                             arxiv                              |                        [GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling](https://arxiv.org/abs/2505.19234v1)                         |                             **Multi-Agent Collaboration**&**Safety Detection**&**Temporal Graph Modeling**                              |
| 25.05 |                                                                                                             Sichuan University                                                                                                             |                             arxiv                              |            [ALRPHFS: Adversarially Learned Risk Patterns with Hierarchical Fast & Slow Reasoning for Robust Agent Defense](https://arxiv.org/abs/2505.19260)            |                                  **Agent Defense**&**Adversarial Learning**&**Hierarchical Reasoning**                                  |
| 25.05 |                                                                                                    University of Maryland, College Park                                                                                                    |                             arxiv                              |                   [CoTGuard: Using Chain-of-Thought Triggering for Copyright Protection in Multi-Agent LLM Systems](https://arxiv.org/abs/2505.19405)                   |                                    **Copyright Protection**&**Chain-of-Thought**&**Multi-Agent LLM**                                    |
| 25.05 |                                                                                                                   NVIDIA                                                                                                                   |                             arxiv                              |                            [Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models](https://arxiv.org/abs/2505.20087v1)                             |                                       **Reasoning Guardrails**&**LLM Safety**&**Data Efficiency**                                       |
| 25.05 |                                                                                                                 Sea AI Lab                                                                                                                 |                             arxiv                              |                                            [Lifelong Safety Alignment for Language Models](https://arxiv.org/abs/2505.20259)                                            |                                  **Lifelong Safety Alignment**&**Jailbreak Attack**&**Meta-Attacker**                                   |
| 25.05 |                                                                                                          University of Melbourne                                                                                                           |                           ICLR 2025                            |                      [MULTI-LEVEL CERTIFIED DEFENSE AGAINST POISONING ATTACKS IN OFFLINE REINFORCEMENT LEARNING](https://arxiv.org/abs/2505.20621)                      |                              **Certified Defense**&**Poisoning Attack**&**Offline Reinforcement Learning**                              |
| 25.05 |                                                                                     NLPR & MAIS, Institute of Automation, Chinese Academy of Sciences                                                                                      |                             arxiv                              |          [Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models](https://arxiv.org/abs/2505.22271v1)          |                                     **Jailbreak Defense**&**Test-Time Learning**&**Multimodal LLM**                                     |
| 25.05 |                                                                                                                 SentinelAI                                                                                                                 |                             arxiv                              |                           [Operationalizing CaMeL: Strengthening LLM Defenses for Enterprise Deployment](https://arxiv.org/abs/2505.22852v1)                            |                                      **LLM Defense**&**Enterprise Security**&**Prompt Injection**                                       |
| 25.05 |                                                                                                             Beihang University                                                                                                             |                             arxiv                              |                    [Detecting Stealthy Backdoor Samples based on Intra-class Distance for Large Language Models](https://arxiv.org/abs/2505.23015v1)                    |                                      **Backdoor Detection**&**LLM Security**&**TF-IDF Clustering**                                      |
| 25.05 |                                                                                                                   Leidos                                                                                                                   |                             arxiv                              |              [MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits using Improved Preference Alignment](https://arxiv.org/abs/2505.23634v1)              |                                    **MCP Safety**&**Falsely Benign Attack**&**Preference Alignment**                                    |
| 25.05 |                                                                                                                 Microsoft                                                                                                                  |                             arxiv                              |                                         [Securing AI Agents with Information-Flow Control](https://arxiv.org/abs/2505.23643v1)                                          |                              **Information-Flow Control**&**Agent Security**&**Prompt Injection Defense**                               |
| 25.05 |                                                                                     Institute of Information Engineering, Chinese Academy of Sciences                                                                                      |                            ACL 2025                            |              [AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models](https://arxiv.org/abs/2505.23020v1)               |                                        **Agent Alignment**&**Safety Alignment**&**Agentic LLM**                                         |
| 25.05 |                                                                                                  Harbin Institute of Technology, Shenzhen                                                                                                  |                            ACL 2025                            |                            [MTSA: Multi-turn Safety Alignment for LLMs through Multi-round Red-teaming](https://arxiv.org/abs/2505.17147v1)                             |                                         **Multi-turn Alignment**&**Red-teaming**&**LLM Safety**                                         |
| 25.05 |                                                                                                   University of California, Los Angeles                                                                                                    |                             arxiv                              |                       [Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap](https://arxiv.org/abs/2505.24208v1)                       |                             **Vision-Language Model**&**Safety Alignment**&**Modality Gap Regularization**                              |
| 25.05 |                                                                                                                 ETH Z√ºrich                                                                                                                 |                           ICML 2025                            |                                       [Learning Safety Constraints for Large Language Models](https://arxiv.org/abs/2505.24445v1)                                       |                                     **LLM Safety**&**Geometric Constraint**&**Adversarial Defense**                                     |
| 25.06 |                                                                                                       Technical University of Munich                                                                                                       |                             arxiv                              |                        [SafeCOMM: What about Safety Alignment in Fine-Tuned Telecom Large Language Models?](https://arxiv.org/abs/2506.00062v1)                         |                                      **Telecom LLM**&**Safety Alignment**&**Fine-Tuning Defense**                                       |
| 25.06 |                                                                                                              Duke University                                                                                                               |                             arxiv                              |                                  [SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues](https://arxiv.org/abs/2506.00668v1)                                  |                                   **LLM Safety**&**Multi-Turn Attack**&**Safety Reasoning Moderator**                                   |
| 25.06 |                                                                                                   Critical ML Lab, University of Toronto                                                                                                   |                             arxiv                              |                            [SafeTuneBed: A Toolkit for Benchmarking LLM Safety Alignment in Fine-Tuning](https://arxiv.org/abs/2506.00676v1)                            |                                      **LLM Safety**&**Fine-Tuning Defense**&**Benchmark Toolkit**                                       |
| 25.06 |                                                                                                             Peking University                                                                                                              |                             arxiv                              |                           [ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs](https://arxiv.org/abs/2506.01770v1)                            |                                 **LLM Safety**&**Model-Based Analysis**&**Representation Engineering**                                  |
| 25.06 |                                                                                                         Michigan State University                                                                                                          |                             arxiv                              |                    [Attention Knows Whom to Trust: Attention-based Trust Management for LLM Multi-Agent Systems](https://arxiv.org/abs/2506.02546v1)                    |                                   **LLM Multi-Agent**&**Trustworthiness**&**Attention-based Defense**                                   |
| 25.06 |                                                                                                      University of Wisconsin-Madison                                                                                                       |                             arxiv                              |                                 [Through the Stealth Lens: Rethinking Attacks and Defenses in RAG](https://arxiv.org/abs/2506.04390v1)                                  |                                        **RAG**&**Stealthy Attack**&**Attention-Variance Filter**                                        |
| 25.06 |                                                                                                     Washington University in St. Louis                                                                                                     |                       ACL 2025 Findings                        |                              [COSMIC: Generalized Refusal Direction Identification in LLM Activations](https://arxiv.org/abs/2506.00085v1)                              |                                  **LLM Safety**&**Refusal Direction**&**Mechanistic Interpretability**                                  |
| 25.06 |                                                    Institute of Information Engineering, Chinese Academy of Sciences; Nanyang Technological University; Sun Yat-sen University-Shenzhen                                                    |                             arxiv                              |                                          [Robust Anti-Backdoor Instruction Tuning in LVLMs](https://arxiv.org/abs/2506.05401)                                           |                                          **LVLM**&**Backdoor Defense**&**Instruction Tuning**                                           |
| 25.06 |                                                                                                                 Qualifire                                                                                                                  |                             arxiv                              |                                      [SENTINEL: SOTA Model to Protect Against Prompt Injections](https://arxiv.org/abs/2506.05446)                                      |                                   **Prompt Injection**&**LLM Security**&**ModernBERT**&**Jailbreak**                                    |
| 25.06 |                                                                                                           Luxembourg Tech School                                                                                                           |                             arxiv                              |             [From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law](https://arxiv.org/abs/2506.06391v1)             | **Generative AI Ethics**&**Harmful Content Mitigation**&**International Humanitarian Law**&**Prompt Engineering**&**Refusal Behaviour** |
| 25.06 |                                                                          University of Southern California, University of California Berkeley, Peking University                                                                           |                             arxiv                              |                                    [Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test](https://arxiv.org/abs/2506.06975)                                    |                         **LLM Auditing**&**API Security**&**Model Equality Testing**&**Adversarial Detection**                          |
| 25.06 |                                                              National University of Singapore, University of Science and Technology of China, Harbin Institute of Technology                                                               |                             arxiv                              |                             [AlphaSteer: Learning Refusal Steering with Principled Null-Space Constraint](https://arxiv.org/abs/2506.07022)                             |                          **Refusal Steering**&**Activation Steering**&**Null-Space Constraint**&**LLM Safety**                          |
| 25.06 |                                                                                         Korea Advanced Institute of Science and Technology (KAIST)                                                                                         |                             arxiv                              |                 [Refusal-Feature-guided Teacher for Safe Finetuning via Data Filtering and Alignment Distillation](https://arxiv.org/abs/2506.07356v1)                  |                  **Safe Finetuning**&**Refusal Feature**&**Data Filtering**&**Alignment Distillation**&**LLM Safety**                   |
| 25.06 |                                                                                                   Massachusetts Institute of Technology                                                                                                    |                             arxiv                              |                       [When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment](https://arxiv.org/abs/2506.07452)                       |                                **LLM Safety**&**Style Alignment**&**Jailbreak**&**Fine-Tuning Defense**                                 |
| 25.06 |                                       Texas A&M University, UC San Diego, UC Irvine, University of Wisconsin‚ÄìMadison, Carnegie Mellon University, University of Michigan, Columbia University, Meta                                        |                             arxiv                              |                    [SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems](https://arxiv.org/abs/2506.07564v3)                     |                     **Agent System**&**LLM/VLM Safety**&**Information Flow Control**&**Concurrency**&**Benchmark**                      |
| 25.06 |                                  National University of Singapore, Cornell University, University of Electronic Science and Technology, Harbin Institute of Technology, Nanyang Technological University                                   |                             arxiv                              |                        [RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards](https://arxiv.org/abs/2506.07736)                         |                       **LLM Safeguard**&**Reasoning Alignment**&**Guard Model**&**RL**&**Safety Generalization**                        |
| 25.06 |                                                                                               Stony Brook University, Penn State University                                                                                                |                             arxiv                              |                                       [Your Agent Can Defend Itself against Backdoor Attacks](https://arxiv.org/abs/2506.08336v2)                                       |                       **LLM Agent**&**Backdoor Attack**&**Self-Defense**&**Consistency Check**&**Agent Security**                       |
| 25.06 |                                                                                                  Shanghai AI Laboratory, Fudan University                                                                                                  |                             arxiv                              |                                       [SAFECOT: Improving VLM Safety with Minimal Reasoning](https://arxiv.org/abs/2506.08399v2)                                        |                            **VLM Safety**&**Chain-of-Thought**&**Refusal Behavior**&**Minimal Supervision**                             |
| 25.06 |                                                                                                     UC Santa Barbara, UC San Francisco                                                                                                     |                             arxiv                              |                        [Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations](https://arxiv.org/abs/2506.09067v1)                         |        **Medical Vision-Language Model**&**Safety Alignment**&**Synthetic Demonstration**&**Jailbreak Defense**&**Over-Defense**        |
| 25.06 |                                                                                         Tsinghua University, Beihang University, Peking University                                                                                         |                             arxiv                              |                  [DAVSP: Safety Alignment for Large Vision-Language Models via Deep Aligned Visual Safety Prompt](https://arxiv.org/abs/2506.09353v1)                   |                           **LVLM Safety**&**Visual Safety Prompt**&**Deep Alignment**&**Adversarial Defense**                           |
| 25.06 |                                                                               The Hong Kong University of Science and Technology, Renmin University of China                                                                               |                             arxiv                              |                                  [SoK: Evaluating Jailbreak Guardrails for Large Language Models](https://arxiv.org/abs/2506.10597v1)                                   |                       **Jailbreak Guardrail**&**LLM Security**&**Defense Evaluation**&**Taxonomy**&**Benchmark**                        |
| 25.06 |                                                                                       New York University, √âcole Polytechnique F√©d√©rale de Lausanne                                                                                        |                             arxiv                              |                           [Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors](https://arxiv.org/abs/2506.10949v1)                           |              **Decomposition Attack**&**Sequential Monitor**&**LLM Safety**&**Prompt Obfuscation**&**Defense Evaluation**               |
| 25.06 |                                                                                   Bytedance; Northern Arizona University; Pennsylvania State University                                                                                    |                            DSN 2025                            |                       [To Protect the LLM Agent Against the Prompt Injection Attack with Polymorphic Prompt](https://arxiv.org/abs/2506.05739v1)                        |                    **LLM**&**Prompt Injection**&**Agent Defense**&**Polymorphic Prompt**&**Separator Randomization**                    |
| 25.06 |                                                                                                            University of Surrey                                                                                                            |                             arxiv                              |                 [SecurityLingua: Efficient Defense of LLM Jailbreak Attacks via Security-Aware Prompt Compression](https://arxiv.org/abs/2506.12707v1)                  |                 **Jailbreak Defense**&**Prompt Compression**&**Security-Aware LLM**&**Token Efficiency**&**LLM Safety**                 |
| 25.06 |                                                                                                            Tel Aviv University                                                                                                             |                             arxiv                              |                                    [Universal Jailbreak Suffixes Are Strong Attention Hijackers](https://arxiv.org/abs/2506.12880v1)                                    |                   **Jailbreak**&**Universal Suffix**&**Attention Hijacking**&**LLM Security**&**Prompt Engineering**                    |
| 25.06 |                                                                                                          University of S√£o Paulo                                                                                                           |                             arxiv                              |                              [LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning](https://arxiv.org/abs/2506.15606v1)                               |                 **LLM Safety**&**Fine-tuning Robustness**&**Low-Rank Extrapolation**&**Alignment**&**Safety Subspace**                  |
| 25.06 |                                                                                                                FnGuide Inc.                                                                                                                |                         ACL 2025 WOAH                          |                                 [QGuard: Question-based Zero-shot Guard for Multi-modal LLM Safety](https://arxiv.org/abs/2506.12299v1)                                 |           **Harmful Prompt Detection**&**Multi-modal LLM**&**Zero-shot Guard**&**Question Prompting**&**White-box Analysis**            |
| 25.06 |                                                                                                         Xi‚Äôan Jiaotong University                                                                                                          |                             arxiv                              |                [The Safety Reminder: A Soft Prompt to Reactivate Delayed Safety Awareness in Vision-Language Models](https://arxiv.org/abs/2506.15734v1)                |                                         **Vision-Language Model**&**Safety**&**Prompt Tuning**                                          |
| 25.06 |                                                                                                             Queen's University                                                                                                             |                             arxiv                              |                           [Security Assessment of DeepSeek and GPT Series Models against Jailbreak Attacks](https://arxiv.org/abs/2506.18543)                           |                                     **Large Language Models**&**Jailbreak Attacks**&**AI Security**                                     |
| 25.06 |                                                                                             Beijing University of Posts and Telecommunications                                                                                             |                             arxiv                              |                [MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection](https://arxiv.org/abs/2506.18919)                |                                 **Harmful Meme Detection**&**Multimodal Dataset**&**Chain-of-Thought**                                  |
| 25.06 |                                                                                                             Nankai University                                                                                                              |                           ICLR 2025                            |              [Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models](https://arxiv.org/abs/2506.16447v1)              |                                          **Backdoor Defense**&**Black-box LLM**&**Alignment**                                           |
| 25.06 |                                                                                                          University of Luxembourg                                                                                                          |                             arxiv                              |                       [Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models](https://arxiv.org/abs/2506.23576v1)                       |                                       **Jailbreaking Defense**&**Multi-Agent LLM**&**Evaluation**                                       |
| 25.06 |                                                                                               FAR.AI, UK AISI, OATML (University of Oxford)                                                                                                |                             arxiv                              |                                       [STACK: Adversarial Attacks on LLM Safeguard Pipelines](https://arxiv.org/abs/2506.24068v1)                                       |                                        **Jailbreak**&**Defense Pipeline**&**Adversarial Attack**                                        |
| 25.07 |                    Nanyang Technological University, Institute of Software (Chinese Academy of Sciences), Beihang University, Sun Yat-sen University, Beijing Institute of Technology, National University of Singapore                    |                             arxiv                              |                 [SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents](https://arxiv.org/abs/2507.00841v1)                 |                                          **Mobile Agent**&**Jailbreak Defense**&**Multimodal**                                          |
| 25.07 |                                                                                                         Carnegie Mellon University                                                                                                         |                             arxiv                              |                                            [Reasoning as an Adaptive Defense for Safety](https://arxiv.org/abs/2507.00971v1)                                            |                                                 **Reasoning**&**Safety Defense**&**RL**                                                 |
| 25.07 | Southwestern University of Finance and Economics, University of Electronic Science and Technology of China, Center for Future Media (UESTC), Tongji University, Engineering Research Center of Intelligent Finance (Ministry of Education) |                             arxiv                              |                    [SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via Prune-then-Restore Mechanism](https://arxiv.org/abs/2507.01513v1)                     |                                       **Jailbreak Defense**&**Multimodal LLM**&**Token Pruning**                                        |
| 25.07 |                                                                 Meta FAIR, Universit√© Paris-Saclay, Inria, CNRS, INSA Rouen Normandy, LITIS, NYU Courant Institute and CDS                                                                 |                             arxiv                              |                     [Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training](https://arxiv.org/abs/2507.01752v1)                      |                                        **Black-Box Optimization**&**Privacy**&**Generalization**                                        |
| 25.07 |                                                                                                           Meta FAIR, UC Berkeley                                                                                                           |                             arxiv                              |                              [Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks](https://arxiv.org/abs/2507.02735v1)                              |                                 **Prompt Injection**&**Model-level Defense**&**Instruction Hierarchy**                                  |
| 25.07 |                                                                                                                   KAIST                                                                                                                    |                       ACL 2025 Findings                        |                                     [Enhancing LLM Agent Safety via Causal Influence Prompting](https://arxiv.org/abs/2507.00979v1)                                     |                                           **Safety Prompting**&**Causal Influence**&**Agent**                                           |
| 25.07 |                                                                                                           BUPT, HKBU, NUS, HKUST                                                                                                           |                            ACL 2025                            |               [AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness](https://arxiv.org/abs/2507.01702v1)                |                                       **Meme Harmfulness**&**Reasoning Probe**&**Multimodal LLM**                                       |
| 25.07 |                                                                                                         Carnegie Mellon University                                                                                                         |                           ICCV 2025                            |                [ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2507.00898v1)                 |                                        **Hallucination**&**Vision-Language Model**&**Decoding**                                         |
| 25.07 |                                                                                            Nanyang Technological University, Beihang University                                                                                            |                           ICML 2025                            |                             [ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks](https://arxiv.org/abs/2507.01321v1)                              |                                         **In-Context Learning**&**Backdoor Attack**&**Defense**                                         |
| 25.07 |                                                                                                    The Chinese University of Hong Kong                                                                                                     |                             arxiv                              |                     [Attention Slipping: A Mechanistic Understanding of Jailbreak Attacks and Defenses in LLMs](https://arxiv.org/abs/2507.04365v1)                     |                                            **Jailbreak**&**Attention Mechanism**&**Defense**                                            |
| 25.07 |                                                                                                      University of Wisconsin-Madison                                                                                                       |                             arxiv                              |                                          [How Not to Detect Prompt Injections with an LLM](https://arxiv.org/abs/2507.05630v1)                                          |                                               **Prompt Injection**&**Detection**&**LLM**                                                |
| 25.07 |                                                                                                             Peking University                                                                                                              |                             arxiv                              |                          [Enhancing LLM Watermark Resilience Against Both Scrubbing and Spoofing Attacks](https://arxiv.org/abs/2507.06274v1)                           |                                           **Watermarking**&**Scrubbing Attack**&**Spoofing**                                            |
| 25.07 |                                                                                                         University of North Texas                                                                                                          |                           COLM 2025                            |             [Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation](https://arxiv.org/abs/2507.07307v1)              |                                                **Counterspeech**&**RAG**&**Multi-Agent**                                                |
| 25.07 |                                                                                                     University of California, Berkeley                                                                                                     |                             arxiv                              |                         [Detection is Not Enough: Defending Against Jailbreaks with Restrictive Generation](https://arxiv.org/abs/2507.07735v1)                         |                                    **Jailbreak Defense**&**Restrictive Decoding**&**LLM Alignment**                                     |
| 25.07 |                                                                                                             Beihang University                                                                                                             |                             arxiv                              |                           [From Red to Green: Aligning Large Language Models via Jailbreak Sample Reuse](https://arxiv.org/abs/2507.07810v1)                            |                                          **Jailbreak Defense**&**Alignment**&**Sample Reuse**                                           |
| 25.07 |                                                                                                            Tsinghua University                                                                                                             |                             arxiv                              |                         [Ctrl-Z for AI Models: Stealthy Backdoor Removal from the Lens of Data Attribution](https://arxiv.org/abs/2507.07916v1)                         |                                          **Backdoor Removal**&**Data Attribution**&**Stealth**                                          |
| 25.07 |                                                                                                            Stanford University                                                                                                             |                             arxiv                              |                           [Beyond Prompt Injection: Jailbreaking via Adversarial Instruction Extraction](https://arxiv.org/abs/2507.07974v1)                            |                                      **Jailbreaking**&**Instruction Extraction**&**LLM Security**                                       |
| 25.05 |                                                                                                     University of Southern California                                                                                                      |                           ICML 2025                            |                            [LLM Auto Repair Shop: Repurposing Jailbreak Attacks for Improving LLM Safety](https://arxiv.org/abs/2505.06943)                             |                                          **Jailbreaking**&**Safety Training**&**Repurposing**                                           |
| 25.07 |                                                                                                                 Ant Group                                                                                                                  |                             arxiv                              |                                          [Agent Safety Alignment via Reinforcement Learning](https://arxiv.org/abs/2507.08270)                                          |                                                       **Agent**&**Safety**&**RL**                                                       |
| 25.07 |                                                                                                                   Apple                                                                                                                    |                             arxiv                              |                         [Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training](https://arxiv.org/abs/2507.08284)                         |                                                 **Safety**&**Guardrail**&**Synthetic**                                                  |
| 25.07 |                                                                                               University of Science and Technology of China                                                                                                |                             arxiv                              |                                    [BURN: Backdoor Unlearning via Adversarial Boundary Analysis](https://arxiv.org/abs/2507.10491v1)                                    |                                                 **Backdoor**&**Unlearning**&**Defense**                                                 |
| 25.07 |                                                                                                      University of Wisconsin-Madison                                                                                                       |                             arxiv                              |                          [ARMOR: Aligning Secure and Safe Large Language Models via Meticulous Reasoning](https://arxiv.org/abs/2507.11500v1)                           |                                                   **LLM**&**Alignment**&**Jailbreak**                                                   |
| 25.07 |                                                                                                              Brown University                                                                                                              |                             arxiv                              |              [Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models](https://arxiv.org/abs/2507.12428v1)               |                                                  **Alignment**&**Monitoring**&**CoT**                                                   |
| 25.07 |                                                                                                            Zhejiang University                                                                                                             |                             arxiv                              |                                   [Automating Steering for Safe Multimodal Large Language Models](https://arxiv.org/abs/2507.13255v1)                                   |                                                    **MLLM**&**Safety**&**Steering**                                                     |
| 25.07 |                                                                                               University of Science and Technology of China                                                                                                |                             arxiv                              |                    [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/abs/2507.14987v1)                    |                                         **Safety Alignment**&**Reinforcement Learning**&**LLM**                                         |
| 25.07 |                                                                                                                  Modulabs                                                                                                                  |                             arxiv                              |                               [SIA: Enhancing Safety via Intent Awareness for Vision-Language Models](https://arxiv.org/abs/2507.16856v1)                               |                                                 **Safety**&**Intent Awareness**&**VLM**                                                 |
| 25.07 |                                                                                                   University of California, Los Angeles                                                                                                    |                             arxiv                              |                                    [LoRA is All You Need for Safety Alignment of Reasoning LLMs](https://arxiv.org/abs/2507.17075v1)                                    |                                               **Safety Alignment**&**LoRA**&**Reasoning**                                               |
| 25.07 |                                                                                                             Xidian University                                                                                                              |                             arxiv                              |               [LLM Meets the Sky: Heuristic Multi-Agent Reinforcement Learning for Secure Heterogeneous UAV Networks](https://arxiv.org/abs/2507.17188v1)               |                                             **UAV**&**Security**&**Reinforcement Learning**                                             |
| 25.07 |                                                                                                Shanghai Artificial Intelligence Laboratory                                                                                                 |                             arxiv                              |                 [Layer-Aware Representation Filtering: Purifying Finetuning Data to Preserve LLM Safety Alignment](https://arxiv.org/abs/2507.18631v1)                  |                                             **Safety Alignment**&**Data Filtering**&**LLM**                                             |
| 25.07 |                                                                                                                  POSTECH                                                                                                                   |                       ACL 2025 Findings                        |      [Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection](https://arxiv.org/abs/2507.18202v1)       |                                               **RAG**&**Poisoning Defense**&**Detection**                                               |
| 25.07 |                                                                                                         Universiti Sains Malaysia                                                                                                          |                        EAI ICDF2C 2025                         |           [PhishIntentionLLM: Uncovering Phishing Website Intentions through Multi-Agent Retrieval-Augmented Generation](https://arxiv.org/abs/2507.15419v1)            |                                                      **Phishing**&**LLM**&**RAG**                                                       |
| 25.07 |                                                                                                         Shanghai Normal University                                                                                                         |                             arxiv                              |       [Self-Aware Safety Augmentation: Leveraging Internal Semantic Understanding to Enhance Safety in Vision-Language Models](https://arxiv.org/abs/2507.21637)        |                                                   **Safety**&**LVLM**&**Projection**                                                    |
| 25.07 |                                                                                                             Swarthmore College                                                                                                             |                             arxiv                              |                                [Promoting Online Safety by Simulating Unsafe Conversations with LLMs](https://arxiv.org/abs/2507.22267)                                 |                                                 **OnlineSafety**&**Simulation**&**LLM**                                                 |
| 25.07 |                                                                                                        The University of Manchester                                                                                                        |                             arxiv                              | [Adversarial Defence without Adversarial Defence: Enhancing Language Model Robustness via Instance-level Principal Component Removal](https://arxiv.org/abs/2507.21750) |                                             **AdversarialRobustness**&**PCA**&**Isotropy**                                              |
| 25.07 |                                                                                                     Mohammed VI Polytechnic University                                                                                                     |                             arxiv                              |                                   [Strategic Deflection: Defending LLMs from Logit Manipulation](https://arxiv.org/abs/2507.22160v1)                                    |                                            **LogitManipulation**&**Defense**&**Deflection**                                             |
| 25.07 |                                                                                                    South China University of Technology                                                                                                    |                            ACL 2025                            |                                     [SDD: Self-Degraded Defense against Malicious Fine-tuning](https://arxiv.org/abs/2507.21182v1)                                      |                                             **MaliciousFinetuning**&**Defense**&**Safety**                                              |
| 25.08 |                                                                                     Fudan University, East China University of Science and Technology                                                                                      |                             arxiv                              |                     [ReasoningGuard: Safeguarding Large Reasoning Models with Inference-time Safety Aha Moments](https://arxiv.org/abs/2508.04204)                      |                            **Large Reasoning Models**&**Jailbreak Defense**&**Inference-Time Intervention**                             |
| 25.08 |                                                                                                            Amazon Web Services                                                                                                             |                           EAI @ KDD                            |                                               [Defend LLMs Through Self-Consciousness](https://arxiv.org/abs/2508.02961)                                                |            **Large Language Model**&**Prompt Injection**&**Self-Consciousness Defense**&**Black-Box Defense**&**Ethical Al**            |
| 25.08 |                                                                                                                   KAIST                                                                                                                    |                             arxiv                              |                          [R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge](https://arxiv.org/abs/2508.00324)                          |                                **Large Reasoning Models**&**Safety Alignment**&**Knowledge Activation**                                 |
| 25.08 |                                                                                                       Politecnico di Milano, ML cube                                                                                                       |                             arxiv                              |                     [LEAKSEALER: A SEMISUPERVISED DEFENSE FOR LLMS AGAINST PROMPT INJECTION AND LEAKAGE ATTACKS](https://arxiv.org/abs/2508.00602)                      |                **Prompt Injection**&**Privacy**&**Data Leakage**&**Defense**&**Jailbreaking**&**Large Language Models**                 |
| 25.08 |                                                                           University of Electronic Science and Technology of China, City University of Hong Kong                                                                           |                             arxiv                              |                           [ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models](https://arxiv.org/abs/2508.01365)                            |                        **Backdoor Detection**&**Large Language Models**&**Sequence Lock**&**Real-Time Defense**                         |
| 25.08 |                                                                           Beijing Electronic Science and Technology Institute, Nanyang Technological University                                                                            |                             arxiv                              |                            [DUP: Detection-guided Unlearning for Backdoor Purification in Language Models](https://arxiv.org/abs/2508.01647)                            |                     **Backdoor Defense**&**Language Models**&**Backdoor Detection**&**Unlearning**&**Purification**                     |
| 25.08 |                                                                                                                   NVIDIA                                                                                                                   |                             arxiv                              |                 [CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications](https://arxiv.org/abs/2508.01710)                 |                  **Multilingual Safety**&**Culturally-Aware Datasets**&**Guard Models**&**Synthetic Data Generation**                   |
| 25.08 |                                                                                                     Microsoft Security Response Center                                                                                                     |                             arxiv                              |                                          [Highlight & Summarize: RAG without the jailbreaks](https://arxiv.org/abs/2508.02872)                                          |            **Retrieval-Augmented Generation (RAG)**&**Jailbreaking**&**Model Hijacking**&**System Design**&**LLM Security**             |
| 25.08 |                                                                                   Zhejiang University, Xiamen University, Shanghai Jiao Tong University                                                                                    |                             arxiv                              |        [HARMONYGUARD: TOWARD SAFETY AND UTILITY IN WEB AGENTS VIA ADAPTIVE POLICY ENHANCEMENT AND DUAL-OBJECTIVE OPTIMIZATION](https://arxiv.org/abs/2508.04010)        |                **Web Agents**&**LLM Safety**&**Dual-Objective Optimization**&**Adaptive Policy**&**Multi-Agent Systems**                |
| 25.08 |                                                                                                                    IBM                                                                                                                     |                             arxiv                              |                          [A Framework for Inherently Safer AGI through Language-Mediated Active Inference](https://arxiv.org/abs/2508.05766v1)                          |                                      **Active Inference**&**AGI Safety**&**Large Language Models**                                      |
| 25.08 |                                                                                                             University of Bonn                                                                                                             |                             arxiv                              |                               [In-Training Defenses against Emergent Misalignment in Language Models](https://arxiv.org/abs/2508.06249v1)                               |                                   **Emergent Misalignment**&**Fine-Tuning Safety**&**Regularization**                                   |
| 25.08 |                                                                                                                 EleutherAI                                                                                                                 |                             arxiv                              |                [Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs](https://arxiv.org/abs/2508.06601v1)                |                                      **Open-Weight LLMs**&**Tamper-Resistance**&**Data Filtering**                                      |
| 25.08 |                                                                                                             Nankai University                                                                                                              |                             arxiv                              |                                        [SafeGrad: Gradient Surgery for Safe LLM Fine-Tuning](https://arxiv.org/abs/2508.07172v1)                                        |                                        **Safe Fine-Tuning**&**Gradient Surgery**&**LLM Safety**                                         |
| 25.08 |                                                                                                              Jilin University                                                                                                              |                             arxiv                              |                           [BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks](https://arxiv.org/abs/2508.08127v1)                            |                          **Multi-Agent Systems Security**&**Unsupervised Defense**&**Graph Anomaly Detection**                          |
| 25.08 |                                                                                                        Chinese Academy of Sciences                                                                                                         |                             arxiv                              |                [Fine-Grained Safety Neurons with Training-Free Continual Projection to Reduce LLM Fine Tuning Risks](https://arxiv.org/abs/2508.09190v1)                |                           **Fine-Grained Safety Neurons**&**Continual Projection**&**LLM Fine-Tuning Risks**                            |
| 25.08 |                                                                                                         Renmin University of China                                                                                                         |                             arxiv                              |           [Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models: A Unified and Accurate Approach](https://arxiv.org/abs/2508.09201v1)           |                                **Jailbreak Detection**&**Vision-Language Models**&**Anomaly Detection**                                 |
| 25.08 |                                                                                                                   OpenAI                                                                                                                   |                             arxiv                              |                           [From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training](https://arxiv.org/abs/2508.09224v1)                           |                                   **Safe-Completions**&**Output-Centric Safety**&**Dual-Use Prompts**                                   |
| 25.08 |                                                                                                              Wuhan University                                                                                                              |                             arxiv                              |                     [NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs](https://arxiv.org/abs/2508.09473v1)                      |                                  **Safety-Utility Alignment**&**Neuron Modulation**&**Meta-Learning**                                   |
| 25.08 |                                                                                                            Southeast University                                                                                                            |                             arxiv                              |                            [Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation](https://arxiv.org/abs/2508.09666v1)                             |                            **Chain-of-Thought Distillation**&**Safety Alignment**&**Small Language Models**                             |
| 25.08 |                                                                                                      University of California, Irvine                                                                                                      |                             arxiv                              |                    [Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs](https://arxiv.org/abs/2508.10031v1)                     |                              **Context Filtering**&**Jailbreak Defense**&**Safety-Helpfulness Trade-off**                               |
| 25.08 |                                                                                                            Macquarie University                                                                                                            |                             arxiv                              |                 [SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory](https://arxiv.org/abs/2508.11290v1)                  |                                 **LLM Safety**&**Over-Refusal Mitigation**&**Representation Steering**                                  |
| 25.08 |                                                                                                      National University of Singapore                                                                                                      |                             arxiv                              |              [SafeSieve: From Heuristics to Experience in Progressive Pruning for LLM-based Multi-Agent Communication](https://arxiv.org/abs/2508.11733v1)              |                            **Multi-Agent Systems**&**Progressive Pruning**&**LLM Communication Efficiency**                             |
| 25.08 |                                                                                                      Nanyang Technological University                                                                                                      |                             arxiv                              |                                           [Mitigating Jailbreaks with Intent-Aware LLMs](https://arxiv.org/abs/2508.12072v1)                                            |                                    **LLM Safety**&**Jailbreak Defense**&**Intent-Aware Fine-Tuning**                                    |
| 25.08 |                                                                                                             Microsoft Research                                                                                                             |                           COLM 2025                            |                                 [Rethinking Safety in LLM Fine-tuning: An Optimization Perspective](https://arxiv.org/abs/2508.12531v1)                                 |                               **LLM Safety**&**Fine-tuning Optimization**&**Exponential Moving Average**                                |
| 25.08 |                                                                                                         University College London                                                                                                          |                             arxiv                              |  [CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2508.12535v1)  |                            **Sparse Autoencoders**&**LLM Steering**&**Correlation-based Feature Selection**                             |
| 25.08 |                                                                                                              Wuhan University                                                                                                              |                             arxiv                              |                               [FuSaR: A Fuzzification-Based Method for LRM Safety-Reasoning Balance](https://arxiv.org/abs/2508.12897v1)                                |                          **Large Reasoning Models**&**Safety-Reasoning Trade-off**&**Fuzzification Alignment**                          |
| 25.08 |                                                                                                 Zhejiang University, Antgroup, Quantstamp                                                                                                  |                             arxiv                              |                                           [ORFUZZ: Fuzzing the "Other Side" of LLM Safety](https://arxiv.org/abs/2508.11222)                                            |                                               **Over-Refusal**&**Fuzzing**&**LLM Safety**                                               |
| 25.08 |                                                                                                                Qiyuan Tech                                                                                                                 |                             arxiv                              |                                                                          [EFFICIENT SWITCHABLE                                                                          |                                                                  25.08                                                                  | Nanjing University, Meituan Inc., Dalian University of Technology | arxiv | [SDGO: Self-Discrimination-Guided Optimization for Consistent Safety in Large Language Models](https://arxiv.org/abs/2508.15648) | **LLM Safety**&**Reinforcement Learning**&**Self-Improvement** |SAFETY CONTROL IN LLMS VIA MAGIC-TOKEN-GUIDED CO-TRAINING](https://arxiv.org/abs/2508.14904) | **LLM Safety**&**Controllable Behavior**&**Magic Token** |
| 25.08 |                                                                                                         University of Southampton                                                                                                          |                             arxiv                              |                            [S¬≥LORA: Safe Spectral Sharpness-Guided Pruning in Adaptation of Agent Planner](https://arxiv.org/abs/2508.15068)                            |                                             **LLM Agent Safety**&**LoRA**&**Model Pruning**                                             |
| 25.08 |                                                                                     Nanjing University, Meituan Inc., Dalian University of Technology                                                                                      |                             arxiv                              |                    [SDGO: Self-Discrimination-Guided Optimization for Consistent Safety in Large Language Models](https://arxiv.org/abs/2508.15648)                     |                                     **LLM Safety**&**Reinforcement Learning**&**Self-Improvement**                                      |
| 25.08 |                                                                              Zhejiang University, University of California, Los Angeles, Westlake University                                                                               |                             arxiv                              |                [IPIGUARD: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents](https://arxiv.org/abs/2508.15310)                |                                     **LLM Agents**&**Indirect Prompt Injection**&**Cybersecurity**                                      |
| 25.08 |                                                                                                          Anglia Ruskin University                                                                                                          |                             arxiv                              |                           [Mechanistic Exploration of Backdoored Large Language Model Attention Patterns](https://arxiv.org/abs/2508.15847v1)                           |                                **Mechanistic Interpretability**&**Backdoor Attacks**&**Attention Heads**                                |
| 25.08 |                                                                                                          Universit√© Paris-Saclay                                                                                                           |                             arxiv                              |                  [LLMSymGuard: A Symbolic Safety Guardrail Framework Leveraging Interpretable Jailbreak Concepts](https://arxiv.org/abs/2508.16325v1)                   |                                 **LLM Safety**&**Mechanistic Interpretability**&**Symbolic Guardrails**                                 |
| 25.08 |                                                                                                        Chinese Academy of Sciences                                                                                                         |                             arxiv                              |         [Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and Investigating the Real Misuse Threat of LLMs](https://arxiv.org/abs/2508.16347v1)          |                                     **Jailbreak Evaluation**&**Misuse Threat**&**VENOM Framework**                                      |
| 25.08 |                                                                                                          University of Cambridge                                                                                                           |                             arxiv                              |               [Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak Prevention for Large Language Models](https://arxiv.org/abs/2508.16406v1)               |                          **Retrieval-Augmented Defense**&**Jailbreak Prevention**&**Safety-Utility Trade-off**                          |
| 25.08 |                                                                                                                   KAIST                                                                                                                    |                             arxiv                              |            [ObjexMT: Objective Extraction and Metacognitive Calibration for LLM-as-a-Judge under Multi-Turn Jailbreaks](https://arxiv.org/abs/2508.16889v1)             |                                      **LLM-as-a-Judge**&**Objective Extraction**&**Metacognition**                                      |
| 25.08 |                                                                                                                  Scale AI                                                                                                                  |                             arxiv                              |                                         [Reliable Weak-to-Strong Monitoring of LLM Agents](https://arxiv.org/abs/2508.19461v1)                                          |                                **Agent Monitoring**&**Monitor Red Teaming**&**Weak-to-Strong Oversight**                                |
| 25.08 |                                                                                                        Chinese Academy of Sciences                                                                                                         |                             arxiv                              |                               [Safety Alignment Should Be Made More Than Just A Few Attention Heads](https://arxiv.org/abs/2508.19697v1)                                |                                     **Safety Alignment**&**Attention Heads**&**Jailbreak Defense**                                      |
| 25.08 |                                                                                                              Fudan University                                                                                                              |                             arxiv                              |          [IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement](https://arxiv.org/abs/2508.20151v1)          |                                      **LLM Safeguards**&**Intent Reasoning**&**Query Refinement**                                       |
| 25.08 |                                                                                                  University of Illinois Urbana-Champaign                                                                                                   |                             arxiv                              |                   [GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs](https://arxiv.org/abs/2508.20325v1)                   |                                **Guideline Compliance**&**Jailbreak Diagnostics**&**Adaptive Role-Play**                                |
| 25.08 |                                                                                                             Nankai University                                                                                                              |                             arxiv                              |                                    [Governable AI: Provable Safety Under Extreme Threat Models](https://arxiv.org/abs/2508.20411v1)                                     |                                     **Governable AI**&**Cryptographic Safety**&**Existential Risk**                                     |
| 25.08 |                                                                                                           Politecnico di Torino                                                                                                            |                             arxiv                              |                               [CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics](https://arxiv.org/abs/2508.20643v1)                                |                                **Cyber Forensics**&**Blue-Team LLM Agents**&**Web Attack Investigation**                                |
| 25.08 |                                                                                                      Nanyang Technological University                                                                                                      |                             arxiv                              |                           [Token Buncher: Shielding LLMs from Harmful Reinforcement Learning Fine-Tuning](https://arxiv.org/abs/2508.20697v1)                           |                              **Harmful Fine-Tuning**&**Reinforcement Learning Defense**&**Token Buncher**                               |
| 25.08 |                                                                                                                   KAUST                                                                                                                    |                             arxiv                              |                    [Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection](https://arxiv.org/abs/2508.20766v1)                    |                           **Safety Alignment**&**Rank-One Safety Injection**&**Mechanistic Interpretability**                           |
| 25.08 |                                                                                                            Texas A&M University                                                                                                            |                             arxiv                              |                              [PromptSleuth: Detecting Prompt Injection via Semantic Intent Invariance](https://arxiv.org/abs/2508.20890v1)                              |                                  **Prompt Injection**&**Semantic Intent Invariance**&**LLM Security**                                   |
| 25.08 |                                                                                                      Nanyang Technological University                                                                                                      |                             arxiv                              |                             [Lethe: Purifying Backdoored Large Language Models with Knowledge Dilution](https://arxiv.org/abs/2508.21004v1)                             |                                   **Backdoor Defense**&**Knowledge Dilution**&**Model Purification**                                    |
| 25.08 |                                                                                                           Utah State University                                                                                                            |                           CIKM 2025                            |                                          [Pruning Strategies for Backdoor Defense in LLMs](https://arxiv.org/abs/2508.20032v1)                                          |                                    **Backdoor Defense**&**Attention Head Pruning**&**LLM Security**                                     |
| 25.08 |                                                                                                      Beijing Institute of Technology                                                                                                       |                           EMNLP 2025                           |                                                 [Speculative Safety-Aware Decoding](https://arxiv.org/abs/2508.17739v1)                                                 |                              **Decoding-Time Defense**&**Deep Safety Alignment**&**Speculative Sampling**                               |
| 25.09 |                                                                                                          University of Liverpool                                                                                                           |                             arxiv                              |              [Activation Steering Meets Preference Optimization: Defense Against Jailbreaks in Vision Language Models](https://arxiv.org/abs/2509.00373v1)              |                                **Vision Language Models**&**Activation Steering**&**Adversarial Attack**                                |
| 25.09 |                                                                                                      Suzhou University of Technology                                                                                                       |                             arxiv                              |                         [LLM Encoder vs. Decoder: Robust Detection of Chinese AI-Generated Text with LoRA](https://arxiv.org/abs/2509.00731v1)                          |                                    **Chinese AI-Generated Text Detection**&**LoRA**&**Transformer**                                     |
| 25.09 |                                                                                                            Zhejiang University                                                                                                             |                             arxiv                              |         [PREE: Towards Harmless and Adaptive Fingerprint Editing in Large Language Models via Knowledge Prefix Enhancement](https://arxiv.org/abs/2509.00918v1)         |                              **Fingerprint Editing**&**Knowledge Prefix Enhancement**&**Model Protection**                              |
| 25.09 |                                                                                                          Duke Kunshan University                                                                                                           |                             arxiv                              |                                    [Unraveling LLM Jailbreaks Through Safety Knowledge Neurons](https://arxiv.org/abs/2509.01631v1)                                     |                                    **Jailbreak Attacks**&**Safety Knowledge Neurons**&**SafeTuning**                                    |
| 25.09 |                                                                                                           University of Toronto                                                                                                            |                             arxiv                              |                       [Poisoned at Scale: A Scalable Audit Uncovers Hidden Scam Endpoints in Production LLMs](https://arxiv.org/abs/2509.02372v1)                       |                                   **Data Poisoning**&**Malicious Code Generation**&**Security Audit**                                   |
| 25.09 |                                                                                                             University of Hull                                                                                                             |                             arxiv                              |                       [RAGuard: A Novel Approach for in-context Safe Retrieval Augmented Generation for LLMs](https://arxiv.org/abs/2509.03768v1)                       |                                  **Retrieval-Augmented Generation**&**AI Safety**&**Decision Support**                                  |
| 25.09 |                                                                                                       Harbin Institute of Technology                                                                                                       |                             arxiv                              |                     [Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via Projection Constraint](https://arxiv.org/abs/2509.06795v1)                      |                                   **Instruction Fine-Tuning**&**Safety Risks**&**Refusal Direction**                                    |
| 25.09 |                                                                                                       Harbin Institute of Technology                                                                                                       |                             arxiv                              |                           [MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and Security](https://arxiv.org/abs/2509.06807v1)                           |                                 **LLM Security**&**Usability-Security Tradeoff**&**Routing Mechanism**                                  |
| 25.09 |                                                                                                          ShanghaiTech University                                                                                                           |                             arxiv                              |                   [AgentSentinel: An End-to-End and Real-Time Security Defense Framework for Computer-Use Agents](https://arxiv.org/abs/2509.07764v1)                   |                                   **Computer-Use Agents**&**Security Defense**&**Real-Time Auditing**                                   |
| 25.09 |                                                                                                            Saarland University                                                                                                             |                             arxiv                              |                            [Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M](https://arxiv.org/abs/2509.09055v1)                            |                             **Supervised Fine-Tuning**&**Direct Preference Optimization**&**LLM Alignment**                             |
| 25.09 |                                                                                                            Macquarie University                                                                                                            |                           EMNLP 2025                           |                                       [Too Helpful, Too Harmless, Too Honest or Just Right?](https://arxiv.org/abs/2509.08486v1)                                        |                                     **HHH Alignment**&**Mixture-of-Experts**&**Model Calibration**                                      |
| 25.09 |                                                                              Singapore University of Technology and Design, Nanyang Technological University                                                                               |                             arxiv                              |                                 [Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal](https://arxiv.org/abs/2509.09708)                                  |                              **Refusal Behavior**&**Mechanistic Interpretability**&**Sparse Autoencoders**                              |
| 25.09 |                                                                                                ByteDance Seed, Hong Kong Baptist University                                                                                                |                           ICLR 2025                            |                             [Reasoned Safety Alignment: Ensuring Jailbreak Defense via Answer-Then-Check](https://arxiv.org/abs/2509.11629)                             |                                    **Safety Alignment**&**Jailbreak Defense**&**Answer-Then-Check**                                     |
| 25.09 |                                                                               Wroclaw University of Science and Technology, NASK, Polish Academy of Sciences                                                                               |                             arxiv                              |                 [Rethinking the Evaluation of Alignment Methods: Insights into Diversity, Generalisation, and Safety](https://arxiv.org/abs/2509.12936)                 |                                **LLM Alignment**&**Evaluation Framework**&**Safety‚ÄìDiversity Trade-off**                                |
| 25.09 |                                                                                     Wichita State University, Marshall University, University of Aizu                                                                                      |                             arxiv                              |                                 [A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks](https://arxiv.org/abs/2509.14285)                                 |                                   **Prompt Injection Defense**&**Multi-Agent Pipeline**&**AI Safety**                                   |
| 25.09 |                                                        City University of Hong Kong, The University of Hong Kong, Hong Kong Baptist University, The Chinese University of Hong Kong                                                        |                             arxiv                              |                                             [LLM Jailbreak Detection for (Almost) Free!](https://arxiv.org/abs/2509.14558)                                              |                                 **Jailbreak Detection**&**Safety Alignment**&**Logit-based Detection**                                  |
| 25.09 |                                                                                                   Apple, Cohere, DeepMind, Meta, MongoDB                                                                                                   |                             arxiv                              |                   [Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection](https://arxiv.org/abs/2509.14622)                    |                        **Malicious Intent Detection**&**Retrieval-Augmented Guard**&**Adversarial Distillation**                        |
| 25.09 |                                                                Institute of Information Engineering, Chinese Academy of Sciences; University of Chinese Academy of Sciences                                                                |                             arxiv                              |             [Beyond Surface Alignment: Rebuilding LLMs Safety Mechanism via Probabilistically Ablating Refusal Direction](https://arxiv.org/abs/2509.15202)             |                                    **Safety Alignment**&**Refusal Direction**&**Jailbreak Defense**                                     |
| 25.09 |                                                                   Keio University, Universitat Polit√®cnica de Val√®ncia, Center for AI Safety, Carnegie Mellon University                                                                   |                             arxiv                              |                        [Psychometric Personality Shaping Modulates Capabilities and Safety in Language Models](https://arxiv.org/abs/2509.16332)                        |                                     **Personality Shaping**&**AI Safety**&**Large Language Models**                                     |
| 25.09 |                                                                                         Monash University, The University of Melbourne, Transurban                                                                                         |                             arxiv                              |        [DecipherGuard: Understanding and Deciphering Jailbreak Prompts for a Safer Deployment of Intelligent Software Systems](https://arxiv.org/abs/2509.16870)        |                                      **LLM Security**&**Jailbreak Attacks**&**Runtime Guardrails**                                      |
| 25.09 |                                                                                   ELLIS Institute T√ºbingen, MPI for Intelligent Systems, Fraunhofer HHI                                                                                    |                             arxiv                              |                              [Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLMs](https://arxiv.org/abs/2509.18058)                              |                                 **Strategic Dishonesty**&**Safety Evaluation**&**Deception Detection**                                  |
| 25.09 |                                                               University of Notre Dame, University of Washington, Johns Hopkins University, Georgia Institute of Technology                                                                |                             arxiv                              |                             [Steering Multimodal Large Language Models Decoding for Context-Aware Safety](https://arxiv.org/abs/2509.19212)                             |                         **Multimodal Large Language Models**&**Context-Aware Safety**&**Contrastive Decoding**                          |
| 25.09 |                                                                                                               Cisco Systems                                                                                                                |                             arxiv                              |                  [A Framework for Rapidly Developing and Deploying Protection Against Large Language Model Attacks](https://arxiv.org/abs/2509.20639)                   |                                    **LLM Security**&**Threat Intelligence**&**Detection Framework**                                     |
| 25.09 |                                                              Nanjing University of Science and Technology, Nanyang Technological University, National University of Singapore                                                              |                             arxiv                              |                  [SafeSteer: Adaptive Subspace Steering for Efficient Jailbreak Defense in Vision-Language Models](https://arxiv.org/abs/2509.21400v1)                  |                                **Vision-Language Models**&**Jailbreak Defense**&**Activation Steering**                                 |
| 25.09 |                                           University of Science and Technology of China, Nanyang Technological University, University of California San Diego, National University of Singapore                                            |                             arxiv                              |                           [Backdoor Attribution: Elucidating and Controlling Backdoor in Language Models](https://arxiv.org/abs/2509.21761v2)                           |                            **Backdoor Attack**&**Mechanistic Interpretability**&**Language Models Security**                            |
| 25.09 |                                                                                               Hong Kong University of Science and Technology                                                                                               |                             arxiv                              |                         [Safety Compliance: Rethinking LLM Safety Reasoning through the Lens of Compliance](https://arxiv.org/abs/2509.22250v1)                         |                                     **LLM Safety**&**Legal Compliance**&**Reinforcement Learning**                                      |
| 25.09 |                                                                                                            Macquarie University                                                                                                            |                             arxiv                              |                      [We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong](https://arxiv.org/abs/2509.22510v1)                       |                                   **LLM Alignment**&**Steering Vectors**&**Multi-Objective Learning**                                   |
| 25.09 |                                              Institute of Automation (Chinese Academy of Sciences), University of Chinese Academy of Sciences, Beijing Institute of AI Safety and Governance                                               |                             arxiv                              |                   [Bidirectional Intention Inference Enhances LLMs' Defense Against Multi-Turn Jailbreak Attacks](https://arxiv.org/abs/2509.22732v1)                   |                                      **Jailbreak Defense**&**Intention Inference**&**LLM Safety**                                       |
| 25.09 |                                                                                                                   KAIST                                                                                                                    |                             arxiv                              |                            [Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment](https://arxiv.org/abs/2509.22745v1)                            |                               **Mixture-of-Experts**&**Harmful Fine-Tuning Defense**&**Safety Alignment**                               |
| 25.09 |                                                                                                             Yonsei University                                                                                                              |                             arxiv                              |                              [A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models](https://arxiv.org/abs/2509.23286v1)                              |                               **Diffusion Language Models**&**Safety Alignment**&**Token-Level Defense**                                |
| 25.09 |                                                                                The University of Tokyo, University of Illinois Chicago, Zhejiang University                                                                                |                             arxiv                              |                                [PSG-Agent: Personality-Aware Safety Guardrail for LLM-based Agents](https://arxiv.org/abs/2509.23614v1)                                 |                                   **LLM-based Agents**&**Personalized Safety**&**Dynamic Guardrail**                                    |
| 25.09 |                                                          The Chinese University of Hong Kong, Shenzhen; State University of New York at Buffalo; Huawei International, Singapore                                                           |                             arxiv                              |                [AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety Alignment of Large Reasoning Models](https://arxiv.org/abs/2509.24269v1)                |                            **Large Reasoning Models**&**Adversarial Alignment**&**Chain-of-Thought Safety**                             |
| 25.09 |                                                                                                BUPT, NUS, NTU, PKU, THU, CUHK, Squirrel AI                                                                                                 |                             arxiv                              |                       [DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models](https://arxiv.org/abs/2509.24296v1)                       |                             **Diffusion Large Language Models**&**Intrinsic Safety**&**Defense Framework**                              |
| 25.09 |                                                                                                     Tsinghua University, RealAI, CASIA                                                                                                     |                             arxiv                              |                           [Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention](https://arxiv.org/abs/2509.24393v1)                            |                                **Large Reasoning Models**&**Safe Reasoning**&**Preference Optimization**                                |
| 25.09 |                                                                                                    HKUST, NUS, SCUT, Beihang University                                                                                                    |                             arxiv                              |                              [GSPR: Aligning LLM Safeguards as Generalizable Safety Policy Reasoners](https://arxiv.org/abs/2509.24418v1)                               |                               **LLM Safeguard**&**Safety Reasoning**&**Reinforcement Learning Alignment**                               |
| 25.09 |                                                                                                         Duke University, Ant Group                                                                                                         |                             arxiv                              |                                             [Fingerprinting LLMs via Prompt Injection](https://arxiv.org/abs/2509.25448v2)                                              |                               **LLM Provenance Detection**&**Prompt Injection**&**Model Fingerprinting**                                |
| 25.10 |                                                                                                   Institute of Science Tokyo, RIKEN AIP                                                                                                    |                             arxiv                              |                     [Toward Safer Diffusion Language Models: Discovery and Mitigation of Priming Vulnerability](https://arxiv.org/abs/2510.00565v1)                     |                             **Diffusion Language Models**&**Jailbreak Vulnerability**&**Safety Alignment**                              |
| 25.10 |                                                                                                        Chinese Academy of Sciences                                                                                                         |                             arxiv                              |                           [Safety Instincts: LLMs Learn to Trust Their Internal Compass for Self-Defense](https://arxiv.org/abs/2510.01088v1)                           |                       **Intrinsic Safety Signal**&**Entropy-Based Reinforcement Learning**&**Jailbreak Defense**                        |
| 25.10 |                                                                                                         University of Huddersfield                                                                                                         |                             arxiv                              |                         [OpenAI's GPT-OSS-20B Model and Safety Alignment Issues in a Low-Resource Language](https://arxiv.org/abs/2510.01266v1)                         |                              **AI Safety**&**Low-Resource Languages**&**Reward Hacking**&**Cultural Bias**                              |
| 25.10 |                                                              Northwestern University, University of Illinois at Chicago, University of Rochester, Carnegie Mellon University                                                               |                             arxiv                              |          [AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.01586v1)          |               **Multi-Agent Reinforcement Learning**&**Adversarial Co-Evolution**&**LLM Safety**&**Internalized Defense**               |
| 25.10 |                                                                                                       Pingla Institute, UNSW Sydney                                                                                                        |                             arxiv                              |                           [NLP Methods for Detecting Novel LLM Jailbreaks and Keyword Analysis with BERT](https://arxiv.org/abs/2510.01644v1)                           |                                 **LLM Jailbreak Detection**&**BERT**&**Keyword Analysis**&**AI Safety**                                 |
| 25.10 |                                                                                               University of Science and Technology of China                                                                                                |                             arxiv                              |                               [UpSafe¬∞C: Upcycling for Controllable Safety in Large Language Models](https://arxiv.org/abs/2510.02194v1)                                |                         **LLM Safety**&**Mixture-of-Experts**&**Safety Temperature**&**Controllable Alignment**                         |
| 25.10 |                                                                               The Pennsylvania State University, Palo Alto Networks, Stony Brook University                                                                                |                            CCS 2025                            |                        [You Can‚Äôt Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors](https://doi.org/10.1145/3719027.3765124)                        |                     **Prompt Leakage**&**Representation Engineering**&**System Vectors (SysVec)**&**LLM Security**                      |
| 25.10 |                                                                                                            New York University                                                                                                             |                      EMNLP 2025 Findings                       |                              [Think Twice, Generate Once: Safeguarding by Progressive Self-Reflection](https://arxiv.org/abs/2510.01270v1)                              |                                                 **LLMÂÆâÂÖ®**&**Ëá™ÂèçÊÄùÊú∫Âà∂**&**Ë∂äÁã±Èò≤Âæ°**&**Êé®ÁêÜÊó∂ÂØπÈΩê**                                                  |
| 25.10 |                                                                                                      Nanyang Technological University                                                                                                      |                             arxiv                              |                                [P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs](https://arxiv.org/abs/2510.04503)                                 |                                 **Backdoor Defense**&**Prompt Learning**&**Data Poisoning Mitigation**                                  |
| 25.10 |                                                                                                          Google Cloud AI Research                                                                                                          |                             arxiv                              |                                 [VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation](https://arxiv.org/abs/2510.05156)                                  |                                       **LLM Agents**&**Formal Verification**&**Safety Alignment**                                       |
| 25.10 |                                                                                                             Purdue University                                                                                                              |                             arxiv                              |                                  [From Poisoned to Aware: Fostering Backdoor Self-Awareness in LLMs](https://arxiv.org/abs/2510.05169)                                  |                           **Backdoor Attacks**&**Self-Awareness in LLMs**&**Reinforcement Learning Defense**                            |
| 25.10 |                                                                                                         The Alan Turing Institute                                                                                                          |                             arxiv                              |                           [Towards Reliable and Practical LLM Security Evaluations via Bayesian Modelling](https://arxiv.org/abs/2510.05709)                            |                           **LLM Security Evaluation**&**Bayesian Modelling**&**Prompt Injection Robustness**                            |
| 25.10 |                                                                                                            Shandong University                                                                                                             |                             arxiv                              |                            [From Defender to Devil? Unintended Risk Interactions Induced by LLM Defenses](https://arxiv.org/abs/2510.07968)                             |                                 **Cross-risk Interactions**&**LLM Defenses**&**Neuron-level Analysis**                                  |
| 25.10 |                                                                                           Meta Superintelligence Labs, Johns Hopkins University                                                                                            |                             arxiv                              |                               [The Alignment Waltz: Jointly Training Agents to Collaborate for Safety](https://arxiv.org/abs/2510.08240)                                |                         **Multi-Agent Reinforcement Learning**&**Safety Alignment**&**Overrefusal Mitigation**                          |
| 25.10 |                                                                                                      Chinese University of Hong Kong                                                                                                       |                          NeurIPS 2025                          |                       [MetaDefense: Defending Finetuning-based Jailbreak Attacks Before and During Generation](https://arxiv.org/abs/2510.07835)                        |                                **LLM Jailbreak Defense**&**Finetuning Attack**&**Harmfulness Detection**                                |
| 25.10 |                                             University of California, Los Angeles & Alibaba Cloud Computing & Shanghai Jiaotong University & Alibaba Group & Nanyang Technological University                                              |                             arxiv                              |                              [Energy-Driven Steering: Reducing False Refusals in Large Language Models](https://arxiv.org/abs/2510.08646)                               |                                **Safety Alignment**&**False Refusal Mitigation**&**Energy-Based Model**                                 |
| 25.10 |                                                                                                             Peking University                                                                                                              |                             arxiv                              |      [Decoupling Safety into Orthogonal Subspace: Cost-Efficient and Performance-Preserving Alignment for Large Language Models](https://arxiv.org/abs/2510.09004)      |                                          **Safety Alignment**&**LoRA**&**Orthogonal Subspace**                                          |
| 25.10 |                                                           South China University of Technology & Georgia Institute of Technology & Sun Yat-sen University & Pengcheng Laboratory                                                           |                             arxiv                              |                  [Pharmacist: Safety Alignment Data Curation for Large Language Models against Harmful Fine-tuning](https://arxiv.org/abs/2510.10085)                   |                          **Harmful Fine-tuning**&**Safety Alignment**&**Data Selection**&**Alignment Defense**                          |
| 25.10 |                                                                     Nanyang Technological University & USTC & UAEU & PayPal Inc. & Walmart Labs & Squirrel Ai Learning                                                                     |                             arxiv                              |                  [Backdoor Collapse: Eliminating Unknown Threats via Known Backdoor Aggregation in Language Models](https://arxiv.org/abs/2510.10265)                   |                        **Backdoor Defense**&**Model Security**&**Representation Aggregation**&**LLM Robustness**                        |
| 25.10 |                                                                 University of Illinois Chicago & University of Tokyo & Tsinghua University & Shanghai Jiao Tong University                                                                 |                             arxiv                              |                 [DeepResearchGuard: Deep Research with Open-Domain Evaluation and Multi-Stage Guardrails for Safety](https://arxiv.org/abs/2510.10994)                  |                                   **Deep Research**&**Safety Guardrails**&**Open-Domain Evaluation**                                    |
| 25.10 |                                                                                                National University of Singapore & Ant Group                                                                                                |                             arxiv                              |                       [TraceAegis: Securing LLM-Based Agents via Hierarchical and Behavioral Anomaly Detection](https://arxiv.org/abs/2510.11203)                       |                                    **Agent Security**&**Anomaly Detection**&**Provenance Analysis**                                     |
| 25.10 |                                                                                                           Independent Researcher                                                                                                           |                             arxiv                              |                            [Countermind: A Multi-Layered Security Architecture for Large Language Models](https://arxiv.org/abs/2510.11837)                             |                                  **LLM Security**&**Defense-in-Depth**&**Prompt Injection Mitigation**                                  |
| 25.10 |                                                                                               Hong Kong University of Science and Technology                                                                                               |                             arxiv                              |                                      [SafeMT: Multi-turn Safety for Multimodal Language Models](https://arxiv.org/abs/2510.12133)                                       |                                        **Multimodal Safety**&**Benchmark**&**Dialogue Security**                                        |
| 25.10 |                                                                                                          Northwestern University                                                                                                           |                             arxiv                              |                     [SENTINEL: A Multi-Level Formal Framework for Safety Evaluation of LLM-based Embodied Agents](https://arxiv.org/abs/2510.12985)                     |                                  **LLM-based Agents**&**Formal Safety Evaluation**&**Temporal Logic**                                   |
| 25.10 |                                                                                                        Sapienza University of Rome                                                                                                         |                             arxiv                              |                             [Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection](https://arxiv.org/abs/2510.13893)                              |                                           **Jailbreak Detection**&**Taxonomy**&**LLM Safety**                                           |
| 25.10 |                                                                                              Pennsylvania State University & Duke University                                                                                               |                             arxiv                              |                               [PIShield: Detecting Prompt Injection Attacks via Intrinsic LLM Features](https://arxiv.org/abs/2510.14005)                               |                                    **Prompt Injection**&**LLM Security**&**Internal Representation**                                    |
| 25.10 |                                                                                                                 Qwen Team                                                                                                                  |                             arxiv                              |                                                     [Qwen3Guard Technical Report](https://arxiv.org/abs/2510.14276)                                                     |                                  **Safety Guardrails**&**Streaming Moderation**&**Multilingual LLMs**                                   |
| 25.10 |                                                                                              Jilin University & KAUST & University of Oxford                                                                                               |                             arxiv                              |               [A Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space](https://arxiv.org/abs/2510.14301)                |                                    **Safety Alignment**&**Fine-tuning Preservation**&**GuardSpace**                                     |
| 25.10 |                                                                                                   Rutgers University, MARCo Health Inc.                                                                                                    |                             arxiv                              |              [Mitigating Harmful Erraticism in LLMs Through Dialectical Behavior Therapy Based De-Escalation Strategies](https://arxiv.org/abs/2510.15889)              |                            **LLM Safety**&**Computational Psychopathology**&**Dialectical Behavior Therapy**                            |
| 25.10 |                                                                                           University of Haifa, University of Zurich, ETH Zurich                                                                                            |                             arxiv                              |      [Detecting and Preventing Harmful Behaviors in AI Companions: Development and Evaluation of the SHIELD Supervisory System](https://arxiv.org/abs/2510.15891)       |                                     **AI Companions**&**Behavioral Safety**&**Supervisory Systems**                                     |
| 25.10 |                                                                                               The University of Edinburgh, Fudan University                                                                                                |                             arxiv                              |                  [SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based Dynamic Threat Detection](https://arxiv.org/abs/2510.16219)                  |                            **Multi-Agent Collaboration**&**Adversarial Detection**&**Decentralized Defense**                            |
| 25.10 |                                                                                         University of California Berkeley, MBZUAI, IIIT Hyderabad                                                                                          |                             arxiv                              |                      [Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety](https://arxiv.org/abs/2510.16492)                       |                                         **Agent Safety**&**Uncertainty Awareness**&**ToolEmu**                                          |
| 25.10 |                                                                                     University of Southern California, University of California Davis                                                                                      |                             arxiv                              |                         [DistilLock: Safeguarding LLMs from Unauthorized Knowledge Distillation on the Edge](https://arxiv.org/abs/2510.16716)                          |                             **Knowledge Distillation**&**Trusted Execution Environment**&**Model Security**                             |
| 25.10 |                                                                                              University of Illinois Urbana-Champaign, Amazon                                                                                               |                             arxiv                              |                                  [SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents](https://arxiv.org/abs/2510.17017)                                   |                                  **LLM Search Agents**&**Safety Alignment**&**Reinforcement Learning**                                  |
| 25.10 |                                                                                                       Cornell University, Microsoft                                                                                                        |                             arxiv                              |                         [Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems](https://arxiv.org/abs/2510.17276)                          |                                   **Multi-Agent Systems**&**Control-Flow Hijacking**&**LLM Security**                                   |
| 25.10 |                                                                           The Hong Kong University of Science and Technology (Guangzhou), Sun Yat-sen University                                                                           |                             arxiv                              |                        [SARSteer: Safeguarding Large Audio Language Models via Safe-Ablated Refusal Steering](https://arxiv.org/abs/2510.17633)                         |                                **Large Audio-Language Models**&**Safety Alignment**&**Refusal Steering**                                |
| 25.10 |                                                                                      City University of Hong Kong, Washington University in St. Louis                                                                                      |                             arxiv                              |                            [CrossGuard: Safeguarding MLLMs against Joint-Modal Implicit Malicious Attacks](https://arxiv.org/abs/2510.17687)                            |                 **Multimodal Large Language Models**&**Implicit Jailbreak Attacks**&**Reinforcement Learning Defense**                  |
| 25.10 |                                                                                                             UC Berkeley, KACST                                                                                                             |                             arxiv                              |                                         [Defending Against Prompt Injection with DataFilter](https://arxiv.org/abs/2510.19207)                                          |                                    **Prompt Injection Defense**&**LLM Security**&**Data Filtering**                                     |
| 25.10 | Independent Researcher, Harbin Institute of Technology, Xi‚Äôan Jiaotong-Liverpool University | arxiv | [VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search](https://arxiv.org/abs/2510.15948v1) | **Safety Alignment**&**Large Vision-Language Models**&**Monte Carlo Tree Search** |
| 25.10 | Hochschule Kempten, Shibaura Institute of Technology | arxiv | [Annotating the Chain-of-Thought: A Behavior-Labeled Dataset for AI Safety](https://arxiv.org/abs/2510.18154) | **Chain-of-Thought**&**AI Safety**&**Representation Engineering** |
| 25.10 | Brown University | arxiv | [Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training](https://arxiv.org/abs/2510.20956) | **Safety Alignment**&**Reasoning Language Models**&**Mechanistic Interpretability** |
| 25.10 | University of St. Gallen, Universit√† della Svizzera italiana | arxiv | [Securing AI Agent Execution](https://arxiv.org/abs/2510.21236v2) | **AI Agent Security**&**Access Control Framework**&**Model Context Protocol (MCP)** |
| 25.10 | Chinese Academy of Sciences, University of Chinese Academy of Sciences | arxiv | [When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large Reasoning Models with Chain-of-Guardrails](https://arxiv.org/abs/2510.21285v2) | **Large Reasoning Models**&**Self-Jailbreak**&**Chain-of-Guardrails** |
| 25.10 | University of Massachusetts Amherst, Microsoft | arxiv | [Preventing Catastrophic Forgetting: Behavior-Aware Sampling for Safer Language Model Fine-Tuning](https://arxiv.org/abs/2510.21885v1) | **Catastrophic Forgetting**&**Safety Alignment**&**Behavior-Aware Sampling** |
| 25.10 | Sun Yat-sen University | arxiv | [Guardian: Decoupling Exploration from Safety in Reinforcement Learning](https://arxiv.org/abs/2510.22859v1) | **Safe Reinforcement Learning**&**Hybrid Offline-Online RL**&**Exploration-Safety Decoupling** |
| 25.10 | Peking University Shenzhen Graduate School | arxiv | [Is Your Prompt Poisoning Code? Defect Induction Rates and Security Mitigation Strategies](https://arxiv.org/abs/2510.22944v1) | **Prompt Normativity**&**Code Security**&**LLM Vulnerability Analysis** |
| 25.10 | Peking University, Tencent | arxiv | [MCPGuard: Automatically Detecting Vulnerabilities in MCP Servers](https://arxiv.org/abs/2510.23673v1) | **Model Context Protocol**&**AI Security**&**Vulnerability Detection** |
| 25.10 | The University of Hong Kong, Fudan University, Shanghai AI Lab, NTU, Nanjing University, Shanghai Jiao Tong University | arxiv | [OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows](https://arxiv.org/abs/2510.24411v1) | **Mobile GUI Agent Safety**&**Hybrid Validation**&**Formal Verification** |
| 25.10 | National University of Singapore, University of Louisville, University of North Texas, Drexel University | arxiv | [Secure Retrieval-Augmented Generation against Poisoning Attacks](https://arxiv.org/abs/2510.25025v1) | **Retrieval-Augmented Generation**&**Poisoning Attack Defense**&**RAG Robustness** |
| 25.10 | Macquarie University | arxiv | [Agentic Moderation: Multi-Agent Design for Safer Vision-Language Models](https://arxiv.org/abs/2510.25179v1) | **Agentic AI**&**Safety Alignment**&**Vision-Language Models** |
| 25.10 | Beijing University of Posts and Telecommunications; NUS; CSIRO‚Äôs Data61; University of Adelaide; Tsinghua University | NeurIPS 2025 | [ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio‚ÄìLanguage Models](https://arxiv.org/abs/2510.26096v1) | **Audio-Language Models**&**Jailbreak Defense**&**Shortcut Learning** |
| 25.10 | Google DeepMind | arxiv | [Consistency Training Helps Stop Sycophancy and Jailbreaks](https://arxiv.org/abs/2510.27062) | **Consistency Training**&**Sycophancy**&**Jailbreak Defense** |
| 25.11 | National University of Singapore | arxiv | [DRIP: Defending Prompt Injection via De-instruction Training and Residual Fusion Model Architecture](https://arxiv.org/abs/2511.00447) | **Prompt Injection Defense**&**De-instruction Training**&**Residual Fusion** |
| 25.11 | Wuhan University | arxiv | [Reimagining Safety Alignment with An Image](https://arxiv.org/abs/2511.00509) | **Safety Alignment**&**Multimodal LLM**&**Visual Prompt Optimization** |
| 25.11 | East China Normal University | arxiv | [KG-DF: A Black-box Defense Framework against Jailbreak Attacks Based on Knowledge Graphs](https://arxiv.org/abs/2511.07480) | **Knowledge Graph Defense**&**Jailbreak Mitigation**&**Semantic Parsing** |
| 25.11 | Seoul National University | arxiv | [Alignment-Aware Quantization for LLM Safety](https://arxiv.org/abs/2511.07842) | **LLM Quantization**&**Safety Alignment**&**Contrastive Loss** |
| 25.11 | Rensselaer Polytechnic Institute | arxiv | [Patching LLM Like Software: A Lightweight Method for Improving Safety Policy in Large Language Models](https://arxiv.org/abs/2511.08484) | **Safety Patching**&**Prompt Prefix Tuning**&**LLM Safety Alignment** |
| 25.11 | Carnegie Mellon University | arxiv | [Stabilizing Reinforcement Learning for Honesty Alignment in Language Models on Deductive Reasoning](https://arxiv.org/abs/2511.09222) | **Honesty Alignment**&**Deductive Reasoning**&**RLVR/ANCHOR** |
| 25.11 | Case Western Reserve University | AAAI 2026 | [EASE: Practical and Efficient Safety Alignment for Small Language Models](https://arxiv.org/abs/2511.06512) | **SLM Safety Alignment**&**Safety Reasoning**&**Selective Activation** |
| 25.11 | Nanjing University of Posts and Telecommunications | AAAI 2026 | [Differentiated Directional Intervention: A Framework for Evading LLM Safety Alignment](https://arxiv.org/abs/2511.06852) | **Activation Steering**&**Safety Alignment Bypass**&**Bi-Directional Safety Mechanism** |
| 25.11 | Ant Group | IEEE Symposium on Security and Privacy 2026 | [ENCHTABLE: Unified Safety Alignment Transfer in Fine-tuned Large Language Models](https://arxiv.org/abs/2511.09880) | **Safety Vector Distillation**&**NTK-based Model Editing**&**Post-hoc Safety Alignment** |
| 25.11 | University of Luxembourg, Institut National Polytechnique de Toulouse, Foyer S.A. | arxiv | [NegBLEURT Forest: Leveraging Inconsistencies for Detecting Jailbreak Attacks](https://arxiv.org/abs/2511.11784) | **Jailbreak Detection**&**Negation-aware Evaluation**&**Isolation Forest** |
| 25.11 | Samsung SDS Technology Research | arxiv | [SGuard-v1: Safety Guardrail for Large Language Models](https://arxiv.org/abs/2511.12497) | **Safety Guardrail**&**Content Filtering**&**Jailbreak Detection** |
| 25.11 | Wuhan University / MiLM Plus, Xiaomi Inc. | arxiv | [SafeGRPO: Self-Rewarded Multimodal Safety Alignment via Rule-Governed Policy Optimization](https://arxiv.org/abs/2511.12982) | **Multimodal Safety Alignment**&**GRPO**&**Rule-based Reward Modeling** |
| 25.11 | The Hong Kong Polytechnic University | arxiv | [ExplainableGuard: Interpretable Adversarial Defense for LLMs Using Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.13771) | **Adversarial Defense**&**Chain-of-Thought Reasoning**&**Explainable AI** |
| 25.11 | Unknown | arxiv | [Securing AI Agents Against Prompt Injection Attacks: A Comprehensive Benchmark and Defense Framework](https://arxiv.org/abs/2511.15759) | **Prompt Injection**&**Retrieval-Augmented Generation**&**AI Agent Security** |
| 25.11 | Capgemini Invent | arxiv | [PSM: Prompt Sensitivity Minimization via LLM-Guided Black-Box Optimization](https://arxiv.org/abs/2511.16209) | **Prompt Sensitivity Minimization**&**Prompt Extraction Defense**&**LLM-as-Optimizer** |
| 25.11 | National University of Singapore | arxiv | [Defending Large Language Models Against Jailbreak Exploits with Responsible AI Considerations](https://arxiv.org/abs/2511.18933) | **LLM Jailbreak Defense**&**Responsible AI**&**Safety Alignment** |
| 25.11 | Northwestern Polytechnical University | arxiv | [Understanding and Mitigating Over-Refusal for Large Language Models via Safety Representation](https://arxiv.org/abs/2511.19009) | **Over-refusal Mitigation**&**Safety Representation**&**Alignment Balance** |
| 25.11 | Fudan University, Alibaba, Yale, IBM, Indiana University | arxiv | [Adversarial Attack-Defense Co-Evolution for LLM Safety Alignment via Tree-Group Dual-Aware Search and Optimization](https://arxiv.org/abs/2511.19218) | **LLMÂÆâÂÖ®ÂØπÈΩê**&**ÂØπÊäóÂÖ±ËøõÂåñ**&**Ê†ëÁæ§ÂèåÊÑüÁü•‰ºòÂåñ** |
| 25.11 | Precise Software Solutions | arxiv | [Cross-LLM Generalization of Behavioral Backdoor Detection in AI Agent Supply Chains](https://arxiv.org/abs/2511.19874) | **Backdoor Detection**&**Cross-LLM Generalization**&**AI Agent Security** |
| 25.11 | University of Science and Technology of China, The University of Hong Kong | arxiv | [GuardTrace-VL: Detecting Unsafe Multimodal Reasoning via Iterative Safety Supervision](https://arxiv.org/abs/2511.20994) | **Multimodal Reasoning Safety**&**Vision-Language Models**&**Iterative Safety Supervision** |
| 25.11 | Duke University, AWS Generative AI Innovation Center | arxiv | [Breaking the Safety‚ÄìCapability Tradeoff: Reinforcement Learning with Verifiable Rewards Maintains Safety Guardrails in LLMs](https://arxiv.org/abs/2511.21050) | **Reinforcement Learning with Verifiable Rewards**&**Safety‚ÄìCapability Tradeoff**&**LLM Alignment** |
| 25.11 | Beijing Jiaotong University, University of International Business and Economics | arxiv | [Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines](https://arxiv.org/abs/2511.21214) | **Reasoning Models**&**Safety Alignment**&**Self-Alignment** |
| 25.11 | University of Central Florida, SAFERR AI Lab | arxiv | [SafeR-CLIP: Mitigating NSFW Content in Vision-Language Models While Preserving Pre-Trained Knowledge](https://arxiv.org/abs/2511.16743) | **NSFW Mitigation**&**Vision-Language Models**&**Representation-Aware Fine-tuning** |
| 25.11 | Aptima, Inc. | arxiv | [Proactive Defense: Compound AI for Detecting Persuasion Attacks and Measuring Inoculation Effectiveness](https://arxiv.org/abs/2511.21749) | **Persuasion Attack Detection**&**Inoculation Theory**&**Compound AI Framework** |
| 25.11 | George Washington University | arxiv | [Semantics as a Shield: Label Disguise Defense (LDD) against Prompt Injection in LLM Sentiment Classification](https://arxiv.org/abs/2511.21752) | **Prompt Injection Defense**&**Label Disguise Defense (LDD)**&**Adversarial Robustness** |
| 25.12 | NVIDIA, University of Illinois Urbana-Champaign, Johns Hopkins University | arxiv | [Mitigating Indirect Prompt Injection via Instruction-Following Intent Analysis](https://arxiv.org/abs/2512.00966) | **Prompt Injection Defense**&**Instruction-Following Intent Analysis**&**LLM Agent Security** |
| 25.12 | University of Galway | arxiv | [Securing Large Language Models (LLMs) from Prompt Injection Attacks](https://arxiv.org/abs/2512.01326) | **Prompt Injection Defense**&**Fine-tuning Security**&**Adversarial Robustness** |
| 25.12 | Fudan University, University of California Davis, Uniphore | arxiv | [OMNIGUARD: Unified Omni-Modal Guardrails with Deliberate Reasoning](https://arxiv.org/abs/2512.02306) | **Omni-Modal Guardrails**&**Safety Reasoning**&**Cross-Modal Alignment** |
| 25.12 | Algoverse, New Jersey Institute of Technology | AAAI 2026 | [When Refusals Fail: Unstable Safety Mechanisms in Long-Context LLM Agents](https://arxiv.org/abs/2512.02445) | **LLM Agent Safety**&**Long-Context Evaluation**&**Refusal Behavior** |
| 25.12 | Repello AI | arxiv | [CREST: Universal Safety Guardrails Through Cluster-Guided Cross-Lingual Transfer](https://arxiv.org/abs/2512.02711) | **Multilingual Safety**&**Cross-Lingual Transfer**&**Low-Resource Languages**&**Guardrails** |
| 25.12 | University of Illinois Urbana-Champaign | arxiv | [Matching Ranks over Probability Yields Truly Deep Safety Alignment](https://arxiv.org/abs/2512.05518) | **LLM Safety Alignment**&**Prefilling Attack**&**Attention Regularization** |
| 25.12 | South China University of Technology | arxiv | [ARGUS: Defending Against Multimodal Indirect Prompt Injection via Steering Instruction-Following Behavior](https://arxiv.org/abs/2512.05745) | **Multimodal LLM Safety**&**Indirect Prompt Injection**&**Activation Steering** |
| 25.12 | Technical University of Munich | arxiv | [Trusted AI Agents in the Cloud](https://arxiv.org/abs/2512.05951) | **Trusted AI Agents**&**Confidential Computing**&**Cloud Security** |
| 25.12 | Polytechnique Montr√©al | arxiv | [Securing the Model Context Protocol: Defending LLMs Against Tool Poisoning and Adversarial Attacks](https://arxiv.org/abs/2512.06556) | **Model Context Protocol**&**Tool Poisoning**&**LLM Agent Security** |
| 25.12 | University of Illinois Urbana-Champaign, CENTAI Institute | arxiv | [GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering](https://arxiv.org/abs/2512.06655) | **LLM Safety Steering**&**Sparse Autoencoders**&**Graph Regularization** |
| 25.12 | Shanghai Jiao Tong University | arxiv | [Patronus: Identifying and Mitigating Transferable Backdoors in Pre-trained Language Models](https://arxiv.org/abs/2512.06899) | **Transferable Backdoor**&**Pre-trained Language Models**&**Backdoor Defense** |
| 25.12 | Amazon AGI Foundations | arxiv | [SABER: Small Actions, Big Errors ‚Äî Safeguarding Mutating Steps in LLM Agents](https://arxiv.org/abs/2512.07850) | **LLM Agents**&**Safety Safeguards**&**Tool-Using Reliability** |
| 25.12 | Zhejiang University | NDSS 2026 | [Attention is All You Need to Defend Against Indirect Prompt Injection Attacks in LLMs](https://arxiv.org/abs/2512.08417) | **Indirect Prompt Injection**&**LLM Security**&**Attention-based Defense** |



## üíªPresentations & Talk


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

## üßë‚Äçüè´Scholars
