# Privacy

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"


## üìëPapers

| Date  |                                                                                   Institute                                                                                    |       Publication        |                                                                                          Paper                                                                                           |                                               Keywords                                               |
|:-----:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------:|
| 18.02 |                                                                                  Google Brain                                                                                  |   USENIX Security 2021   |        [The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting)        |                                       **Memorization**&**LSTM**                                      |
| 19.12 |                                                                                   Microsoft                                                                                    |         CCS2020          |                                [Analyzing Information Leakage of Updates to Natural Language Models](https://dl.acm.org/doi/abs/10.1145/3372297.3417880)                                 |                          **Privacy Leakage**&**Model Update**&**Duplicated**                         |
| 21.07 |                                                                                Google Research                                                                                 |         ACL2022          |                                         [Deduplicating Training Data Makes Language Models Better](https://aclanthology.org/2022.acl-long.577/)                                          |                       **Privacy Protected**&**Deduplication**&**Memorization**                       |
| 21.10 |                                                                                    Stanford                                                                                    |         ICLR2022         |                                    [Large language models can be strong differentially private learners](https://openreview.net/forum?id=bVuP3ltATMz)                                    |                            **Differential Privacy**&**Gradient Clipping**                            |
| 22.02 |                                                                                Google Research                                                                                 |         ICLR2023         |                                           [Quantifying Memorization Across Neural Language Models](https://openreview.net/forum?id=TatRHT_1cK)                                           |                                **Memorization**&**Verbatim Sequence**                                |
| 22.02 |                                                                                UNC Chapel Hill                                                                                 |         ICML2022         |                               [Deduplicating Training Data Mitigates Privacy Risks in Language Models](https://proceedings.mlr.press/v162/kandpal22a.html)                               |                            **Memorization**&**Deduplicate Training Data**                            |
| 22.05 |                                                                                      UCSD                                                                                      |        EMNLP2022         |                           [An Empirical Analysis of Memorization in Fine-tuned Autoregressive Language Models](https://aclanthology.org/2022.emnlp-main.119/)                            |                                  **Privacy Risks**&**Memorization**                                  |
| 22.05 |                                                                                   Princeton                                                                                    |         NIPS2022         | [Recovering Private Text in Federated Learning of Language Models](https://proceedings.neurips.cc/paper_files/paper/2022/hash/35b5c175e139bff5f22a5361270fce87-Abstract-Conference.html) |                               **Federated Learning**&**Gradient Based**                              |
| 22.05 |                                                                   University of Illinois at Urbana-Champaign                                                                   |   EMNLP2022(findings)    |                              [Are Large Pre-Trained Language Models Leaking Your Personal Information?](https://aclanthology.org/2022.findings-emnlp.148/)                               |                      **Personal Information**&**Memorization**&**Privacy Risk**                      |
| 22.10 |                                                                                Google Research                                                                                 |         INLG2023         |                      [Preventing Generation of Verbatim Memorization in Language Models Gives a False Sense of Privacy](https://aclanthology.org/2023.inlg-main.3/)                      |                    **Verbatim Memorization**&**Filter**&**Style Transfer Prompts**                   |
| 23.02 |                                                                             University of Waterloo                                                                             | Security and Privacy2023 |             [Analyzing Leakage of Personally Identifiable Information in Language Models](https://www.computer.org/csdl/proceedings-article/sp/2023/933600a346/1NrbXJj80H6)              |                    **PII Leakage**&**PII Reconstruction**&**Differential Privacy**                   |
| 23.04 |                                                                 Hong Kong University of Science and Technology                                                                 |   EMNLP2023(findings)    |                                                  [Multi-step Jailbreaking Privacy Attacks on ChatGPT](https://arxiv.org/abs/2304.05197)                                                  |                                      **Privacy**&**Jailbreaks**                                      |
| 23.05 |                                                                   University of Illinois at Urbana-Champaign                                                                   |          arxiv           |                        [Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage](https://arxiv.org/abs/2305.12707)                         |                                       **Co-occurrence**&**PII**                                      |
| 23.05 |                                                                       The University of Texas at Dallas                                                                        |         ACL2023          |                               [Controlling the Extraction of Memorized Datafrom Large Language Models via Prompt-Tuning](https://arxiv.org/abs/2305.11759)                               |                                  **Prompt-Tuning**&**Memorization**                                  |
| 23.06 |                                                                   University of Illinois at Urbana-Champaign                                                                   |          arxiv           |                                      [DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models](https://arxiv.org/abs/2306.11698)                                      |                          **Robustness**&**Ethics**&**Privacy**&**Toxicity**                          |
| 23.09 |                                                                                UNC Chapel Hill                                                                                 |          arxiv           |                         [Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks](https://arxiv.org/abs/2309.17410)                          |         **Hidden States Attack**&**Hidden States Defense**&**Deleting Sensitive Information**        |
| 23.09 |                                                                         Princeton University&Microsoft                                                                         |          arxiv           |                                [Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation](https://arxiv.org/abs/2309.11765)                                |                           **In-Context Learning**&**Differential Privacy**                           |
| 23.10 |                                                                                      ETH                                                                                       |          arxiv           |                                   [Beyond Memorization: Violating Privacy Via Inference with Large Language Models](https://arxiv.org/abs/2310.07298)                                    |                      **Context Inference**&**Privacy-Invasive**&**Extract PII**                      |
| 23.10 |                                                     University of Washington & Allen Institute for Artificial Intelligence                                                     |          arxiv           |                       [Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory](https://arxiv.org/abs/2310.17884)                        |                       **Benchmark**&**Contextual Privacy**&**Chain-of-thought**                      |
| 23.10 |                                                                        Georgia Institute of Technology                                                                         |          arxiv           |                                            [Unlearn What You Want to Forget: Efficient Unlearning for LLMs](https://arxiv.org/abs/2310.20150)                                            |                   **Unlearning**&**Teacher-student Framework**&**Data Protection**                   |
| 23.10 |                                                                               Tianjin University                                                                               |        EMNLP2023         |                                      [DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models](https://arxiv.org/abs/2310.20138)                                       |                 **Privacy Neuron Detection**&**Model Editing**&**Data Memorization**                 |
| 23.11 |                                                                              Zhejiang University                                                                               |          arxiv           |                                     [Input Reconstruction Attack against Vertical Federated Large Language Models](https://arxiv.org/abs/2311.07585)                                     |             **Vertical Federated Learning**&**Input Reconstruction**&**Privacy Concerns**            |
| 23.11 |                                                          Georgia Institute of Technology, Carnegie Mellon University                                                           |          arxiv           |                                        [Reducing Privacy Risks in Online Self-Disclosures with Language Models](https://arxiv.org/abs/2311.09538)                                        |             **Online Self-Disclosure**&**Privacy Risks**&**Self-Disclosure Abstraction**             |
| 23.11 |                                                                               Cornell University                                                                               |          arxiv           |                                                               [Language Model Inversion](https://arxiv.org/abs/2311.13647)                                                               |                       **Model Inversion**&**Prompt Reconstruction**&**Privacy**                      |
| 23.11 |                                                                                   Ant Group                                                                                    |          arxiv           |                                                   [PrivateLoRA for Efficient Privacy Preserving LLM](https://arxiv.org/abs/2311.14030)                                                   |                                    **Privacy Preserving**&**LoRA**                                   |
| 23.12 |                                                                               Drexel University                                                                                |          arXiv           |                              [A Survey on Large Language Model (LLM) Security and Privacy: The Good the Bad and the Ugly](https://arxiv.org/abs/2312.02003)                              |                                 **Security**&**Privacy**&**Attacks**                                 |
| 23.12 |                                                University of Texas at Austin, Princeton University, MIT, University of Chicago                                                 |          arxiv           |                                      [DP-OPT: MAKE LARGE LANGUAGE MODEL YOUR PRIVACY-PRESERVING PROMPT ENGINEER](https://arxiv.org/abs/2312.03724)                                       |                              **Prompt Tuning**&**Differential Privacy**                              |
| 23.12 |                                                                         Delft University of Technology                                                                         |        ICSE 2024         |                                               [Traces of Memorisation in Large Language Models for Code](https://arxiv.org/abs/2312.11658)                                               |                           **Code Memorisation**&**Data Extraction Attacks**                          |
| 23.12 |                                                                         University of Texas at Austin                                                                          |          arXiv           |                     [SentinelLMs: Encrypted Input Adaptation and Fine-tuning of Language Models for Private and Secure Inference](https://arxiv.org/abs/2312.17342)                      |                        **Privacy**&**Security**&**Encrypted Input Adaptation**                       |
| 23.12 |                                                             Rensselaer Polytechnic Institute, Columbia University                                                              |          arXiv           |                             [Differentially Private Low-Rank Adaptation of Large Language Model Using Federated Learning](https://arxiv.org/abs/2312.17493)                              |               **Federated Learning**&**Differential Privacy**&**Efficient Fine-Tuning**              |
| 24.01 |                                                     Harbin Institute of Technology Shenzhen&Peng Cheng Laboratory Shenzhen                                                     |          arxiv           |                             [SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models](https://arxiv.org/abs/2401.00793)                              | **Privacy-Preserving Inference (PPI)**&**Secure Multi-Party Computing (SMPC)**&**Transformer Models** |
| 24.01 |                                          NUS (Chongqing) Research Institute, Huawei Noah‚Äôs Ark Lab, National University of Singapore                                           |          arxiv           |                                                    [Teach Large Language Models to Forget Privacy](https://arxiv.org/abs/2401.00870)                                                     |                    **Data Privacy**&**Prompt Learning**&**Problem Decomposition**                    |
| 24.01 |                                                                 Princeton University, Google DeepMind, Meta AI                                                                 |          arxiv           |                                     [Private Fine-tuning of Large Language Models with Zeroth-order Optimization](https://arxiv.org/abs/2401.04343)                                      |                        **Differential Privacy**&**Zeroth-order Optimization**                        |
| 24.02 |                                                                        Florida International University                                                                        |          arxiv           |                                          [Security and Privacy Challenges of Large Language Models: A Survey](https://arxiv.org/abs/2402.00888)                                          |                            **Security**&**Privacy Challenges**&**Suevey**                            |
| 24.02 |                                             Northeastern University, Carnegie Mellon University, Rensselaer Polytechnic Institute                                              |          arxiv           |                                         [Human-Centered Privacy Research in the Age of Large Language Models](https://arxiv.org/abs/2402.01994)                                          |                     **Generative AI**&**Privacy**&**Human-Computer Interaction**                     |
| 24.02 |                                                                CISPA Helmholtz Center for Information Security                                                                 |          arxiv           |                                                [Conversation Reconstruction Attack Against GPT Models](https://arxiv.org/abs/2402.02987)                                                 |                 **Conversation Reconstruction Attack**&**Privacy risks**&**Security**                |
| 24.02 |                                                             Columbia University, M365 Research, Microsoft Research                                                             |          arxiv           |                                             [Differentially Private Training of Mixture of Experts Models](https://arxiv.org/abs/2402.07334)                                             |                            **Differential Privacy**&**Mixture of Experts**                           |
| 24.02 |                                                               Stanford University, Truera ,Princeton University                                                                |          arxiv           |                                      [De-amplifying Bias from Differential Privacy in Language Model Fine-tuning](https://arxiv.org/abs/2402.04489)                                      |                      **Fairness**&**Differential Privacy**&**Data Augmentation**                     |
| 24.02 |                                                                    Sun Yat-sen University, Google Research                                                                     |          arxiv           |                                          [Privacy-Preserving Instructions for Aligning Large Language Models](https://arxiv.org/abs/2402.13659)                                          |                             **Privacy Risks**&**Synthetic Instructions**                             |
| 24.02 |                                                                   National University of Defense Technology                                                                    |          arxiv           |            [LLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification](https://arxiv.org/abs/2402.16515)            |       **Privacy Data Augmentation**&**Knowledge Distillation**&**Medical Text Classification**       |
| 24.02 |                                                                     Michigan State University, Baidu Inc.                                                                      |          arxiv           |                                [The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2402.16893)                                |                         **Privacy**&**Retrieval-Augmented Generation (RAG)**                         |
| 24.03 |                                                                                 Virginia Tech                                                                                  |          arxiv           |                                                [Privacy-Aware Semantic Cache for Large Language Models](https://arxiv.org/abs/2403.02694)                                                |                           **Federated Learning**&**Cache Hit**&**Privacy**                           |
| 24.03 |                                                                              Tsinghua University                                                                               |          arxiv           |                 [CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following](https://arxiv.org/abs/2403.03129)                  |             **Small Language Models**&**Privacy**&**Context-Aware Instruction Following**            |
| 24.03 |                                                           Shandong University, Leiden University, Drexel University                                                            |          arxiv           |                                       [On Protecting the Data Privacy of Large Language Models (LLMs): A Survey](https://arxiv.org/abs/2403.05156)                                       |                          **Data Privacy**&**Privacy Protection**&**Survey**                          |
| 24.03 | Arizona State University, University of Minnesota, University of Science and Technology of China, North Carolina State University, University of North Carolina at Chapel Hill |          arxiv           |                                       [Privacy-preserving Fine-tuning of Large Language Models through Flatness](https://arxiv.org/abs/2403.04124)                                       |                           **Differential Privacy**&**Model Generalization**                          |
| 24.03 | University of Southern California | arxiv | [Differentially Private Next-Token Prediction of Large Language Models](https://arxiv.org/abs/2403.15638) | **Differential Privacy**|
| 24.04 | University of Maryland, Oregon State University, ELLIS Institute T√ºbingen & MPI Intelligent Systems, T√ºbingen AI Center, Google DeepMind | arxiv | [Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models](https://arxiv.org/abs/2404.01231) | **Privacy Backdoors**&**Membership Inference**&**Model Poisoning** |


## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles
| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.11 | News  |Wild: GPT-3.5 leaked a random dude's photo in the output. | [link](https://twitter.com/thealexker/status/1719896871009694057) |

## üßë‚Äçüè´Scholars
