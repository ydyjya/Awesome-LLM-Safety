Other resources: 

https://zhuanlan.zhihu.com/p/103593948

https://zhuanlan.zhihu.com/p/84174278


1. <details>
    <summary>[ICLR 2014] <b>Intriguing properties of neural networks</b></summary>
    <ul>
        <li>Adding slight perturbations to samples can cause neural network models to misclassify these samples.</li>
        <li>These samples are referred to as <b>adversarial examples</b>.</li>
        <li>This paper is generally considered the seminal work on adversarial examples.</li>
    </ul>
   </details>

2. <details>
          <summary>[ICLR 2015] FGSM: Explaining and harnessing adversarial examples.</summary>
     <ul>
        <li>Neural networks are vulnerable to adversarial examples due to their <b>inherent linearity</b> in high-dimensional spaces.</li>
        <li>Introduces the Fast Gradient Sign Method (FGSM), a technique to efficiently generate adversarial examples.</li>
    </ul>
   </details>

3. <details>
          <summary>[ICLR 2018] PGD: Towards Deep Learning Models Resistant to Adversarial Attacks.</summary>
   </details>

4. <details>
          <summary>Threat of adversarial attacks on deep learning in computer vision: Survey II</summary>
   </details>

5. <details>
          <summary>A systematic review of robustness in deep learning for computer vision: Mind the gap?</summary>
   </details>

6. <details>
          <summary>[ICLR 2019] Benchmarking neural network robustness to common corruptions and perturbations.</summary>
   </details>

7. <details>
          <summary>[NDSS 2019] <b>TextBugger: Generating Adversarial Text Against Real-world Applications</b></summary>
   </details>

8. <details>
          <summary>[ICLR 2020] FreeLB: Enhanced Adversarial Training for Natural Language Understanding</summary>
   </details>

9. <details>
          <summary>[AAAI 2020] TextFooler:Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment</summary>
   </details>

10. <details>
          <summary>[ACL 2019] PWWS: Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency</summary>
   </details>
