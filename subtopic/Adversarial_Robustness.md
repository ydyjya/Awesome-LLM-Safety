1. <details>
    <summary>[ICLR 2014] <b>Intriguing properties of neural networks</b></summary>
    <ul>
        <li>Adding slight perturbations to samples can cause neural network models to misclassify these samples.</li>
        <li>These samples are referred to as <b>adversarial examples</b>.</li>
        <li>This paper is generally considered the seminal work on adversarial examples.</li>
    </ul>
   </details>

2. <details>
          <summary>[ICLR 2015] Explaining and harnessing adversarial examples.</summary>
     <ul>
        <li>Neural networks are vulnerable to adversarial examples due to their <b>inherent linearity</b> in high-dimensional spaces.</li>
        <li>Introduces the Fast Gradient Sign Method (FGSM), a technique to efficiently generate adversarial examples.</li>
    </ul>
   </details>

3. <details>
          <summary>[ICLR 2018] Towards Deep Learning Models Resistant to Adversarial Attacks.</summary>
   </details>

4. <details>
          <summary>Threat of adversarial attacks on deep learning in computer vision: Survey II</summary>
   </details>

5. <details>
          <summary>A systematic review of robustness in deep learning for computer vision: Mind the gap?</summary>
   </details>

6. <details>
          <summary>[ICLR 2019] Benchmarking neural network robustness to common corruptions and perturbations.</summary>
   </details>

7. <details>
          <summary>[ACL 2022] <b>TextBugger: Generating Adversarial Text Against Real-world Applications</b></summary>
   </details>
