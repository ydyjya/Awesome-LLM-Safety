1. <details>
    <summary>[ICLR 2014] Intriguing properties of neural networks</summary>
    <ul>
        <li>Adding slight perturbations to samples can cause neural network models to misclassify these samples.</li>
        <li>These samples are referred to as <b>adversarial examples</b>.</li>
        <li>This paper is generally considered the seminal work on adversarial examples.</li>
    </ul>
   </details>

2. <details>
          <summary>[ICLR 2015] Explaining and harnessing adversarial examples.</summary>
     <ul>
        <li>Neural networks are vulnerable to adversarial examples due to their <b>inherent linearity</b> in high-dimensional spaces.</li>
        <li>Introduces the Fast Gradient Sign Method (FGSM), a technique to efficiently generate adversarial examples.</li>
    </ul>
   </details>

3. <details>
          <summary>[ICLR 2018] Towards Deep Learning Models Resistant to Adversarial Attacks.</summary>
   </details>
