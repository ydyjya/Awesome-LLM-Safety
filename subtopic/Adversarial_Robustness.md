1. <details>
    <summary>[ICLR 2014] Intriguing properties of neural networks</summary>
    <ul>
        <li>Adding slight perturbations to samples can cause neural network models to misclassify these samples.</li>
        <li>These samples are referred to as <b>adversarial examples</b>.</li>
        <li>This paper is generally considered the seminal work on adversarial examples.</li>
    </ul>
   </details>

2. <details>
          <summary>[ICLR 2015] Explaining and harnessing adversarial examples.</summary>
           
   </details>
