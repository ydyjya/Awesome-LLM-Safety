# Defense

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"


## üìëPapers

| Date  |                                                              Institute                                                               | Publication |                                                                           Paper                                                                           |                                     Keywords                                      |
|:-----:|:------------------------------------------------------------------------------------------------------------------------------------:|:-----------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------:|
| 21.07 |                                                           Google Research                                                            |   ACL2022   |                          [Deduplicating Training Data Makes Language Models Better](https://aclanthology.org/2022.acl-long.577/)                          |             **Privacy Protected**&**Deduplication**&**Memorization**              |
| 23.08 |                                                       Georgia Tech, Intel Labs                                                       |    arxiv    |                        [LLM Self Defense: By Self Examination LLMs Know They Are Being Tricked](https://arxiv.org/abs/2308.07308)                         |      **Adversarial Attacks**&**Self Defense**&**Harmful Content Detection**       |
| 23.08 |                                                        University of Michigan                                                        |    arxiv    |                                  [DETECTING LANGUAGE MODEL ATTACKS WITH PERPLEXITY](https://arxiv.org/abs/2308.14132v3)                                   |           **Adversarial Suffixes**&**Perplexity**&**Attack Detection**            |
| 23.09 |                                                        University of Maryland                                                        |    arxiv    |                                  [Certifying LLM Safety against Adversarial Prompting](https://arxiv.org/abs/2309.02705)                                  |                     **Safety Filter**&**Adversarial Prompts**                     |
| 23.09 |                                                        University of Maryland                                                        |    arxiv    |                       [BASELINE DEFENSES FOR ADVERSARIAL ATTACKS AGAINST ALIGNED LANGUAGE MODELS](https://arxiv.org/abs/2309.00614)                       |          **Perplexity**&**Input Preprocessing**&**Adversarial Training**          |
| 23.09 |                                                  The Pennsylvania State University                                                   |    arxiv    |                         [DEFENDING AGAINST ALIGNMENT-BREAKING ATTACKS VIA ROBUSTLY ALIGNED LLM](https://arxiv.org/abs/2309.14348)                         |  **Alignment-Breaking Attacks**&**Adversarial Prompts**&**Jailbreaking Prompts**  |
| 23.10 |                                                      University of Pennsylvania                                                      |    arxiv    |                        [SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks](https://arxiv.org/abs/2310.03684)                        |               **Jailbreak**&**Adversarial Attack**&**Perturbation**               |
| 23.10 |                                                      Michigan State University                                                       |    arXiv    |                         [Jailbreaker in Jail: Moving Target Defense for Large Language Models](https://arxiv.org/abs/2310.02417)                          |  **Dialogue System**&**Trustworthy Machine Learning**&**Moving Target Defense**   |
| 23.10 |                                                          Peking University                                                           |    arxiv    |                  [Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations](https://arxiv.org/abs/2310.06387)                  |   **In-Context Learning**&**Adversarial Attacks**&**In-Context Demonstrations**   |
| 23.11 |                                                   University of California Irvine                                                    |    arxiv    |                     [Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield](https://arxiv.org/abs/2311.00172)                     |                **Adversarial Prompt Shield**&**Safety Classifier**                |
| 23.11 |                                                   Child Health Evaluative Sciences                                                   |    arxiv    |                         [Pyclipse, a library for deidentification of free-text clinical notes](https://arxiv.org/abs/2311.02748)                          |                    **Clinical Text Data**&**Deidentification**                    |
| 23.11 |                                                         Tsinghua University                                                          |    arxiv    |               [Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization](https://arxiv.org/abs/2311.09096)                |            **Jailbreaking Attacks**&**Goal Prioritization**&**Safety**            |
| 23.11 |        University of Southern California, Harvard University, University of California Davis, University of Wisconsin-Madison        |    arxiv    |            [Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations](https://arxiv.org/abs/2311.09763)            |      **Backdoor Attacks**&**Defensive Demonstrations**&**Test-Time Defense**      |
| 23.11 |                                                 University of Maryland College Park                                                  |    arxiv    |           [Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information](https://arxiv.org/abs/2311.11509)            | **Adversarial Prompt Detection**&**Perplexity Measures**&**Token-level Analysis** |
| 23.12 |                                      Rensselaer Polytechnic Institute, Northeastern University                                       |    arxiv    |                     [Combating Adversarial Attacks through a Conscience-Based Alignment Framework](https://arxiv.org/abs/2312.00029)                      |         **Adversarial Attacks**&**Conscience-Based Alignment**&**Safety**         |
| 23.12 |                                          Azure Research, Microsoft Security Response Center                                          |    arXiv    |                           [Maatphor: Automated Variant Analysis for Prompt Injection Attacks](https://arxiv.org/abs/2312.11513)                           |            **Prompt Injection Attacks**&**Automated Variant Analysis**            |
| 23.12 |              University of Massachusetts Amherst, Columbia University, Google, Stanford University, New York University              |    arxiv    |                           [Learning and Forgetting Unsafe Examples in Large Language Models](https://arxiv.org/abs/2312.12736)                            |          **Safety Issues**&**ForgetFilter Algorithm**&**Unsafe Content**          |
| 23.12 |                                     UC Berkeley, King Abdulaziz City for Science and Technology                                      |    arXiv    |                              [Jatmo: Prompt Injection Defense by Task-Specific Finetuning](https://arxiv.org/abs/2312.17673)                              |                           Prompt Injection&LLM Security                           |
| 24.01 |                                                       Arizona State University                                                       |    arxiv    | [The Art of Defending: A Systematic Evaluation and Analysis of LLM Defense Strategies on Safety and Over-Defensiveness](https://arxiv.org/abs/2401.00287) |             **Safety**&**Over-Defensiveness**&**Defense Strategies**              |
| 24.01 |                                        Logistic and Supply Chain MultiTech R&D Centre (LSCM)                                         |    arxiv    |          [Detection and Defense Against Prominent Attacks on Preconditioned LLM-Integrated Virtual Assistants](https://arxiv.org/abs/2401.00994)          |                      **Preconditioning**&**Cyber Security**                       |
| 24.01 | The Hong Kong University of Science and Technology, University of Illinois at Urbana-Champaign, The Hong Kong Polytechnic University |    arxiv    |                          [MLLM-Protector: Ensuring MLLM‚Äôs Safety without Hurting Performance](https://arxiv.org/abs/2401.02906)                           |   **Multimodal Large Language Models (MLLMs)**&**Safety**&**Malicious Attacks**   |
| 24.01 |                                                      Carnegie Mellon University                                                      |    arxiv    |                                    [TOFU: A Task of Fictitious Unlearning for LLMs](https://arxiv.org/abs/2401.06121)                                     |               **Data Privacy**&**Ethical Concerns**&**Unlearning**                |
| 24.01 |                                              Wuhan University, The University of Sydney                                              |    arxiv    |                  [Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender](https://arxiv.org/abs/2401.06561)                   |              **Intention Analysis**&**Jailbreak Defense**&**Safety**              |
| 24.01 |                                                 The Hong Kong Polytechnic University                                                 |    arxiv    |         [Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks Against LLM-Integrated Applications](https://arxiv.org/abs/2401.07612)         |                   **AI Security**&**Prompt Injection Attacks**                    |


## üíªPresentations & Talk


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

## üßë‚Äçüè´Scholars
