# Robustness

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"

## üìëPapers

| Date  |                                                                                            Institute                                                                                             |                                     Publication                                     |                                                                                                   Paper                                                                                                   |                                             Keywords                                             |
|:-----:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:------------------------------------------------------------------------------------------------:|
| 23.02 |                                                                                        Microsoft Research                                                                                        | ICLR 2023(workshop on Trustworthy and Reliable Large-Scale Machine Learning Models) |                                           [On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective](https://arxiv.org/abs/2302.12095)                                            |        **Robustness Evaluation**&**Adversarial Robustness**&**Out-of-Distribution (OOD)**        |
| 23.05 |                                                                                  Harbin Institute of Technology                                                                                  |                                      NAACL2024                                      |                                      [Enhancing Large Language Models Against Inductive Instructions with Dual-critique Prompting](https://arxiv.org/abs/2305.13733)                                      |         **Large Language Models**&**Inductive Instructions**&**Dual-critique Prompting**         |
| 23.05 |                                                                  National Key Laboratory for Multimedia Information Processing                                                                   |                                      NAACL2024                                      |                                              [DialogVCS: Robust Natural Language Understanding in Dialogue System Upgrade](https://arxiv.org/abs/2305.14751)                                              |      **Natural Language Understanding**&**Dialogue System**&**Multi-label Classification**       |
| 23.06 |                                                                            University of Illinois at Urbana-Champaign                                                                            |                                        arxiv                                        |                                              [DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models](https://arxiv.org/abs/2306.11698)                                               |                        **Robustness**&**Ethics**&**Privacy**&**Toxicity**                        |
| 23.08 |                                                              CISPA Helmholtz Center for Information Security & Tsinghua University                                                               |                                        arxiv                                        |                       [Robustness Over Time: Understanding Adversarial Examples‚Äô Effectiveness on Longitudinal Versions of Large Language Models](https://arxiv.org/abs/2308.07847)                       |                         **Longitudinal Study**&**Robustness Assessment**                         |
| 23.11 |                                                                                               CMU                                                                                                |                          AACL2023(ART or Safety workshop)                           |                                                                    [Measuring Adversarial Datasets](https://arxiv.org/abs/2311.03566)                                                                     |                **Adversarial Robustness**&**AI Safety**&**Adversarial Datasets**                 |
| 23.11 |                                                                                        Amazon Alexa AI-NU                                                                                        |                                        arXiv                                        |                                                       [JAB: Joint Adversarial Prompting and Belief Augmentation](https://arxiv.org/abs/2311.09473)                                                        |                 **Adversarial Prompting**&T**oxicity Reduction**&**Robustness**                  |
| 23.11 |                                                                                        Amazon Alexa AI-NU                                                                                        |                                      NAACL2024                                      |                                                       [JAB: Joint Adversarial Prompting and Belief Augmentation](https://arxiv.org/abs/2311.09473)                                                        |                 **Adversarial Prompting**&T**oxicity Reduction**&**Robustness**                  |
| 23.11 |                                                                                    Michigan State University                                                                                     |                                 NAACL2024(findings)                                 |                                           [A Robust Semantics-based Watermark for Large Language Models against Paraphrasing](https://arxiv.org/abs/2311.08721)                                           |                     **Watermark**&**Large Language Models**&**Paraphrasing**                     |
| 24.01 |                                                               University of Trento, Concordia University, Mila-Quebec AI Institute                                                               |                                        arxiv                                        |                                                                 [Are LLMs Robust for Spoken Dialogues?](https://arxiv.org/abs/2401.02297)                                                                 |         **Task-Oriented Dialogues**&**Automatic Speech Recognition**&**Error Analysis**          |
| 24.01 |                                                                         School of Computer Science University of Windsor                                                                         |                                        arxiv                                        |             [Finding a Needle in the Adversarial Haystack: A Targeted Paraphrasing Approach For Uncovering Edge Cases with Minimal Distribution Distortion](https://arxiv.org/abs/2401.11373)             |           **Adversarial Attacks**&**Targeted Paraphrasing**&**Reinforcement Learning**           |
| 24.02 |                                                                                     University of Cambridge                                                                                      |                                        arxiv                                        |                                   [Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment](https://arxiv.org/abs/2402.14016)                                   |                       **LLM as a Judge**&**Universal Adversarial Attacks**                       |
| 24.02 |                                                                                  Technical University of Munich                                                                                  |                                        arxiv                                        |                                     [Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images](https://arxiv.org/abs/2402.14899)                                     |    **Multimodal Large Language Models**&**Chain-of-Thought Reasoning**&**Adversarial Images**    |
| 24.03 |                                                     Beijing Institute of Technology, The University of Sydney, Hong Kong Baptist University                                                      |                                        arxiv                                        |                                                    [Few-Shot Adversarial Prompt Learning on Vision-Language Models](https://arxiv.org/abs/2403.14774)                                                     |             **Few-Shot Learning**&**Adversarial Prompt**&**Vision-Language Models**              |
| 24.03 |                                                                                               UIUC                                                                                               |                                      NAACL2024                                      |                                                                   [Fact Checking Beyond Training Set](https://arxiv.org/abs/2403.18671)                                                                   |                 **Fact Checking**&**Domain Adaptation**&**Adversarial Training**                 |
| 24.03 |                                                                   Institute of Data Science, National University of Singapore                                                                    |                                      NAACL2024                                      |                                  [SemRoDe: Macro Adversarial Training to Learn Representations That are Robust to Word-Level Attacks](https://arxiv.org/abs/2403.18423)                                   |            **Adversarial Training**&**Word-Level Attacks**&**Robust Representations**            |
| 24.03 |                                                                                     Georgia State University                                                                                     |                                 NAACL2024(findings)                                 |                                  [RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning](https://arxiv.org/abs/2403.11082)                                   |            **Sentence Embeddings**&**Adversarial Learning**&**Contrastive Learning**             |
| 24.04 |                             University of Chinese Academy of Sciences, Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences                             |                                     COLING 2024                                     |                                      [Humanizing Machine-Generated Content: Evading AI-Text Detection through Adversarial Attack](https://arxiv.org/abs/2404.01907)                                       |          **Adversarial Attack**&**AI-Text Detection**&**Dynamic Adversarial Learning**           |
| 24.04 |                                              Institute for Intelligent Computing, Alibaba Group; School of Information Management, Wuhan University                                              |                                        arxiv                                        |                                       [Enhance Robustness of Language Models Against Variation Attack through Graph Integration](https://arxiv.org/abs/2404.12014)                                        |                       **Chinese Adversarial Attacks**&**Variation Graph**                        |
| 24.05 |                                                                                   Mila, Universit√© de Montr√©al                                                                                   |                                        arxiv                                        |                                                    [Efficient Adversarial Training in LLMs with Continuous Attacks](https://arxiv.org/abs/2405.15589)                                                     |                         **Adversarial Training**&**Continuous Attacks**                          |
| 24.05 |                                                                                     University of Edinburgh                                                                                      |                                        arxiv                                        |                                [Evaluating the Adversarial Robustness of Retrieval-Based In-Context Learning for Large Language Models](https://arxiv.org/abs/2405.15984)                                 |        **Adversarial Robustness**&**In-Context Learning**&**Retrieval-Augmented Methods**        |
| 24.05 |                                                                          Tokyo University of Agriculture and Technology                                                                          |                                        arxiv                                        |                                             [Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent](https://arxiv.org/abs/2405.20770)                                              |                   **Adversarial Robustness**&**LLM Agent**&**Textual Attacks**                   |
| 24.06 |                                                                                       Polytechnic of Porto                                                                                       |                                      DCAI2024                                       |                                                  [Adversarial Evasion Attack Efficiency against Large Language Models](https://arxiv.org/abs/2406.08050)                                                  |                     **Adversarial Attacks**&**Robustness**&**Cybersecurity**                     |
| 24.06 |                                                                                 National University of Singapore                                                                                 |                                      ICML2024                                       |                                [Helpful or Harmful Data? Fine-tuning-free Shapley Attribution for Explaining Language Model Predictions](https://arxiv.org/abs/2406.04606)                                | **Fine-tuning-free Shapley Attribution**&**Instance Attribution**&**Language Model Predictions** |
| 24.06 |                                                                                              KAIST                                                                                               |                                        arxiv                                        |                                        [Adversarial Style Augmentation via Large Language Model for Robust Fake News Detection](https://arxiv.org/abs/2406.11260)                                         |                    **Adversarial Style Augmentation**&**Fake News Detection**                    |
| 24.07 |                                                                    Hong Kong University of Science and Technology (Guangzhou)                                                                    |                                        arxiv                                        |                                    [On Evaluating The Performance of Watermarked Machine-Generated Texts Under Adversarial Attacks](https://arxiv.org/abs/2407.04794)                                     |            **Watermarked Texts**&**Adversarial Attacks**&**Machine-Generated Texts**             |
| 24.07 |                                                                                              FAR AI                                                                                              |                                        arxiv                                        |                                                              [Exploring Scaling Trends in LLM Robustness](https://arxiv.org/abs/2407.18213)                                                               |                  **Scaling Trends**&**LLM Robustness**&**Adversarial Training**                  |
| 24.07 |                                                                                      University of Alicante                                                                                      |                                        arxiv                                        |                                    [Detecting and Understanding Vulnerabilities in Language Models via Mechanistic Interpretability](https://arxiv.org/abs/2407.19842)                                    |       **Mechanistic Interpretability**&**Adversarial Attacks**&**Vulnerability Detection**       |
| 24.07 |                                                                                            Microsoft                                                                                             |                                        arxiv                                        |                                                       [Can LLMs be Fooled? Investigating Vulnerabilities in LLMs](https://arxiv.org/abs/2407.20529)                                                       |                     **Vulnerabilities**&**Adversarial Attacks**&**Security**                     |
| 24.08 |                                                                            Guilin University of Electronic Technology                                                                            |                                        arxiv                                        |                             [Enhancing Robustness in Large Language Models: Prompting for Mitigating the Impact of Irrelevant Information](https://arxiv.org/abs/2408.10615)                              |                            **Prompt-Tuning**&**Arithmetic Reasoning**                            |
| 24.09 |                                                                                     Northwestern University                                                                                      |                                        arxiv                                        |                                       [PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs](https://arxiv.org/abs/2409.14729)                                        |                        **Prompt Injection**&**Fuzzing**&**LLM Security**                         |
| 24.10 |                                                           Hong Kong University of Science and Technology, Beijing Jiaotong University                                                            |                                        arxiv                                        |                         [AnyAttack: Towards Large-scale Self-supervised Generation of Targeted Adversarial Examples for Vision-Language Models](https://arxiv.org/abs/2410.05346)                         |         **Adversarial Examples**&**Vision-Language Models**&**Self-supervised Learning**         |
| 24.10 |                                                                 University of Science and Technology of China, Tencent YouTu Lab                                                                 |                                 ACM Multimedia 2024                                 |                        [Break the Visual Perception: Adversarial Attacks Targeting Encoded Visual Tokens of Large Vision-Language Models](https://doi.org/10.1145/3664647.3680779)                        |             **Large Vision-Language Model**&**Adversarial Attack**&**Image Encoder**             |
| 24.11 |                                                                                 National University of Singapore                                                                                 |                                        arXiv                                        |                                                   [Reasoning Robustness of LLMs to Adversarial Typographical Errors](https://arxiv.org/abs/2411.05345)                                                    |               **Chain-of-Thought**&**Adversarial Typo**&**Robustness Evaluation**                |
| 24.12 |                                                                                       Tsinghua University                                                                                        |                                        arxiv                                        |                                     [Seeker: Towards Exception Safety Code Generation with Intermediate Language Agents Framework](https://arxiv.org/abs/2412.11713)                                      |                **Exception Handling**&**Intermediate Language (IL)**&**Deep-RAG**                |
| 25.01 |                                                              Conversational AI and Social Analytics (CAISA) Lab, University of Bonn                                                              |                                        arxiv                                        |                                         [ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving](https://arxiv.org/abs/2501.08203)                                          |                 **Robustness Testing**&**Math Problem Solving**&**Noisy Inputs**                 |
| 25.01 |                                                                                  University of Central Florida                                                                                   |                                        arxiv                                        |                                      [I Can Find You in Seconds! Leveraging Large Language Models for Code Authorship Attribution](https://arxiv.org/abs/2501.08165)                                      |        **Code Authorship Attribution**&**Zero-Shot Prompting**&**Adversarial Robustness**        |
| 25.01 |                                                                                       Macquarie University                                                                                       |                                        arxiv                                        |                                            [SpaLLM-Guard: Pairing SMS Spam Detection Using Open-source and Commercial LLMs](https://arxiv.org/abs/2501.04985)                                             |               **SMS Spam Detection**&**Adversarial Robustness**&**Concept Drift**                |
| 25.03 |                                                                                            Dynamo AI                                                                                             |                             ICBINB Workshop @ ICLR 2025                             |                                                [Know Thy Judge: On the Robustness Meta-Evaluation of LLM Safety Judges](https://arxiv.org/abs/2503.04474)                                                 |                 **LLM Safety Evaluation**&**Robustness**&**Adversarial Attacks**                 |
| 25.03 |                                                                                     Simon Fraser University                                                                                      |                                        arxiv                                        |                             [Quantifying the Robustness of Retrieval-Augmented Language Models Against Spurious Features in Grounding Data](https://arxiv.org/abs/2503.05587)                             |        **Retrieval-Augmented Generation**&**Spurious Features**&**Robustness Evaluation**        |
| 25.03 |                                                                                              Cohere                                                                                              |                                        arxiv                                        |                                                [Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts](https://arxiv.org/abs/2503.09347)                                                |                 **LLM-as-a-Judge**&**Safety Evaluation**&**Artifact Robustness**                 |
| 25.04 |                                                                                       Southeast University                                                                                       |                                        arxiv                                        |                                            [How does Watermarking Affect Visual Language Models in Document Understanding?](https://arxiv.org/abs/2504.01048)                                             |          **Visual Language Models**&**Document Understanding**&**Watermark Robustness**          |
| 25.04 |                                                                                     Imperial College London                                                                                      |                         Building Trust Workshop @ ICLR 2025                         |                                                [ENHANCING LLM ROBUSTNESS TO PERTURBED INSTRUCTIONS: AN EMPIRICAL STUDY](https://arxiv.org/abs/2504.02733)                                                 |                **LLM Robustness**&**Instruction Perturbation**&**Self-Denoising**                |
| 25.04 |                                                                           University of North Carolina at Chapel Hill                                                                            |                                        arxiv                                        |                                             [Breach in the Shield: Unveiling the Vulnerabilities of Large Language Models](https://arxiv.org/abs/2504.03714)                                              |                    **LLM Robustness**&**Stability Measure**&**Model Merging**                    |
| 25.04 |                                                                                       Tsinghua University                                                                                        |                                        arxiv                                        |                                                  [QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models](https://arxiv.org/abs/2504.11038)                                                   |         **Visual Adversarial Attack**&**LVLM Security**&**Query-Agnostic Perturbation**          |
| 25.04 |                                                                          University of Science and Technology of China                                                                           |                                      CVPR 2025                                      |                                   [R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning](https://arxiv.org/abs/2504.11195)                                   |        **Vision-Language Models**&**Adversarial Robustness**&**Test-Time Prompt Tuning**         |
| 25.04 |                                                                                       Texas A&M University                                                                                       |                                      WOOT 2025                                      |                           [Making Acoustic Side-Channel Attacks on Noisy Keyboards Viable with LLM-Assisted Spectrograms‚Äô ‚ÄúTypo‚Äù Correction](https://arxiv.org/abs/2504.11622)                            |   **Acoustic Side-Channel Attack**&**Spectrogram Typo Correction**&**LLM-Assisted Inference**    |
| 25.04 |                                                                               The Hong Kong Polytechnic University                                                                               |                                      KDD 2024                                       |                                                 [CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent](https://arxiv.org/abs/2504.13192)                                                 |               **Recommender Systems**&**Adversarial Attacks**&**LLMs-based Agent**               |
| 25.05 |                                                                                      Ben Gurion University                                                                                       |                                      ISIT 2025                                      |                                                      [Optimized Couplings for Watermarking Large Language Models](https://arxiv.org/abs/2505.08878)                                                       |              **LLM Watermarking**&**Hypothesis Testing**&**Coupling Optimization**               |
| 25.05 |                                                                                         Tufts University                                                                                         |                                        arxiv                                        |                                             [Noise Injection Systemically Degrades Large Language Model Safety Guardrails](https://arxiv.org/abs/2505.13500)                                              |                  **Safety Fine-Tuning**&**Activation Noise**&**LLM Robustness**                  |
| 25.05 |                                                                               The Hong Kong Polytechnic University                                                                               |                                      ICML 2025                                      |                                          [Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?](https://arxiv.org/abs/2505.12871v1)                                           |                    **LoRA**&**Training-Time Attacks**&**Robustness Analysis**                    |
| 25.06 |                                                                                         Anhui University                                                                                         |                                        arxiv                                        |                                  [Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks](https://arxiv.org/abs/2506.03627v1)                                   |                   **LLM Robustness**&**Prompting Attack**&**Error Correction**                   |
| 25.06 |                                                                               University of Massachusetts Amherst                                                                                |                                        arxiv                                        |                                   [Chain-of-Code Collapse: Reasoning Failures in LLMs via Adversarial Prompting in Code Generation](https://arxiv.org/abs/2506.06971v2)                                   | **Code Generation**&**Reasoning Robustness**&**Adversarial Prompting**&**Semantic Perturbation** |
| 25.06 |                                                                              Peking University, Huawei Technologies                                                                              |                                        arxiv                                        |                                           [More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents](https://arxiv.org/abs/2506.21967v1)                                            |                    **Tool-Integrated Agents**&**Stability**&**Vulnerability**                    |
| 25.07 |                                                                                        Queen‚Äôs University                                                                                        |                                        arxiv                                        |                                                [On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks](https://arxiv.org/abs/2507.06489v1)                                                |                   **Verbal Confidence**&**Adversarial Attack**&**Robustness**                    |
| 25.07 |                                                                                           UC Berkeley                                                                                            |                                        arxiv                                        |                                      [Small Edits, Big Consequences: Telling Good from Bad Robustness in Large Language Models](https://arxiv.org/abs/2507.15868v1)                                       |                              **Robustness**&**LLM**&**Sensitivity**                              |
| 25.08 |                                                                                         York University                                                                                          |                                    IEEE VIS 2025                                    |                                     [The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models](https://arxiv.org/abs/2508.09716v1)                                      |           **Misleading Visualizations**&**Vision-Language Models**&**Chart Reasoning**           |
| 25.08 |                                                     National University of Singapore, Nanyang Technological University, Tsinghua University                                                      |                                        arxiv                                        |                         [When Audio and Text Disagree: Benchmarking Text Bias in Large Audio-Language Models under Cross-Modal Inconsistencies](https://arxiv.org/abs/2508.15407)                         |                      **Audio-Language Models**&**Text Bias**&**Benchmark**                       |
| 25.09 |                                                                                      Independent researcher                                                                                      |                                        arxiv                                        | [BioBlue: Notable runaway-optimiser-like LLM failure modes on biologically and economically aligned AI safety benchmarks for LLMs with simplified observation format](https://arxiv.org/abs/2509.02655v1) |                 **AI Safety**&**Runaway Optimisation**&**Alignment Benchmarks**                  |
| 25.09 |                                                                                        Bocconi University                                                                                        |                                        arxiv                                        |                                    [Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation](https://arxiv.org/abs/2509.08825v1)                                     |                   **LLM Hacking**&**Data Annotation**&**Statistical Validity**                   |
| 25.09 | Dalian Maritime University, The Chinese University of Hong Kong, Tsinghua Shenzhen International Graduate School, Renmin University of China, Nanyang Technological University, Wuhan University |                                        arxiv                                        |                                             [Benchmarking MLLM-based Web Understanding: Reasoning, Robustness and Safety](https://arxiv.org/abs/2509.21782v1)                                             |  **Multimodal Large Language Models**&**Web Understanding**&**Robustness and Safety Benchmark**  |
| 25.10 | Carnegie Mellon University | arxiv | [RAG Makes Guardrails Unsafe? Investigating Robustness of Guardrails under RAG-style Contexts](https://arxiv.org/abs/2510.05310) | **Guardrail Robustness**&**Retrieval-Augmented Generation (RAG)**&**Safety Alignment** |
| 25.10 | Duke Kunshan University | arxiv | [SmoothGuard: Defending Multimodal Large Language Models with Noise Perturbation and Clustering Aggregation](https://arxiv.org/abs/2510.26830) | **Multimodal Large Language Model**&**Adversarial Defense**&**Randomized Smoothing** |
| 25.11 | Astana IT University | arxiv | [Prompt Injection as an Emerging Threat: Evaluating the Resilience of Large Language Models](https://arxiv.org/abs/2511.01634) | **Prompt Injection**&**Resilience Evaluation**&**Safety Compliance** |




## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

## üßë‚Äçüè´Scholars