# Security

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"

## üìëPapers

| Date  |                                                                                                                      Institute                                                                                                                       |           Publication            |                                                                                            Paper                                                                                            |                                             Keywords                                              |
|:-----:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------:|
| 20.10 |                                                                                                                 Facebook AI Research                                                                                                                 |              arxiv               |                                                       [Recipes for Safety in Open-domain Chatbots](https://arxiv.org/abs/2010.07079)                                                        |                                **Toxic Behavior**&**Open-domain**                                 |
| 22.02 |                                                                                                                       DeepMind                                                                                                                       |            EMNLP2022             |                                              [Red Teaming Language Models with Language Model](https://aclanthology.org/2022.emnlp-main.225/)                                               |                                   **Red Teaming**&**Harm Test**                                   |
| 22.03 |                                                                                                                        OpenAI                                                                                                                        |             NIPS2022             | [Training language models to follow instructions with human feedback](https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html) |                               **InstructGPT**&**RLHF**&**Harmless**                               |
| 22.04 |                                                                                                                      Anthropic                                                                                                                       |              arxiv               |                                [Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2204.05862)                                |                                     **Helpful**&**Harmless**                                      |
| 22.05 |                                                                                                                         UCSD                                                                                                                         |            EMNLP2022             |                             [An Empirical Analysis of Memorization in Fine-tuned Autoregressive Language Models](https://aclanthology.org/2022.emnlp-main.119/)                             |                                **Privacy Risks**&**Memorization**                                 |
| 22.09 |                                                                                                                      Anthropic                                                                                                                       |              arxiv               |                              [Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned](https://arxiv.org/abs/2209.07858)                               |                             **Red Teaming**&**Harmless**&**Helpful**                              |
| 22.12 |                                                                                                                      Anthropic                                                                                                                       |              arxiv               |                                                    [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)                                                     |                            **Harmless**&**Self-improvement**&**RLAIF**                            |
| 23.07 |                                                                                                                     UC Berkeley                                                                                                                      |             NIPS2023             |                                                     [Jailbroken: How Does LLM Safety Training Fail?](https://arxiv.org/abs/2307.02483)                                                      |               **Jailbreak**&**Competing Objectives**&**Mismatched Generalization**                |
| 23.08 |                                                                       The Chinese University of Hong Kong Shenzhen China, Tencent AI Lab, The Chinese University of Hong Kong                                                                        |              arxiv               |                                            [GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs Via Cipher](https://arxiv.org/abs/2308.06463)                                            |                            **Safety Alignment**&**Adversarial Attack**                            |
| 23.08 |                                                                                       University College London, University College London, Tilburg University                                                                                       |              arxiv               |                                   [Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities](https://arxiv.org/abs/2308.12833)                                   |                                   **Security**&**AI Alignment**                                   |
| 23.09 |                                                                                                                  Peking University                                                                                                                   |              arxiv               |                                           [RAIN: Your Language Models Can Align Themselves without Finetuning](https://arxiv.org/abs/2309.07124)                                            |                              **Self-boosting**&**Rewind Mechanisms**                              |
| 23.10 |                                                                                        Princeton University, Virginia Tech, IBM Research, Stanford University                                                                                        |              arxiv               |                                [FINE-TUNING ALIGNED LANGUAGE MODELS COMPROMISES SAFETY EVEN WHEN USERS DO NOT INTEND TO!](https://arxiv.org/abs/2310.03693)                                 |                     **Fine-tuning****Safety Risks**&**Adversarial Training**                      |
| 23.10 |                                                                                                                     UC Riverside                                                                                                                     |              arXiv               |                                   [Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks](https://arxiv.org/abs/2310.10844)                                    |                  **Adversarial Attacks**&**Vulnerabilities**&**Model Security**                   |
| 23.11 |                                                                                                                       KAIST AI                                                                                                                       |              arxiv               |                                           [HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning](https://arxiv.org/abs/2311.00321)                                           |                                   **Hate Speech**&**Detection**                                   |
| 23.11 |                                                                                                                         CMU                                                                                                                          | AACL2023(ART or Safety workshop) |                                                             [Measuring Adversarial Datasets](https://arxiv.org/abs/2311.03566)                                                              |                 **Adversarial Robustness**&**AI Safety**&**Adversarial Datasets**                 |
| 23.11 |                                                                                                                         UIUC                                                                                                                         |              arxiv               |                                                   [Removing RLHF Protections in GPT-4 via Fine-Tuning](https://arxiv.org/abs/2311.05553)                                                    |                               **Remove Protection**&**Fine-Tuning**                               |
| 23.11 |                                                                                                 IT University of CopenhagenÔºåUniversity of Washington                                                                                                 |              arxiv               |                                      [Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild](https://arxiv.org/abs/2311.06237)                                       |                                          **Red Teaming**                                          |
| 23.11 |                                                                                                           Fudan University&Shanghai AI lab                                                                                                           |              arxiv               |                                                      [Fake Alignment: Are LLMs Really Aligned Well?](https://arxiv.org/abs/2311.05915)                                                      |                            **Alignment Failure**&**Safety Evaluation**                            |
| 23.11 |                                                                                                          University of Southern California                                                                                                           |              arxiv               |                                         [SAFER-INSTRUCT: Aligning Language Models with Automated Preference Data](https://arxiv.org/abs/2311.08685)                                         |                                        **RLHF**&**Safety**                                        |
| 23.11 |                                                                                                                   Google Research                                                                                                                    |              arxiv               |                               [AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-powered Applications](https://arxiv.org/abs/2311.08592)                               |            **Adversarial Testing**&**AI-Assisted Red Teaming**&**Application Safety**             |
| 23.11 |                                                                                                                    Tencent AI Lab                                                                                                                    |              arxiv               |                                                           [ADVERSARIAL PREFERENCE OPTIMIZATION](https://arxiv.org/abs/2311.08045)                                                           |  **Human Preference Alignment**&**Adversarial Preference Optimization**&**Annotation Reduction**  |
| 23.11 |                                                                                                                       Docta.ai                                                                                                                       |              arxiv               |                          [Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models](https://arxiv.org/abs/2311.11202)                          |                             **Data Credibility**&**Safety alignment**                             |
| 23.11 |                                                                                                                 CIIRC CTU in Prague                                                                                                                  |              arxiv               |                                                   [A Security Risk Taxonomy for Large Language Models](https://arxiv.org/abs/2311.11415)                                                    |                     **Security risks**&**Taxonomy**&**Prompt-based attacks**                      |
| 23.12 |                                                                                                                  Drexel University                                                                                                                   |              arXiv               |                               [A Survey on Large Language Model (LLM) Security and Privacy: The Good the Bad and the Ugly](https://arxiv.org/abs/2312.02003)                                |                               **Security**&**Privacy**&**Attacks**                                |
| 23.12 |                                                                                                                        Tenyx                                                                                                                         |              arXiv               |                                  [Characterizing Large Language Model Geometry Solves Toxicity Detection and Generation](https://arxiv.org/abs/2312.01648)                                  |            **Geometric Interpretation**&**Intrinsic Dimension**&**Toxicity Detection**            |
| 23.12 |                                                                                                         Independent (Now at Google DeepMind)                                                                                                         |              arXiv               |                                           [Scaling Laws for Adversarial Attacks on Language Model Activations](https://arxiv.org/abs/2312.02780)                                            |              **Adversarial Attacks**&**Language Model Activations**&**Scaling Laws**              |
| 23.12 |                                                                                                University of Liechtenstein, University of Duesseldorf                                                                                                |              arxiv               |                                         [NEGOTIATING WITH LLMS: PROMPT HACKS, SKILL GAPS, AND REASONING DEFICITS](https://arxiv.org/abs/2312.03720)                                         |                         **Negotiation**&**Reasoning**&**Prompt Hacking**                          |
| 23.12 |                                                                            University of Wisconsin Madison, University of Michigan Ann Arbor, ASU, Washington University                                                                             |              arXiv               |                                            [Exploring the Limits of ChatGPT in Software Security Applications](https://arxiv.org/abs/2312.05275)                                            |                                         Software Security                                         |
| 23.12 |                                                                                                                    GenAI at Meta                                                                                                                     |              arxiv               |                                        [Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations](https://arxiv.org/abs/2312.06674)                                         |                            Human-AI Conversation&Safety Risk taxonomy                             |
| 23.12 |                                                                                                    University of California Riverside, Microsoft                                                                                                     |              arxiv               |                                   [Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack](https://arxiv.org/abs/2312.06924)                                   |                           Safety Alignment&Summarization&Vulnerability                            |
| 23.12 |                                                                                                                     MIT, Harvard                                                                                                                     |        NIPS2023(Workshop)        |                                          [Forbidden Facts: An Investigation of Competing Objectives in Llama-2](https://arxiv.org/abs/2312.08793)                                           |             **Competing Objectives**&**Forbidden Fact Task**&**Model Decomposition**              |
| 23.12 |                                                                                                    University of Science and Technology of China                                                                                                     |              arxiv               |                                  [Silent Guardian: Protecting Text from Malicious Exploitation by Large Language Models](https://arxiv.org/abs/2312.09669)                                  |                              **Text Protection**&**Silent Guardian**                              |
| 23.12 |                                                                                                                        OpenAI                                                                                                                        |             Open AI              |                                 [Practices for Governing Agentic AI Systems](https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf)                                  |                             **Agentic AI Systems**&**LM Based Agent**                             |
| 23.12 |                                                                      University of Massachusetts Amherst, Columbia University, Google, Stanford University, New York University                                                                      |              arxiv               |                                            [Learning and Forgetting Unsafe Examples in Large Language Models](https://arxiv.org/abs/2312.12736)                                             |                  **Safety Issues**&**ForgetFilter Algorithm**&**Unsafe Content**                  |
| 23.12 |                                                                                                 Tencent AI Lab, The Chinese University of Hong Kong                                                                                                  |              arxiv               |                                                         [Aligning Language Models with Judgments](https://arxiv.org/abs/2312.14591)                                                         |                   **Judgment Alignment**&**Contrastive Unlikelihood Training**                    |
| 24.01 |                                                                                                            Delft University of Technology                                                                                                            |              arxiv               |                              [Red Teaming for Large Language Models At Scale: Tackling Hallucinations on Mathematics Tasks](https://arxiv.org/abs/2401.00290)                               |                     **Red Teaming**&**Hallucinations**&**Mathematics Tasks**                      |
| 24.01 |                                                                                Apart Research, University of Edinburgh, Imperial College London, University of Oxford                                                                                |              arxiv               |                                                     [Large Language Models Relearn Removed Concepts](https://arxiv.org/abs/2401.01814)                                                      |                          **Neuroplasticity**&**Concept Redistribution**                           |
| 24.01 |                                Tsinghua University, Xiaomi AI Lab, Huawei, Shenzhen Heytap Technology, vivo AI Lab, Viomi Technology, Li Auto, Beijing University of Posts and Telecommunications, Soochow University                                |              arxiv               |                                  [PERSONAL LLM AGENTS: INSIGHTS AND SURVEY ABOUT THE CAPABILITY EFFICIENCY AND SECURITY](https://arxiv.org/abs/2401.05459)                                  |             **Intelligent Personal Assistant**&**LLM Agent**&**Security and Privacy**             |
| 24.01 |                                                              Zhongguancun Laboratory, Tsinghua University, Institute of Information Engineering Chinese Academy of Sciences, Ant Group                                                               |              arxiv               |                                   [Risk Taxonomy Mitigation and Assessment Benchmarks of Large Language Model Systems](https://arxiv.org/abs/2401.05778)                                    |                      **Safety**&**Risk Taxonomy**&**Mitigation Strategies**                       |
| 24.01 |                                                                                                      Ben-Gurion University of the Negev Israel                                                                                                       |              arxiv               |                                                  [GPT IN SHEEP‚ÄôS CLOTHING: THE RISK OF CUSTOMIZED GPTS](https://arxiv.org/abs/2401.09075)                                                   |                              **GPTs**&**Cybersecurity**&**ChatGPT**                               |
| 24.01 |                                                                                                            Shanghai Jiao Tong University                                                                                                             |              arxiv               |                                               [R-Judge: Benchmarking Safety Risk Awareness for LLM Agents](https://arxiv.org/abs/2401.10019)                                                |                      **LLM Agents**&**Safety Risk Awareness**&**Benchmark**                       |
| 24.01 |                                                                                                                      Ant Group                                                                                                                       |              arxiv               |                                             [A FAST PERFORMANT SECURE DISTRIBUTED TRAINING FRAMEWORK FOR LLM](https://arxiv.org/abs/2401.09796)                                             |                                 **Distributed LLM**&**Security**                                  |
| 24.01 |                                                             Shanghai Artificial Intelligence Laboratory, Dalian University of Technology, University of Science and Technology of China                                                              |              arxiv               |                  [PsySafe: A Comprehensive Framework for Psychological-based Attack Defense and Evaluation of Multi-agent System Safety](https://arxiv.org/abs/2401.11880)                  |                      **Multi-agent Systems**&**Agent Psychology**&**Safety**                      |
| 24.01 |                                                                                                          Rochester Institute of Technology                                                                                                           |              arxiv               |                                                           [Mitigating Security Threats in LLMs](https://arxiv.org/abs/2401.12273)                                                           |                    **Security Threats**&**Prompt Injection**&**Jailbreaking**                     |
| 24.01 |                                                                                     Johns Hopkins University, University of Pennsylvania, Ohio State University                                                                                      |              arxiv               |                                   [The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts](https://arxiv.org/abs/2401.13136)                                   |                       **Multilingualism**&**Safety**&**Resource Disparity**                       |
| 24.01 |                                                                                                                University of Florida                                                                                                                 |              arxiv               |                                                    [Adaptive Text Watermark for Large Language Models](https://arxiv.org/abs/2401.13927)                                                    |                         **Text Watermarking**&**Robustness**&**Security**                         |
| 24.01 |                                                                                                                The Hebrew University                                                                                                                 |              arXiv               |                                             [TRADEOFFS BETWEEN ALIGNMENT AND HELPFULNESS IN LANGUAGE MODELS](https://arxiv.org/abs/2401.16332)                                              |             **Language Model Alignment**&**AI Safety**&**Representation Engineering**             |
| 24.01 |                                                                                                              Google ResearchÔºå Anthropic                                                                                                              |              arxiv               |                                                        [Gradient-Based Language Model Red Teaming](https://arxiv.org/abs/2401.16656)                                                        |                          **Red Teaming**&**Safety**&**Prompt Learning**                           |
| 24.01 |                                                                                           National University of SingaporeÔºå Pennsylvania State University                                                                                            |              arxiv               |                                 [Provably Robust Multi-bit Watermarking for AI-generated Text via Error Correction Code](https://arxiv.org/abs/2401.16820)                                  |                     **Watermarking**&**Error Correction Code**&**AI Ethics**                      |
| 24.01 |                                                                                  Tsinghua University, University of California Los Angeles, WeChat AI Tencent Inc.                                                                                   |              arxiv               |                                         [Prompt-Driven LLM Safeguarding via Directed Representation Optimization](https://arxiv.org/abs/2401.18018)                                         |                        **Safety Prompts**&**Representation Optimization**                         |
| 24.02 |                                                                                   Rensselaer Polytechnic Institute, IBM T.J. Watson Research Center, IBM Research                                                                                    |              arxiv               |                                               [Adaptive Primal-Dual Method for Safe Reinforcement Learning](https://arxiv.org/abs/2402.00355)                                               |       **Safe Reinforcement Learning**&**Adaptive Primal-Dual**&**Adaptive Learning Rates**        |
| 24.02 |                                                      Jagiellonian University, University of Modena and Reggio Emilia, Alma Mater Studiorum University of Bologna, European University Institute                                                      |              arxiv               |                                             [No More Trade-Offs: GPT and Fully Informative Privacy Policies](https://arxiv.org/abs/2402.00013)                                              |                      **ChatGPT**&**Privacy Policies**&**Legal Requirements**                      |
| 24.02 |                                                                                                           Florida International University                                                                                                           |              arxiv               |                                           [Security and Privacy Challenges of Large Language Models: A Survey](https://arxiv.org/abs/2402.00888)                                            |                          **Security**&**Privacy Challenges**&**Suevey**                           |
| 24.02 |                                                                                    Rutgers University, University of California, Santa Barbara, NEC Labs America                                                                                     |              arxiv               |                                  [TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution](https://arxiv.org/abs/2402.01586)                                   |                        **LLM-based Agents**&**Safety**&**Trustworthiness**                        |
| 24.02 |                                                                        University of Maryland College Park, JPMorgan AI Research, University of Waterloo, Salesforce Research                                                                        |              arxiv               |                                                [Shadowcast: Stealthy Data Poisoning Attacks against VLMs](https://arxiv.org/abs/2402.06659)                                                 |                    **Vision-Language Models**&**Data Poisoning**&**Security**                     |
| 24.02 |                                                    Shanghai Artificial Intelligence Laboratory, Harbin Institute of Technology, Beijing Institute of Technology, Chinese University of Hong Kong                                                     |              arxiv               |                                [SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models](https://arxiv.org/abs/2402.05044)                                 |                **Safety Benchmark**&Safety Evaluation**&**Hierarchical Taxonomy**                 |
| 24.02 |                                                                                                                   Fudan University                                                                                                                   |              arxiv               |                            [ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages](https://arxiv.org/abs/2402.10753)                             |        **Tool Learning**&**Large Language Models (LLMs)**&**Safety Issues**&**ToolSword**         |
| 24.02 |                                                                                   Paul G. Allen School of Computer Science & Engineering, University of Washington                                                                                   |              arxiv               |                                            [SPML: A DSL for Defending Language Models Against Prompt Attacks](https://arxiv.org/abs/2402.11755)                                             | **Domain-Specific Language (DSL)**&**Chatbot Definitions**&**System Prompt Meta Language (SPML)** |
| 24.02 |                                                                                                                 Tsinghua University                                                                                                                  |              arxiv               |                                   [ShieldLM: Empowering LLMs as Aligned Customizable and Explainable Safety Detectors](https://arxiv.org/abs/2402.16444)                                    |                       **Safety Detectors**&**Customizable**&**Explainable**                       |
| 24.02 |                                                                                                                 Dalhousie University                                                                                                                 |              arxiv               |                                                    [Immunization Against Harmful Fine-tuning Attacks](https://arxiv.org/abs/2402.16382)                                                     |                             **Fine-tuning Attacks**&**Immunization**                              |
| 24.02 |                                                                                Chinese Academy of Sciences, University of Chinese Academy of Sciences, Alibaba Group                                                                                 |              arxiv               |                                             [SoFA: Shielded On-the-fly Alignment via Priority Rule Following](https://arxiv.org/abs/2402.17358)                                             |                             **Priority Rule Following**&**Alignment**                             |
| 24.02 |                                                                                                        Universidade Federal de Santa Catarina                                                                                                        |              arxiv               |                                                   [A Survey of Large Language Models in Cybersecurity](https://arxiv.org/abs/2402.16968)                                                    |                          **Cybersecurity**&**Vulnerability Assessment**                           |
| 24.02 |                                                                                                                 Zhejiang University                                                                                                                  |              arxiv               |                                           [PRSA: Prompt Reverse Stealing Attacks against Large Language Models](https://arxiv.org/abs/2402.19200)                                           |                         **Prompt Reverse Stealing Attacks**&**Security**                          |
| 24.03 |                                                                                                                  Tulane University                                                                                                                   |              arxiv               |                                           [ENHANCING LLM SAFETY VIA CONSTRAINED DIRECT PREFERENCE OPTIMIZATION](https://arxiv.org/abs/2403.02475)                                           |               **Reinforcement Learning**&**Human Feedback**&**Safety Constraints**                |
| 24.03 |                                                                                                       University of Illinois Urbana-Champaign                                                                                                        |              arxiv               |                           [INJECAGENT: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents](https://arxiv.org/abs/2403.02691)                            |                  **Tool Integration**&**Security**&**Indirect Prompt Injection**                  |
| 24.03 |                                                                                                                  Harvard University                                                                                                                  |              arxiv               |                                               [Towards Safe and Aligned Large Language Models for Medicine](https://arxiv.org/abs/2403.03744)                                               |                     ***Medical Safety**&**Alignment**&**Ethical Principles**                      |
| 24.03 |                                                                            Rensselaer Polytechnic Institute, University of Michigan, IBM Research, MIT-IBM Watson AI Lab                                                                             |              arxiv               |                                                         [ALIGNERS: DECOUPLING LLMS AND ALIGNMENT](https://arxiv.org/abs/2403.04224)                                                         |                                 **Alignment**&**Synthetic Data**                                  |
| 24.03 | MIT, Princeton University, Stanford University, Georgetown University, AI Risk and Vulnerability Alliance, Eleuther AI, Brown University, Carnegie Mellon University, Virginia Tech, Northeastern University, UCSB, University of Pennsylvania, UIUC |              arxiv               |                                                     [A Safe Harbor for AI Evaluation and Red Teaming](https://arxiv.org/abs/2403.04893)                                                     |                         **AI Evaluation**&**Red Teaming**&**Safe Harbor**                         |
| 24.03 |                                                                                                          University of Southern California                                                                                                           |              arxiv               |                                                [Logits of API-Protected LLMs Leak Proprietary Information](https://arxiv.org/abs/2403.09539)                                                |            **API-Protected LLMs**&**Softmax Bottleneck**&**Embedding Size Detection**             |
| 24.03 |                                                                                                                University of Bristol                                                                                                                 |              arxiv               |                           [Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention](https://arxiv.org/abs/2403.09795)                            |                                 **Safety**&**Prompt Engineering**                                 |
| 24.03 |                                                                 XiaMen University, Yanshan University, IDEA Research, Inner Mongolia University, Microsoft, Microsoft Research Asia                                                                  |              arxiv               |                                [Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models](https://arxiv.org/abs/2403.11838)                                 |                              **Safety**&**Guidelines**&**Alignment**                              |
| 24.03 |                                                               Tianjin University, Tianjin University, Zhengzhou University, China Academy of Information and Communications Technology                                                               |              arxiv               |                                      [OpenEval: Benchmarking Chinese LLMs across Capability, Alignment, and Safety](https://arxiv.org/abs/2403.12316)                                       |                           **Chinese LLMs**&**Benchmarking**&**Safety**                            |
| 24.03 |                                                                      Center for Cybersecurity Systems and Networks, AIShield Bosch Global Software Technologies Bengaluru India                                                                      |              arxiv               |                                  [Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal](https://arxiv.org/abs/2403.13309)                                  |                     **LLM Security**&**Threat modeling**&**Risk Assessment**                      |
| 24.03 |                                                                                                              Queen‚Äôs University Belfast                                                                                                              |              arxiv               |                                             [AI Safety: Necessary but insufficient and possibly problematic](https://arxiv.org/abs/2403.17419)                                              |                        **AI Safety**&**Transparency**&**Structural Harm**                         |
| 24.04 |                                                                      Provable Responsible AI and Data Analytics (PRADA) Lab, King Abdullah University of Science and Technology                                                                      |              arxiv               |                                     [Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs](https://arxiv.org/abs/2404.00486)                                     |                  **Dialectical Alignment**&**3H Principle**&**Security Threats**                  |
| 24.04 |                                                                 LibrAI, Tsinghua University, Harbin Institute of Technology, Monash University, The University of Melbourne, MBZUAI                                                                  |              arxiv               |                                        [Against The Achilles‚Äô Heel: A Survey on Red Teaming for Generative Models](https://arxiv.org/abs/2404.00629)                                        |                                    **Red Teaming**&**Safety**                                     |
| 24.04 |                                                                                                   University of California, Santa Barbara, Meta AI                                                                                                   |              arxiv               |                                [Towards Safety and Helpfulness Balanced Responses via Controllable Large Language Models](https://arxiv.org/abs/2404.01295)                                 |                          **Safety**&**Helpfulness**&**Controllability**                           |
| 24.04 |                                                                       School of Information and Software Engineering, University of Electronic Science and Technology of China                                                                       |              arxiv               |                                                    [Exploring Backdoor Vulnerabilities of Chat Models](https://arxiv.org/abs/2404.02406)                                                    |                         **Backdoor Attacks**&**Chat Models**&**Security**                         |
| 24.04 |                                                                                                                      Enkrypt AI                                                                                                                      |              arxiv               |                                             [INCREASED LLM VULNERABILITIES FROM FINE-TUNING AND QUANTIZATION](https://arxiv.org/abs/2404.04392)                                             |                     **Fine-tuning**&**Quantization**&**LLM Vulnerabilities**                      |
| 24.04 |                                                          TongJi University, Tsinghua University&, eijing University of Technology, Nanyang Technological University, Peng Cheng Laboratory                                                           |              arxiv               |                     [Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in Multimodal Large Language Model Security](https://arxiv.org/abs/2404.05264)                      |        **Multimodal Large Language Models**&**Security Vulnerabilities**&**Image Inputs**         |
| 24.04 |                                                                    University of Washington, Carnegie Mellon University, University of British Columbia, Vector Institute for AI                                                                     |              arxiv               |                      [CulturalTeaming: AI-Assisted Interactive Red-Teaming for Challenging LLMs‚Äô (Lack of) Multicultural Knowledge](https://arxiv.org/abs/2404.06664)                       |                      **AI-Assisted Red-Teaming**&**Multicultural Knowledge**                      |
| 24.04 |                                                                                                                  Nanjing University                                                                                                                  |            DLSP 2024             |                                  [Subtoxic Questions: Dive Into Attitude Change of LLM‚Äôs Response in Jailbreak Attempts](https://arxiv.org/abs/2404.08309)                                  |                        **Jailbreak**&**Subtoxic Questions**&**GAC Model**                         |



## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

| Date  |  Type  |                                Title                                 |                           URL                            |
|:-----:|:------:|:--------------------------------------------------------------------:|:--------------------------------------------------------:|
| 23.01 | video  | ChatGPT and InstructGPT: Aligning Language Models to Human Intention | [link](https://www.youtube.com/watch?v=RkFS6-GwCxE&t=6s) |
| 23.06 | Report |         ‚ÄúDual-use dilemma‚Äù for GenAI Workshop Summarization          |         [link](https://arxiv.org/abs/2308.14840)         |
| 23.10 |  News  |              Joint Statement on AI Safety and Openness               |         [link](https://open.mozilla.org/letter/)         |

## üßë‚Äçüè´Scholars
