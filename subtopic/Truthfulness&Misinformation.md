# Truthfulness&Misinformation

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"

## üìëPapers

| Date  |                                                                                         Institute                                                                                         |     Publication     |                                                                                            Paper                                                                                            |                                                     Keywords                                                     |
|:-----:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------:|
| 21.09 |                                                                                   University of Oxford                                                                                    |       ACL2022       |                                                 [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)                                                 |                                          **Benchmark**&**Truthfulness**                                          |
| 23.07 |                   Microsoft Research Asia, Hong Kong University of Science and Technology, University of Science and Technology of China, Tsinghua University, Sony AI                    |   ResearchSquare    |                                    [Defending ChatGPT against Jailbreak Attack via Self-Reminder](https://www.researchsquare.com/article/rs-2873090/v1)                                     |                              **Jailbreak Attack**&**Self-Reminder**&**AI Security**                              |
| 23.10 |                                                                                   University of Zurich                                                                                    |        arxiv        |                                          [Lost in Translation -- Multilingual Misinformation and its Evolution](https://arxiv.org/abs/2310.18089)                                           |                                       **Misinformation**&**Multilingual**                                        |
| 23.10 |                                                                             New York University&Javier Rando                                                                              |        arxiv        |                                               [Personas as a Way to Model Truthfulness in Language Models](https://arxiv.org/abs/2310.18168)                                                |                                      **Truthfulness**&**Truthful Persona**                                       |
| 23.11 |                                                                                    Dialpad Canada Inc                                                                                     |        arxiv        |                          [Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs](https://arxiv.org/abs/2311.00681)                           |                                            **Factuality Assessment**                                             |
| 23.11 |                                                                               The University of Manchester                                                                                |        arxiv        |                                                     [Emotion Detection for Misinformation: A Review](https://arxiv.org/abs/2311.00671)                                                      |                                    **Survey**&**Misinformation**&**Emotions**                                    |
| 23.11 |                                                                                  University of Virginia                                                                                   |        arxiv        |                             [Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder with Semantics](https://arxiv.org/abs/2311.01386)                             |                                              **Language Illusions**                                              |
| 23.11 |                                                                          University of Illinois Urbana-Champaign                                                                          |        arxiv        |          [Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism](https://arxiv.org/abs/2311.01041)          |                                     **Hallucinations**&**Refusal Mechanism**                                     |
| 23.11 |                                                                             University of Washington Bothell                                                                              |        arxiv        |                                         [Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI](https://arxiv.org/abs/2311.01463)                                         |                              **Healthcare**&**Trustworthiness**&**Hallucinations**                               |
| 23.11 |                                                                                    Intuit AI Research                                                                                     |      EMNLP2023      |                     [SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency](https://arxiv.org/abs/2311.01740)                      |                                 **Hallucination Detection**&**Trustworthiness**                                  |
| 23.11 |                                                                               Shanghai Jiao Tong University                                                                               |        arxiv        |                          [Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation](https://arxiv.org/abs/2311.01766)                           |                             **Misinformation**&**Disinformation**&**Out-of-Context**                             |
| 23.11 |                                                                               Hamad Bin Khalifa University                                                                                |        arxiv        |                                 [ArAIEval Shared Task: Persuasion Techniques and Disinformation Detection in Arabic Text](https://arxiv.org/abs/2311.03179)                                 |                                        **Disinformation**&**Arabic Text**                                        |
| 23.11 |                                                                                      UNC-Chapel Hill                                                                                      |        arxiv        |                                  [Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges](https://arxiv.org/abs/2311.03287)                                  |                                  **Hallucination**&**Benchmark**&**Multimodal**                                  |
| 23.11 |                                                                                    Cornell University                                                                                     |        arxiv        |                                            [Adapting Fake News Detection to the Era of Large Language Models](https://arxiv.org/abs/2311.04917)                                             |                          **Fake news detection**&**Generated News**&**Misinformation**                           |
| 23.11 |                                                                              Harbin Institute of Technology                                                                               |        arxiv        |                        [A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/abs/2311.05232)                         |                          **Hallucination**&**Factual Consistency**&**Trustworthiness**                           |
| 23.11 |                                                                         Korea University, KAIST AI,LG AI Research                                                                         |        arXiv        |                                   [VOLCANO: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision](https://arxiv.org/abs/2311.07362)                                    |                            **Multimodal Models**&**Hallucination**&**Self-Feedback**                             |
| 23.11 |                                                                Beijing Jiaotong University, Alibaba Group, Peng Cheng Lab                                                                 |        arXiv        |                                    [AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation](https://arxiv.org/abs/2311.07397)                                    |                            Multi-modal Large Language Models&Hallucination&Benchmark                             |
| 23.11 |                                                              LMU Munich; Munich Center of Machine Learning; Google Research                                                               |        arXiv        |                                                 [Hallucination Augmented Recitations for Language Models](https://arxiv.org/abs/2311.07424)                                                 |                                  **Hallucination**&**Counterfactual Datasets**                                   |
| 23.11 |                                                                           Stanford University, UNC Chapel Hill                                                                            |        arxiv        |                                                       [Fine-tuning Language Models for Factuality](https://arxiv.org/abs/2311.08401)                                                        |                **Factuality**&**Reference-Free Truthfulness**&**Direct Preference Optimization**                 |
| 23.11 |                                                                        Corporate Data and Analytics Office (CDAO)                                                                         |        arxiv        |                                     [Hallucination-minimized Data-to-answer Framework for Financial Decision-makers](https://arxiv.org/abs/2311.07592)                                      |                           **Financial Decision Making**&**Hallucination Minimization**                           |
| 23.11 |                                                                                 Arizona State University                                                                                  |        arxiv        |                                             [Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey](https://arxiv.org/abs/2311.07914)                                              |                                **Knowledge Graphs**&**Hallucinations**&**Survey**                                |
| 23.11 |                                                       Kempelen Institute of Intelligent Technologies; Brno University of Technology                                                       |        arxiv        |                                                  [Disinformation Capabilities of Large Language Models](https://arxiv.org/abs/2311.08838)                                                   |                    **Disinformation Generation**&**Safety Filters**&**Automated Evaluation**                     |
| 23.11 |                                                                         UNC-Chapel Hill, University of Washington                                                                         |        arxiv        |                        [EVER: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification](https://arxiv.org/abs/2311.09114)                         |                          **Hallucination**&**Real-Time Verification**&**Rectification**                          |
| 23.11 |                                                                        Peking University, WeChat AI, Tencent Inc.                                                                         |        arXiv        |                                    [RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge](https://arxiv.org/abs/2311.08147)                                    |                      **External Counterfactual Knowledge**&**Benchmarking**&**Robustness**                       |
| 23.11 |                                                                                      PolyAI Limited                                                                                       |        arXiv        |                     [Dial BEINFO for Faithfulness: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning](https://arxiv.org/abs/2311.09800)                      |                           **Factuality**&**Behavioural Fine-Tuning**&**Hallucination**                           |
| 23.11 |                                                The Hong Kong University of Science and Technology, University of Illinois Urbana-Champaign                                                |        arxiv        |                                          [R-Tuning: Teaching Large Language Models to Refuse Unknown Questions](https://arxiv.org/abs/2311.09677)                                           |                     **Hallucination**&**Refusal-Aware Instruction Tuning**&**Knowledge Gap**                     |
| 23.11 |                                               University of Southern California, University of Pennsylvania, University of California Davis                                               |        arxiv        |                             [Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?](https://arxiv.org/abs/2311.09702)                              |                            **Hallucinations**&**Semantic Associations**&**Benchmark**                            |
| 23.11 |                                                                 The Ohio State University, University of California Davis                                                                 |        arxiv        |                     [How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities](https://arxiv.org/abs/2311.09447)                      |                     **Trustworthiness**&**Malicious Demonstrations**&**Adversarial Attacks**                     |
| 23.11 |                                                                                  University of Sheffield                                                                                  |        arXiv        |                  [Lighter yet More Faithful: Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization](https://arxiv.org/abs/2311.09335)                  |                                **Hallucinations**&&**Language Model Reliability**                                |
| 23.11 |                                        Institute of Information Engineering Chinese Academy of Sciences, University of Chinese Academy of Sciences                                        |        arxiv        |                      [Can Large Language Models Understand Content and Propagation for Misinformation Detection: An Empirical Study](https://arxiv.org/abs/2311.12699)                      |                                           **Misinformation Detection**                                           |
| 23.11 |                                        Shanghai Jiaotong University, Amazon AWS AI, Westlake University, IGSNRR Chinese Academy of Sciences, China                                        |        arXiv        |                                         [Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus](https://arxiv.org/abs/2311.13230)                                         |                **Hallucination Detection**&**Uncertainty-Based Methods**&**Factuality Checking**                 |
| 23.11 |                                               Institute of Software Chinese Academy of Sciences, University of Chinese Academy of Sciences                                                |        arXiv        |                            [Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting](https://arxiv.org/abs/2311.13314)                             |                             **Hallucinations**&**Knowledge Graphs**&**Retrofitting**                             |
| 23.11 |                                                                                Applied Research Quantiphi                                                                                 |        arxiv        |                                       [Minimizing Factual Inconsistency and Hallucination in Large Language Models](https://arxiv.org/abs/2311.13878)                                       |                                   **Factual Inconsistency**&**Hallucination**                                    |
| 23.11 |                                                                             Microsoft Research, Georgia Tech                                                                              |        arxiv        |                                                       [Calibrated Language Models Must Hallucinate](https://arxiv.org/abs/2311.14648)                                                       |                              **Hallucination&Calibration**&**Statistical Analysis**                              |
| 23.11 |                                                                     School of Information Renmin University of China                                                                      |        arxiv        |                          [UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation](https://arxiv.org/abs/2311.15296)                          |                                    **Hallucination**&**Evaluation Benchmark**                                    |
| 23.11 |                                                          DAMO Academy Alibaba Group, Nanyang Technological University, Hupan Lab                                                          |        arxiv        |                          [Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding](https://arxiv.org/abs/2311.16922)                           |                               **Vision-Language Models**&**Object Hallucinations**                               |
| 23.11 |                                                                                  Shanghai AI Laboratory                                                                                   |        arxiv        |                            [Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization](https://arxiv.org/abs/2311.16839)                            |           **Multimodal Language Models**&**Hallucination Problem**&**Direct Preference Optimization**            |
| 23.12 |                                  Singapore Management University, Beijing Forestry University, University of Electronic Science and Technology of China                                   |      MMM 2024       |                         [Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites](https://arxiv.org/abs/2312.01701)                         |                     **Vision-language Models**&**Hallucination**&**Fine-grained Evaluation**                     |
| 23.12 |                                                                                  Mila, McGill University                                                                                  | EMNLP2023(findings) |                           [Evaluating Dependencies in Fact Editing for Language Models: Specificity and Implication Awareness](https://arxiv.org/abs/2312.01858)                            |                             **Knowledge Bases**&**Dataset**&**Evaluation Protocol**                              |
| 23.12 |                                                                                         MIT CSAIL                                                                                         |        arxiv        |                       [Cognitive Dissonance: Why Do Language Model Outputs Disagree with Internal Representations of Truthfulness?](https://arxiv.org/abs/2312.03729)                       |                                  **Truthfulness**&**Internal Representations**                                   |
| 23.12 |                              University of Illinois Chicago, Bosch Research North America & Bosch Center for Artificial Intelligence (BCAI), UNC Chapel-Hill                              |        arxiv        |                                       [DELUCIONQA: Detecting Hallucinations in Domain-specific Question Answering](https://arxiv.org/abs/2312.05200)                                        |                 **Hallucination Detection**&**Domain-specific QA**&**Retrieval-augmented LLMs**                  |
| 23.12 |                                                                      The University of Hong Kong, Beihang University                                                                      |      AAAI2024       |                                         [Improving Factual Error Correction by Learning to Inject Factual Errors](https://arxiv.org/abs/2312.07049)                                         |                                           **Factual Error Correction**                                           |
| 23.12 |                                                                                  Allen Institute for AI                                                                                   |        arxiv        |                               [BARDA: A Belief and Reasoning Dataset that Separates Factual Accuracy and Reasoning Ability](https://arxiv.org/abs/2312.07527)                               |                              **Dataset**&**Factual Accuracy**&**Reasoning Ability**                              |
| 23.12 |                                         Tsinghua University, Shanghai Jiao Tong University, Stanford University, Nanyang Technological University                                         |        arxiv        |                       [The Earth is Flat because...: Investigating LLMs‚Äô Belief towards Misinformation via Persuasive Conversation](https://arxiv.org/abs/2312.09085)                       |                       **Misinformation**&**Persuasive Conversation**&**Factual Questions**                       |
| 23.12 |                                                                              University of California Davis                                                                               |        arXiv        |                                         [A Revisit of Fake News Dataset with Augmented Fact-checking by ChatGPT](https://arxiv.org/abs/2312.11870)                                          |                                         **Fake News**&**Fact-checking**                                          |
| 23.12 |                                                                                    Amazon Web Services                                                                                    |        arxiv        |                                           [On Early Detection of Hallucinations in Factual Question Answering](https://arxiv.org/abs/2312.14183)                                            |                                **Hallucinations**&**Factual Question Answering**                                 |
| 23.12 |                                                                            University of California Santa Cruz                                                                            |        arxiv        | [Don‚Äôt Believe Everything You Read: Enhancing Summarization Interpretability through Automatic Identification of Hallucinations in Large Language Models](https://arxiv.org/abs/2312.14346) |                               **Hallucinations**&**Faithfulness**&**Token-level**                                |
| 23.12 |                                                                 Department of Radiology, The University of Tokyo Hospital                                                                 |        arxiv        |                                                     [Theory of Hallucinations based on Equivariance](https://arxiv.org/abs/2312.14504)                                                      |                                       **Hallucinations**&**Equivariance**                                        |
| 23.12 |                                                                              Georgia Institute of Technology                                                                              |        arXiv        |                                               [REDUCING LLM HALLUCINATIONS USING EPISTEMIC NEURAL NETWORKS](https://arxiv.org/abs/2312.15576)                                               |                           **Hallucinations**&**Uncertainty Estimation**&**TruthfulQA**                           |
| 23.12 |                                    Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Tencent AI Lab                                    |        arXiv        |                                   [Alleviating Hallucinations of Large Language Models through Induced Hallucinations](https://arxiv.org/abs/2312.15710)                                    |                       **Hallucinations**&**Induce-then-Contrast Decoding**&**Factuality**                        |
| 23.12 |                        SKLOIS Institute of Information Engineering Chinese Academy of Sciences, School of Cyber Security University of Chinese Academy of Sciences                        |        arXiv        |                                   [LLM Factoscope: Uncovering LLMs‚Äô Factual Discernment through Inner States Analysis](https://arxiv.org/abs/2312.16374)                                    |                                      **Factual Detection**&**Inner States**                                      |
| 24.01 |                                                                    The Chinese University of Hong Kong, Tencent AI Lab                                                                    |        arxiv        |                                          [The Earth is Flat? Unveiling Factual Errors in Large Language Models](https://arxiv.org/abs/2401.00761)                                           |                           **Factual Errors**&**Knowledge Graph**&**Answer Assessment**                           |
| 24.01 |                                                                    NewsBreak, University of Illinois Urbana-Champaign                                                                     |        arxiv        |                             [RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models](https://arxiv.org/abs/2401.00396)                             |                    **Retrieval-Augmented Generation**&**Hallucination Detection**&**Dataset**                    |
| 24.01 |                                                    University of California Berkeley, Universit√© de Montr√©al, McGill UniversityÔºå Mila                                                     |        arxiv        |                                                   [Uncertainty Resolution in Misinformation Detection](https://arxiv.org/abs/2401.01197)                                                    |                                  **Misinformation**&**Uncertainty Resolution**                                   |
| 24.01 |                                                                           Yale University, Stanford University                                                                            |        arxiv        |                                      [Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models](https://arxiv.org/abs/2401.01301)                                      |                                             **Legal Hallucinations**                                             |
| 24.01 |                                        Islamic University of Technology, AI Institute University of South Carolina, Stanford University, Amazon AI                                        |        arxiv        |                                 [A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models](https://arxiv.org/abs/2401.01313)                                  |                           √ü                               **Hallucination Mitigation**                           |
| 24.01 |                                                   Renmin University of China, Renmin University of China, DIRO, Universit√© de Montr√©al                                                    |        arxiv        |                            [The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models](https://arxiv.org/abs/2401.03205)                             |                        **Hallucination**&**Detection and Mitigation**&**Empirical Study**                        |
| 24.01 |                                       IIT Hyderabad India, Parmonic USA, University of Glasgow UK, LDRP Institute of Technology and Research India                                        |        arxiv        |                              [Fighting Fire with Fire: Adversarial Prompting to Generate a Misinformation Detection Dataset](https://arxiv.org/abs/2401.04481)                              |                          **Misinformation Detection**&**LLM-generated Synthetic Data**                           |
| 24.01 |                                                                                 University College London                                                                                 |        arxiv        |                                              [Hallucination Benchmark in Medical Visual Question Answering](https://arxiv.org/abs/2401.05827)                                               |                        **Medical Visual Question Answering**&**Hallucination Benchmark**                         |
| 24.01 |                                                                                    Soochow University                                                                                     |        arxiv        |                                                        [LightHouse: A Survey of AGI Hallucination](https://arxiv.org/abs/2401.06792)                                                        |                                              **AGI Hallucination**                                               |
| 24.01 |                                                       University of Washington, Carnegie Mellon University, Allen Institute for AI                                                        |        arxiv        |                                          [Fine-grained Hallucination Detection and Editing for Language Models](https://arxiv.org/abs/2401.06855)                                           |                                       **Hallucination Detection**&**FAVA**                                       |
| 24.01 |                                                             Dartmouth College, Universit√© de Montr√©al, McGill UniversityÔºåMila                                                             |        arxiv        |                                      [Comparing GPT-4 and Open-Source Language Models in Misinformation Mitigation](https://arxiv.org/abs/2401.06920)                                       |                                      **GPT-4**&**Misinformation Detection**                                      |
| 24.01 |                                                                                    Utrecht University                                                                                     |        arxiv        |                                                         [The Pitfalls of Defining Hallucination](https://arxiv.org/abs/2401.07897)                                                          |                                                **Hallucination**                                                 |
| 24.01 |                                                                                     Samsung AI Center                                                                                     |        arxiv        |                                         [Hallucination Detection and Hallucination Mitigation: An Investigation](https://arxiv.org/abs/2401.08358)                                          |                             **Hallucination Detection**&**Hallucination Mitigation**                             |
| 24.01 |                                                                      McGill UniversityÔºå Mila, Universit√© de Montr√©al                                                                      |        arxiv        |                  [Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation](https://arxiv.org/abs/2401.08694)                  |            **Misinformation Mitigation**&**Uncertainty Quantification**&**Sample-based Consistency**             |
| 24.01 |                                                                                      LY Corporation                                                                                       |        arxiv        |                                            [On the Audio Hallucinations in Large Audio-Video Language Models](https://arxiv.org/abs/2401.09774)                                             |                **Audio Hallucinations**&**Audio-visual Learning**&**Audio-video language Models**                |
| 24.01 |                                                                           Sun Yat-sen University Tencent AI Lab                                                                           |        arXiv        |                                  [Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment](https://arxiv.org/abs/2401.10768)                                  |                         **Hallucination Mitigation**&**Knowledge Consistent Alignment**                          |
| 24.01 |                                                                             National University of Singapore                                                                              |        arxiv        |                                       [Hallucination is Inevitable: An Innate Limitation of Large Language Models](https://arxiv.org/abs/2401.11817)                                        |                                      **Hallucination**&**Real World LLMs**                                       |
| 24.01 |                                                                       X2Robot&International Digital Economy Academy                                                                       |        arXiv        |                             [Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for Hallucination Mitigation](https://arxiv.org/abs/2401.15449)                             |                  **Hallucination Mitigation**&**Knowledge Probing**&**Reinforcement Learning**                   |
| 24.01 |                                                                  University of Texas at Austin, Northeastern University                                                                   |        arxiv        |                       [Diverse but Divisive: LLMs Can Exaggerate Gender Differences in Opinion Related to Harms of Misinformation](https://arxiv.org/abs/2401.16558)                        |                             **Misinformation Detection**&**Socio-Technical Systems**                             |
| 24.01 |                                                        National University of Defense Technology, National University of Singapore                                                        |        arxiv        |                              [SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering](https://arxiv.org/abs/2401.17809)                              |                                **Factual Knowledge Editing**&**Word Embeddings**                                 |
| 24.02 |                        University of Washington, University of California Berkeley, The Hong Kong University of Science and Technology, Carnegie Mellon University                        |        arxiv        |                                  [Don‚Äôt Hallucinate Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration](https://arxiv.org/abs/2402.00367)                                  |                                  **Knowledge Gaps**&**Multi-LLM Collaboration**                                  |
| 24.02 |                                                                  IT Innovation and Research Center, Huawei Technologies                                                                   |        arxiv        |                                                [A Survey on Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2402.00253)                                                |                   **Large Vision-Language Models**&**Hallucination**&**Mitigation Strategies**                   |
| 24.02 |                                                               Tianjin University, National University of Singapore, A*STAR                                                                |        arxiv        |                                    [SKIP \N: A SIMPLE METHOD TO REDUCE HALLUCINATION IN LARGE VISION-LANGUAGE MODELS](https://arxiv.org/abs/2402.01345)                                     |                 **Semantic Shift Bias**&**Hallucination Mitigation**&**Vision-Language Models**                  |
| 24.02 |                                                                       University of Marburg, University of Mannheim                                                                       | EACL Findings 2024  |                                  [The Queen of England is not England‚Äôs Queen: On the Lack of Factual Coherency in PLMs](https://arxiv.org/abs/2402.01453)                                  |                                    **Factual Coherency**&**Knowledge Bases**                                     |
| 24.02 |                                                                    MBZUAI, Monash University, LibrAI, Sofia University                                                                    |        arxiv        |                                                  [Factuality of Large Language Models in the Year 2024](https://arxiv.org/abs/2402.02420)                                                   |                                **Factuality**&**Evaluation**&**Multimodal LLMs**                                 |
| 24.02 |                                       Institute of Information Engineering, Chinese Academy of Sciences, University of Chinese Academy of Sciences                                        |        arxiv        |                                                  [Are Large Language Models Table-based Fact-Checkers?](https://arxiv.org/abs/2402.02549)                                                   |                            **Table-based Fact Verification**&**In-context Learning**                             |
| 24.02 |                                                                              Zhejiang University, Ant Group                                                                               |        arxiv        |                                          [Unified Hallucination Detection for Multimodal Large Language Models](https://arxiv.org/abs/2402.03190)                                           |                  **Multimodal Large Language Models**&**Hallucination Detection**&**Benchmark**                  |
| 24.02 |                                                                            Alibaba Cloud, Zhejiang University                                                                             |      ICLR2024       |                                        [INSIDE: LLMS‚Äô INTERNAL STATES RETAIN THE POWER OF HALLUCINATION DETECTION](https://arxiv.org/abs/2402.03744)                                        |                                    **Hallucination Detection**&**EigenScore**                                    |
| 24.02 |                           The Hong Kong University of Science and Technology, University of Illinois at Urbana-Champaign, The Hong Kong Polytechnic University                            |        arxiv        |                                          [The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs](https://arxiv.org/abs/2402.03757)                                           |                              **Multimodal Large Language Models**&**Hallucination**                              |
| 24.02 |                                              Institute of Automation Chinese Academy of Sciences, University of Chinese Academy of Sciences                                               |        arxiv        |                                                [Can Large Language Models Detect Rumors on Social Media?](https://arxiv.org/abs/2402.03916)                                                 |                                       **Rumor Detection**&**Social Media**                                       |
| 24.02 |         CAS Key Laboratory of AI Safety, School of Computer Science and Technology University of Chinese Academy of Science, International Digital Economy Academy IDEA Research          |        arxiv        |                                       [A Survey on Large Language Model Hallucination via a Creativity Perspective](https://arxiv.org/abs/2402.06647)                                       |                                         **Creativity**&**Hallucination**                                         |
| 24.02 |                                                             University College London, Speechmatics, MATS, Anthropic, FAR AI                                                              |        arxiv        |                                            [Debating with More Persuasive LLMs Leads to More Truthful Answers](https://arxiv.org/abs/2402.06782)                                            |                                           **Debate**&**Truthfulness**                                            |
| 24.02 |                                               University of Illinois Urbana-Champaign, DAMO Academy Alibaba Group, Northwestern University                                                |        arxiv        |                                            [Towards Faithful Explainable Fact-Checking via Multi-Agent Debate](https://arxiv.org/abs/2402.07401)                                            |                                       **Fact-checking**&**Explainability**                                       |
| 24.02 |                                  Rice Universitym, Texas A&M University, Wake Forest University, New Jersey Institute of Technology, Meta Platforms Inc.                                  |        arxiv        |                                                      [Large Language Models As Faithful Explainers](https://arxiv.org/abs/2402.04678)                                                       |                                 **Explainability**&**Fidelity**&**Optimization**                                 |
| 24.02 |                                                                    The Hong Kong University of Science and Technology                                                                     |        arxiv        |                                   [Do LLMs Know about Hallucination? An Empirical Investigation of LLM‚Äôs Hidden States](https://arxiv.org/abs/2402.09733)                                   |                           **Hallucination**&**Hidden States**&**Model Interpretation**                           |
| 24.02 |                                                                UC Santa Cruz, ByteDance Research, Northwestern University                                                                 |        arxiv        |                             [MEASURING AND REDUCING LLM HALLUCINATION WITHOUT GOLD-STANDARD ANSWERS VIA EXPERTISE-WEIGHTING](https://arxiv.org/abs/2402.10412)                              |             **Large Language Models (LLMs)**&**Hallucination**&**Factualness Evaluations**&**FEWL**              |
| 24.02 |                                                     Paul G. Allen School of Computer Science & Engineering, University of Washington                                                      |        arxiv        |                                          [Comparing Hallucination Detection Metrics for Multilingual Generation](https://arxiv.org/abs/2402.10496)                                          | **Hallucination Detection**&**Multilingual Generation**&**Lexical Metrics**&**Natural Language Inference (NLI)** |
| 24.02 |                                         Institute of Computing Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences                                         |        arxiv        |                   [Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models](https://arxiv.org/abs/2402.10612)                    |        **Large Language Models (LLMs)**&**Hallucination Mitigation**&**Retrieval Augmentation**&**Rowen**        |
| 24.02 |                                    Institute of Automation, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Nanjing University                                    |        arxiv        |                                  [Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2402.11622)                                  |                           **Object Hallucination**&**Vision-Language Models (LVLMs)**                            |
| 24.02 |                                 Institute of Mathematics and Statistics University of S√£o Paulo, Artificial Intelligence Specialist in the Banking Sector                                 |        arxiv        |                [Hallucinations or Attention Misdirection? The Path to Strategic Value Extraction in Business Using Large Language Models](https://arxiv.org/abs/2402.14002)                 |                            **Hallucinations**&**Generative Artificial Intelligence**                             |
| 24.02 |                                                                       Stevens Institute of Technology, Peraton Labs                                                                       |        arxiv        |                                      [Can Large Language Models Detect Misinformation in Scientific News Reporting?](https://arxiv.org/abs/2402.14268)                                      |                          **Scientific Reporting**&**Misinformation**&**Explainability**                          |
| 24.02 |                                                                             Middle East Technical University                                                                              |        arxiv        |                                 [HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs](https://arxiv.org/abs/2402.16211)                                  |                                    **Hallucination**&**Benchmarking Dataset**                                    |
| 24.02 |                                                                             National University of Singapore                                                                              |        arxiv        |                         [Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding](https://arxiv.org/abs/2402.15300)                          |                      **Vision-Language Models**&**Hallucination**&**CLIP-Guided Decoding**                       |
| 24.02 |                                                                   University of California Los Angeles, Cisco Research                                                                    |        arxiv        |                             [Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension](https://arxiv.org/abs/2402.18048)                              |                                  **Truthfulness**&**Local Intrinsic Dimension**                                  |
| 24.02 |                 Institute of Automation Chinese Academy of Sciences, School of Artificial Intelligence University of Chinese Academy of Sciences, Hunan Normal University                 |        arxiv        |                     [Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models](https://arxiv.org/abs/2402.19103)                     |                             **False Premise Hallucinations**&**Attention Mechanism**                             |
| 24.02 |        Shanghai Artificial Intelligence Laboratory, Renmin University of China, University of Chinese Academy of Sciences, Shanghai Jiao Tong University, The University of Sydney        |        arxiv        |                            [Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models](https://arxiv.org/abs/2402.19465)                            |                                  **Trustworthiness Dynamics**&**Pre-training**                                   |
| 24.03 |                                         √âcole polytechnique f√©d√©rale de Lausanne, Carnegie Mellon University, University of Maryland College Park                                         |        arxiv        |                                      ["Flex Tape Can‚Äôt Fix That": Bias and Misinformation in Edited Language Models](https://arxiv.org/abs/2403.00180)                                      |                            **Model Editing**&**Demographic Bias**&**Misinformation**                             |
| 24.03 |                                                                               East China Normal University                                                                                |        arxiv        |                                 [DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2403.00896)                                  |                 **Dialogue-level Hallucination**&**Benchmarking**&**Human-machine Interaction**                  |
| 24.03 |                                                                                     Peking University                                                                                     |        arxiv        |                       [Evaluating and Mitigating Number Hallucinations in Large Vision-Language Models: A Consistency Perspective](https://arxiv.org/abs/2403.01373)                        |                   **Number Hallucination**&**Vision-Language Models**&**Consistency Training**                   |
| 24.03 | City University of Hong Kong, National University of Singapore, Shanghai Jiao Tong University, Stanford University, Penn State University, Hong Kong University of Science and Technology |        arxiv        |                            [IN-CONTEXT SHARPNESS AS ALERTS: AN INNER REPRESENTATION PERSPECTIVE FOR HALLUCINATION MITIGATION](https://arxiv.org/abs/2403.01548)                             |                              **Hallucination**&**Inner Representation**&**Entropy**                              |
| 24.03 |                                                                                         Microsoft                                                                                         |        arxiv        |                                        [In Search of Truth: An Interrogation Approach to Hallucination Detection](https://arxiv.org/abs/2403.02889)                                         |                  **Hallucination Detection**&**Interrogation Technique**&**Balanced Accuracy**                   |
| 24.03 |                                                                  Mohamed bin Zayed University of Artificial Intelligence                                                                  |        arxiv        |                                          [Multimodal Large Language Models to Support Real-World Fact-Checking](https://arxiv.org/abs/2403.03627)                                           |                    **Multimodal Large Language Models**&**Fact-Checking**&**Misinformation**                     |
| 24.03 |                                                                              KAIST, Microsoft Research Asia                                                                               |        arxiv        |                    [ERBENCH: AN ENTITY-RELATIONSHIP BASED AUTOMATICALLY VERIFIABLE HALLUCINATION BENCHMARK FOR LARGE LANGUAGE MODELS](https://arxiv.org/abs/2403.05266)                     |                         **Hallucination**&**Entity-Relationship Model**&**Benchmarking**                         |
| 24.03 |                                                                University of Alberta, Platform and Content Group, Tencent                                                                 |        arxiv        |                                            [SIFiD: Reassess Summary Factual Inconsistency Detection with LLM](https://arxiv.org/abs/2403.07557)                                             |                                    **Factual Consistency**&**Summarization**                                     |
| 24.03 |              Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences              |        arxiv        |                [Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts](https://arxiv.org/abs/2403.07556)                |                                    **Truth Detection**&**Context Selection**                                     |
| 24.03 |                                                                               UC Berkeley, Google DeepMind                                                                                |        arxiv        |                                         [Unfamiliar Finetuning Examples Control How Language Models Hallucinate](https://arxiv.org/abs/2403.05612)                                          |                        **Large Language Models**&**Finetuning**&**Hallucination Control**                        |
| 24.03 |                                                                University of Alberta, Platform and Content Group, Tencent                                                                 |        arxiv        |                                            [SIFiD: Reassess Summary Factual Inconsistency Detection with LLM](https://arxiv.org/abs/2403.07557)                                             |                                    **Factual Consistency**&**Summarization**                                     |
| 24.03 |              Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences              |        arxiv        |                [Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts](https://arxiv.org/abs/2403.07556)                |                                    **Truth Detection**&**Context Selection**                                     |
| 24.03 |                                                                               UC Berkeley, Google DeepMind                                                                                |        arxiv        |                                         [Unfamiliar Finetuning Examples Control How Language Models Hallucinate](https://arxiv.org/abs/2403.05612)                                          |                        **Large Language Models**&**Finetuning**&**Hallucination Control**                        |
| 24.03 |                                                                               Google Research, UC San Diego                                                                               |     COLING 2024     |                         [Detecting Hallucination and Coverage Errors in Retrieval Augmented Generation for Controversial Topics](https://arxiv.org/abs/2403.08904)                          |                             **Conversational Systems**&**Evaluation Methodologies**                              |
| 24.03 |                                                            University of Maryland, University of Antwerp, New York University                                                             |        arxiv        |                                                [Evaluating LLMs for Gender Disparities in Notable Persons](https://arxiv.org/abs/2403.09148)                                                |                                     **Bias**&**Fairness**&**Hallucinations**                                     |
| 24.03 |                                                                               University of Duisburg-Essen                                                                                |        arxiv        |              [The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions](https://arxiv.org/abs/2403.09743)               |                                                **Hallucination**                                                 |
| 24.03 |                                             Wuhan University, Beihang University, The University of Sydney, Nanyang Technological University                                              |     COLING 2024     |                         [Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in Factual Knowledge Extraction](https://arxiv.org/abs/2403.09963)                         |                                 **Factual Knowledge Extraction**&**Prompt Bias**                                 |
| 24.03 |                                                                                Carnegie Mellon University                                                                                 |        arxiv        |          [Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases](https://arxiv.org/abs/2403.10446)          |             **Retrieval Augmented Generation (RAG)**&**Private Knowledge-Bases**&**Hallucinations**              |
| 24.03 |                                                                   Integrated Vision and Language Lab KAIST South Korea                                                                    |        arxiv        |                           [What if...?: Counterfactual Inception to Mitigate Hallucination Effects in Large Multimodal Models](https://arxiv.org/abs/2403.13513)                            |                                  **Large Multimodal Models**&**Hallucination**                                   |
| 24.03 |                                                                                           UCAS                                                                                            |        arxiv        |                         [MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation](https://arxiv.org/abs/2403.14171)                          |                        **Multimodal Misinformation Detection**&**Knowledge Distillation**                        |
| 24.03 |                                                                       Seoul National University, Sogang University                                                                        |        arxiv        |                                 [Exploiting Semantic Reconstruction to Mitigate Hallucinations in Vision-Language Models](https://arxiv.org/abs/2403.16167)                                 |               **Semantic Reconstruction**&**Vision-Language Models**&**Hallucination Mitigation**                |
| 24.03 |                                                                          University of Illinois Urbana-Champaign                                                                          |        arxiv        |               [Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art](https://arxiv.org/abs/2403.16527)                |                      **Hallucination Detection**&**Foundation Models**&**Decision-Making**                       |
| 24.03 |                                                                               Shanghai Jiao Tong University                                                                               |        arxiv        |                       [Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback](https://arxiv.org/abs/2403.18349)                        |                      **Knowledge Feedback**&**Reliable Reward Model**&**Refusal Mechanism**                      |
| 24.03 |                                                                       Universit√§t Hamburg, The University of Sydney                                                                       |        arxiv        |                             [Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding](https://arxiv.org/abs/2403.18715)                             |        **Instruction Contrastive Decoding**&**Large Vision-Language Models**&**Hallucination Mitigation**        |
| 24.03 |                   AI Institute University of South Carolina, Indian Institute of Technology Kharagpur, Islamic University of Technology, Stanford University, Amazon AI                   |        arxiv        |            [‚ÄúSorry Come Again?‚Äù Prompting ‚Äì Enhancing Comprehension and Diminishing Hallucination with [PAUSE] -injected Optimal Paraphrasing](https://arxiv.org/abs/2403.18976)            |                    **Prompt Engineering**&**Hallucination Mitigation**&**[PAUSE] Injection**                     |
| 24.04 |                                          Beihang University, School of Computer Science and Engineering, School of Software, Shandong University                                          |        arxiv        |                                         [Exploring and Evaluating Hallucinations in LLM-Powered Code Generation](https://arxiv.org/abs/2404.00971)                                          |                                      **Code Generation**&**Hallucination**                                       |
| 24.03 |                                   Department of Electronic Engineering, Tsinghua University, Pattern Recognition Center, WeChat AI, Tencent Inc, China                                    |     NAACL 2024      |                                           [On Large Language Models‚Äô Hallucination with Regard to Known Facts](https://arxiv.org/abs/2403.20009)                                            |                                     **Hallucination**&**Inference Dynamics**                                     |
| 24.04 |                                                                            University of California, Berkeley                                                                             |     NAACL 2024      |                                              [ALOHa: A New Measure for Hallucination in Captioning Models](https://davidmchan.github.io/aloha)                                              |                                   **Adversarial Attack**&**AI-Text Detection**                                   |
| 24.04 |                                                      Technical University of Munich, University of Stavanger, University of Alberta                                                       |        arxiv        |                              [PoLLMgraph: Unraveling Hallucinations in Large Language Models via State Transition Dynamics](https://arxiv.org/abs/2404.04722)                               |               **Hallucination Detection**&**State Transition Dynamics**&**Large Language Models**                |
| 24.04 |                                                    University of Edinburgh, University College London, Peking University, Together AI                                                     |        arxiv        |                           [The Hallucinations Leaderboard ‚Äì An Open Effort to Measure Hallucinations in Large Language Models](https://arxiv.org/abs/2404.05904)                            |                                   **Hallucination Detection**&**Benchmarking**                                   |
| 24.04 |                                                IIIT Hyderabad, Purdue University, Northwestern University, Indiana University Indianapolis                                                |        arxiv        |               [Halu-NLP at SemEval-2024 Task 6: MetaCheckGPT - A Multi-task Hallucination Detection Using LLM Uncertainty and Meta-models](https://arxiv.org/abs/2404.06948)                |                         **Hallucination Detection**&**LLM Uncertainty**&**Meta-models**                          |
| 24.04 |                                                                                        ServiceNow                                                                                         |     NAACL 2024      |                                     [Reducing hallucination in structured outputs via Retrieval-Augmented Generation](https://arxiv.org/abs/2404.08189)                                     |                   **Retrieval-Augmented Generation**&**Structured Outputs**&**Generative AI**                    |



## üíªPresentations & Talks



## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

## üßë‚Äçüè´Scholars