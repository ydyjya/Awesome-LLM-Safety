# Truthfulness&Misinformation

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"

## üìëPapers

| Date  |                                                                                         Institute                                                                                         |      Publication       |                                                                                                 Paper                                                                                                 |                                                     Keywords                                                   |
|:-----:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:----------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------:|
| 21.09 |                                                                                   University of Oxford                                                                                    |        ACL2022         |                                                      [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)                                                      |                                          **Benchmark**&**Truthfulness**                                        |
| 23.05 |                                                                                           KAIST                                                                                           |  NAACL2024(findings)   |                               [Why So Gullible? Enhancing the Robustness of Retrieval-Augmented Models against Counterfactual Noise](https://arxiv.org/abs/2305.01579)                                |            **Retrieval-Augmented Models**&**Counterfactual Noise**&**Open-Domain Question Answering**          |
| 23.07 |                   Microsoft Research Asia, Hong Kong University of Science and Technology, University of Science and Technology of China, Tsinghua University, Sony AI                    |     ResearchSquare     |                                         [Defending ChatGPT against Jailbreak Attack via Self-Reminder](https://www.researchsquare.com/article/rs-2873090/v1)                                          |                              **Jailbreak Attack**&**Self-Reminder**&**AI Security**                            |
| 23.10 |                                                                                   University of Zurich                                                                                    |         arxiv          |                                               [Lost in Translation -- Multilingual Misinformation and its Evolution](https://arxiv.org/abs/2310.18089)                                                |                                       **Misinformation**&**Multilingual**                                      |
| 23.10 |                                                                             New York University&Javier Rando                                                                              |         arxiv          |                                                    [Personas as a Way to Model Truthfulness in Language Models](https://arxiv.org/abs/2310.18168)                                                     |                                      **Truthfulness**&**Truthful Persona**                                     |
| 23.10 |                                                   Tsinghua University, Allen Institute for AI, University of Illinois Urbana-Champaign                                                    |       NAACL2024        |                                                  [Language Models Hallucinate, but May Excel at Fact Verification](https://arxiv.org/abs/2310.14564)                                                  |                        **Large Language Models**&**Hallucination**&**Fact Verification**                       |
| 23.10 |                                 Stanford University&University of Maryland&Carnegie Mellon University&NYU Shanghai&New York University&Microsoft Research                                 |       NAACL2024        |                                   [Large Language Models Help Humans Verify Truthfulness‚ÄîExcept When They Are Convincingly Wrong](https://arxiv.org/abs/2310.12558)                                   |                           **Large Language Models**&**Fact-Checking**&**Truthfulness**                         |
| 23.10 |                                                                                    Shandong University                                                                                    |       NAACL2024        |                                            [Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method](https://arxiv.org/abs/2310.17918)                                            |                    **Large Language Models**&**Self-Detection**&**Non-Factuality Detection**                   |
| 23.10 |                                                                                     Fudan University                                                                                      |       CIKM 2023        |                                   [Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models](https://doi.org/10.1145/3583780.3614905)                                   |                                 **Hallucination Detection**&**Reliable Answers**                               |
| 23.11 |                                                                                    Dialpad Canada Inc                                                                                     |         arxiv          |                               [Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs](https://arxiv.org/abs/2311.00681)                                |                                            **Factuality Assessment**                                           |
| 23.11 |                                                                               The University of Manchester                                                                                |         arxiv          |                                                          [Emotion Detection for Misinformation: A Review](https://arxiv.org/abs/2311.00671)                                                           |                                    **Survey**&**Misinformation**&**Emotions**                                  |
| 23.11 |                                                                                  University of Virginia                                                                                   |         arxiv          |                                  [Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder with Semantics](https://arxiv.org/abs/2311.01386)                                  |                                              **Language Illusions**                                            |
| 23.11 |                                                                          University of Illinois Urbana-Champaign                                                                          |         arxiv          |               [Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism](https://arxiv.org/abs/2311.01041)               |                                     **Hallucinations**&**Refusal Mechanism**                                   |
| 23.11 |                                                                             University of Washington Bothell                                                                              |         arxiv          |                                              [Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI](https://arxiv.org/abs/2311.01463)                                              |                              **Healthcare**&**Trustworthiness**&**Hallucinations**                             |
| 23.11 |                                                                                    Intuit AI Research                                                                                     |       EMNLP2023        |                          [SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency](https://arxiv.org/abs/2311.01740)                           |                                 **Hallucination Detection**&**Trustworthiness**                                |
| 23.11 |                                                                               Shanghai Jiao Tong University                                                                               |         arxiv          |                               [Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation](https://arxiv.org/abs/2311.01766)                                |                             **Misinformation**&**Disinformation**&**Out-of-Context**                           |
| 23.11 |                                                                               Hamad Bin Khalifa University                                                                                |         arxiv          |                                      [ArAIEval Shared Task: Persuasion Techniques and Disinformation Detection in Arabic Text](https://arxiv.org/abs/2311.03179)                                      |                                        **Disinformation**&**Arabic Text**                                      |
| 23.11 |                                                                                      UNC-Chapel Hill                                                                                      |         arxiv          |                                       [Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges](https://arxiv.org/abs/2311.03287)                                       |                                  **Hallucination**&**Benchmark**&**Multimodal**                                |
| 23.11 |                                                                                    Cornell University                                                                                     |         arxiv          |                                                 [Adapting Fake News Detection to the Era of Large Language Models](https://arxiv.org/abs/2311.04917)                                                  |                          **Fake news detection**&**Generated News**&**Misinformation**                         |
| 23.11 |                                                                              Harbin Institute of Technology                                                                               |         arxiv          |                             [A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/abs/2311.05232)                              |                          **Hallucination**&**Factual Consistency**&**Trustworthiness**                         |
| 23.11 |                                                                         Korea University, KAIST AI,LG AI Research                                                                         |         arXiv          |                                        [VOLCANO: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision](https://arxiv.org/abs/2311.07362)                                         |                            **Multimodal Models**&**Hallucination**&**Self-Feedback**                           |
| 23.11 |                                                                Beijing Jiaotong University, Alibaba Group, Peng Cheng Lab                                                                 |         arXiv          |                                         [AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation](https://arxiv.org/abs/2311.07397)                                         |                            Multi-modal Large Language Models&Hallucination&Benchmark                           |
| 23.11 |                                                              LMU Munich; Munich Center of Machine Learning; Google Research                                                               |         arXiv          |                                                      [Hallucination Augmented Recitations for Language Models](https://arxiv.org/abs/2311.07424)                                                      |                                  **Hallucination**&**Counterfactual Datasets**                                 |
| 23.11 |                                                                           Stanford University, UNC Chapel Hill                                                                            |         arxiv          |                                                            [Fine-tuning Language Models for Factuality](https://arxiv.org/abs/2311.08401)                                                             |                **Factuality**&**Reference-Free Truthfulness**&**Direct Preference Optimization**               |
| 23.11 |                                                                        Corporate Data and Analytics Office (CDAO)                                                                         |         arxiv          |                                          [Hallucination-minimized Data-to-answer Framework for Financial Decision-makers](https://arxiv.org/abs/2311.07592)                                           |                           **Financial Decision Making**&**Hallucination Minimization**                         |
| 23.11 |                                                                                 Arizona State University                                                                                  |         arxiv          |                                                  [Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey](https://arxiv.org/abs/2311.07914)                                                   |                                **Knowledge Graphs**&**Hallucinations**&**Survey**                              |
| 23.11 |                                                       Kempelen Institute of Intelligent Technologies; Brno University of Technology                                                       |         arxiv          |                                                       [Disinformation Capabilities of Large Language Models](https://arxiv.org/abs/2311.08838)                                                        |                    **Disinformation Generation**&**Safety Filters**&**Automated Evaluation**                   |
| 23.11 |                                                                         UNC-Chapel Hill, University of Washington                                                                         |         arxiv          |                             [EVER: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification](https://arxiv.org/abs/2311.09114)                              |                          **Hallucination**&**Real-Time Verification**&**Rectification**                        |
| 23.11 |                                                                        Peking University, WeChat AI, Tencent Inc.                                                                         |         arXiv          |                                         [RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge](https://arxiv.org/abs/2311.08147)                                         |                      **External Counterfactual Knowledge**&**Benchmarking**&**Robustness**                     |
| 23.11 |                                                                                      PolyAI Limited                                                                                       |         arXiv          |                          [Dial BEINFO for Faithfulness: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning](https://arxiv.org/abs/2311.09800)                           |                           **Factuality**&**Behavioural Fine-Tuning**&**Hallucination**                         |
| 23.11 |                                                The Hong Kong University of Science and Technology, University of Illinois Urbana-Champaign                                                |         arxiv          |                                               [R-Tuning: Teaching Large Language Models to Refuse Unknown Questions](https://arxiv.org/abs/2311.09677)                                                |                     **Hallucination**&**Refusal-Aware Instruction Tuning**&**Knowledge Gap**                   |
| 23.11 |                                               University of Southern California, University of Pennsylvania, University of California Davis                                               |         arxiv          |                                  [Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?](https://arxiv.org/abs/2311.09702)                                   |                            **Hallucinations**&**Semantic Associations**&**Benchmark**                          |
| 23.11 |                                                                 The Ohio State University, University of California Davis                                                                 |         arxiv          |                          [How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities](https://arxiv.org/abs/2311.09447)                           |                     **Trustworthiness**&**Malicious Demonstrations**&**Adversarial Attacks**                   |
| 23.11 |                                                                                  University of Sheffield                                                                                  |         arXiv          |                       [Lighter yet More Faithful: Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization](https://arxiv.org/abs/2311.09335)                       |                                **Hallucinations**&&**Language Model Reliability**                              |
| 23.11 |                                        Institute of Information Engineering Chinese Academy of Sciences, University of Chinese Academy of Sciences                                        |         arxiv          |                           [Can Large Language Models Understand Content and Propagation for Misinformation Detection: An Empirical Study](https://arxiv.org/abs/2311.12699)                           |                                           **Misinformation Detection**                                         |
| 23.11 |                                        Shanghai Jiaotong University, Amazon AWS AI, Westlake University, IGSNRR Chinese Academy of Sciences, China                                        |         arXiv          |                                              [Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus](https://arxiv.org/abs/2311.13230)                                              |                **Hallucination Detection**&**Uncertainty-Based Methods**&**Factuality Checking**               |
| 23.11 |                                               Institute of Software Chinese Academy of Sciences, University of Chinese Academy of Sciences                                                |         arXiv          |                                 [Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting](https://arxiv.org/abs/2311.13314)                                  |                             **Hallucinations**&**Knowledge Graphs**&**Retrofitting**                           |
| 23.11 |                                                                                Applied Research Quantiphi                                                                                 |         arxiv          |                                            [Minimizing Factual Inconsistency and Hallucination in Large Language Models](https://arxiv.org/abs/2311.13878)                                            |                                   **Factual Inconsistency**&**Hallucination**                                  |
| 23.11 |                                                                             Microsoft Research, Georgia Tech                                                                              |         arxiv          |                                                            [Calibrated Language Models Must Hallucinate](https://arxiv.org/abs/2311.14648)                                                            |                              **Hallucination&Calibration**&**Statistical Analysis**                            |
| 23.11 |                                                                     School of Information Renmin University of China                                                                      |         arxiv          |                               [UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation](https://arxiv.org/abs/2311.15296)                               |                                    **Hallucination**&**Evaluation Benchmark**                                  |
| 23.11 |                                                          DAMO Academy Alibaba Group, Nanyang Technological University, Hupan Lab                                                          |         arxiv          |                               [Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding](https://arxiv.org/abs/2311.16922)                                |                               **Vision-Language Models**&**Object Hallucinations**                             |
| 23.11 |                                                                                  Shanghai AI Laboratory                                                                                   |         arxiv          |                                 [Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization](https://arxiv.org/abs/2311.16839)                                 |           **Multimodal Language Models**&**Hallucination Problem**&**Direct Preference Optimization**          |
| 23.11 |                                                                                 Arizona State University                                                                                  |       NAACL2024        |                                                  [Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey](https://arxiv.org/abs/2311.07914)                                                   |                    **Knowledge Graphs**&**Large Language Models**&**Hallucination Reduction**                  |
| 23.11 |                                                                  Mohamed bin Zayed University of Artificial Intelligence                                                                  |       NAACL2024        |                                            [A Survey of Confidence Estimation and Calibration in Large Language Models](https://arxiv.org/abs/2311.08298)                                             |                       **Confidence Estimation**&**Calibration**&**Large Language Models**                      |
| 23.11 |                                                                              University of California, Davis                                                                              |       NAACL2024        |                                  [Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?](https://arxiv.org/abs/2311.09702)                                   |                          **Semantic Shortcuts**&**Reasoning Chains**&**Hallucination**                         |
| 23.11 |                                                                                    University of Utah                                                                                     |       NAACL2024        |                                                   [To Tell The Truth: Language of Deception and Language Models](https://arxiv.org/abs/2311.07092)                                                    |                     **Deception Detection**&**Language Models**&**Conversational Analysis**                    |
| 23.11 |                                                                                    Cornell University                                                                                     |  NAACL2024(findings)   |                                                 [Adapting Fake News Detection to the Era of Large Language Models](https://arxiv.org/abs/2311.04917)                                                  |                 **Fake News Detection**&**Large Language Models**&**Machine-Generated Content**                |
| 23.12 |                                  Singapore Management University, Beijing Forestry University, University of Electronic Science and Technology of China                                   |        MMM 2024        |                              [Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites](https://arxiv.org/abs/2312.01701)                              |                     **Vision-language Models**&**Hallucination**&**Fine-grained Evaluation**                   |
| 23.12 |                                                                                  Mila, McGill University                                                                                  |  EMNLP2023(findings)   |                                [Evaluating Dependencies in Fact Editing for Language Models: Specificity and Implication Awareness](https://arxiv.org/abs/2312.01858)                                 |                             **Knowledge Bases**&**Dataset**&**Evaluation Protocol**                            |
| 23.12 |                                                                                         MIT CSAIL                                                                                         |         arxiv          |                            [Cognitive Dissonance: Why Do Language Model Outputs Disagree with Internal Representations of Truthfulness?](https://arxiv.org/abs/2312.03729)                            |                                  **Truthfulness**&**Internal Representations**                                 |
| 23.12 |                              University of Illinois Chicago, Bosch Research North America & Bosch Center for Artificial Intelligence (BCAI), UNC Chapel-Hill                              |         arxiv          |                                            [DELUCIONQA: Detecting Hallucinations in Domain-specific Question Answering](https://arxiv.org/abs/2312.05200)                                             |                 **Hallucination Detection**&**Domain-specific QA**&**Retrieval-augmented LLMs**                |
| 23.12 |                                                                      The University of Hong Kong, Beihang University                                                                      |        AAAI2024        |                                              [Improving Factual Error Correction by Learning to Inject Factual Errors](https://arxiv.org/abs/2312.07049)                                              |                                           **Factual Error Correction**                                         |
| 23.12 |                                                                                  Allen Institute for AI                                                                                   |         arxiv          |                                    [BARDA: A Belief and Reasoning Dataset that Separates Factual Accuracy and Reasoning Ability](https://arxiv.org/abs/2312.07527)                                    |                              **Dataset**&**Factual Accuracy**&**Reasoning Ability**                            |
| 23.12 |                                         Tsinghua University, Shanghai Jiao Tong University, Stanford University, Nanyang Technological University                                         |         arxiv          |                            [The Earth is Flat because...: Investigating LLMs‚Äô Belief towards Misinformation via Persuasive Conversation](https://arxiv.org/abs/2312.09085)                            |                       **Misinformation**&**Persuasive Conversation**&**Factual Questions**                     |
| 23.12 |                                                                              University of California Davis                                                                               |         arXiv          |                                              [A Revisit of Fake News Dataset with Augmented Fact-checking by ChatGPT](https://arxiv.org/abs/2312.11870)                                               |                                         **Fake News**&**Fact-checking**                                        |
| 23.12 |                                                                                    Amazon Web Services                                                                                    |         arxiv          |                                                [On Early Detection of Hallucinations in Factual Question Answering](https://arxiv.org/abs/2312.14183)                                                 |                                **Hallucinations**&**Factual Question Answering**                               |
| 23.12 |                                                                            University of California Santa Cruz                                                                            |         arxiv          |      [Don‚Äôt Believe Everything You Read: Enhancing Summarization Interpretability through Automatic Identification of Hallucinations in Large Language Models](https://arxiv.org/abs/2312.14346)      |                               **Hallucinations**&**Faithfulness**&**Token-level**                              |
| 23.12 |                                                                 Department of Radiology, The University of Tokyo Hospital                                                                 |         arxiv          |                                                          [Theory of Hallucinations based on Equivariance](https://arxiv.org/abs/2312.14504)                                                           |                                       **Hallucinations**&**Equivariance**                                      |
| 23.12 |                                                                              Georgia Institute of Technology                                                                              |         arXiv          |                                                    [REDUCING LLM HALLUCINATIONS USING EPISTEMIC NEURAL NETWORKS](https://arxiv.org/abs/2312.15576)                                                    |                           **Hallucinations**&**Uncertainty Estimation**&**TruthfulQA**                         |
| 23.12 |                                    Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Tencent AI Lab                                    |         arXiv          |                                        [Alleviating Hallucinations of Large Language Models through Induced Hallucinations](https://arxiv.org/abs/2312.15710)                                         |                       **Hallucinations**&**Induce-then-Contrast Decoding**&**Factuality**                      |
| 23.12 |                        SKLOIS Institute of Information Engineering Chinese Academy of Sciences, School of Cyber Security University of Chinese Academy of Sciences                        |         arXiv          |                                        [LLM Factoscope: Uncovering LLMs‚Äô Factual Discernment through Inner States Analysis](https://arxiv.org/abs/2312.16374)                                         |                                      **Factual Detection**&**Inner States**                                    |
| 24.01 |                                                                    The Chinese University of Hong Kong, Tencent AI Lab                                                                    |         arxiv          |                                               [The Earth is Flat? Unveiling Factual Errors in Large Language Models](https://arxiv.org/abs/2401.00761)                                                |                           **Factual Errors**&**Knowledge Graph**&**Answer Assessment**                         |
| 24.01 |                                                                    NewsBreak, University of Illinois Urbana-Champaign                                                                     |         arxiv          |                                  [RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models](https://arxiv.org/abs/2401.00396)                                  |                    **Retrieval-Augmented Generation**&**Hallucination Detection**&**Dataset**                  |
| 24.01 |                                                    University of California Berkeley, Universit√© de Montr√©al, McGill UniversityÔºå Mila                                                     |         arxiv          |                                                        [Uncertainty Resolution in Misinformation Detection](https://arxiv.org/abs/2401.01197)                                                         |                                  **Misinformation**&**Uncertainty Resolution**                                 |
| 24.01 |                                                                           Yale University, Stanford University                                                                            |         arxiv          |                                           [Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models](https://arxiv.org/abs/2401.01301)                                           |                                             **Legal Hallucinations**                                           |
| 24.01 |                                        Islamic University of Technology, AI Institute University of South Carolina, Stanford University, Amazon AI                                        |         arxiv          |                                      [A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models](https://arxiv.org/abs/2401.01313)                                       |                           √ü                               **Hallucination Mitigation**                         |
| 24.01 |                                                   Renmin University of China, Renmin University of China, DIRO, Universit√© de Montr√©al                                                    |         arxiv          |                                 [The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models](https://arxiv.org/abs/2401.03205)                                  |                        **Hallucination**&**Detection and Mitigation**&**Empirical Study**                      |
| 24.01 |                                       IIT Hyderabad India, Parmonic USA, University of Glasgow UK, LDRP Institute of Technology and Research India                                        |         arxiv          |                                   [Fighting Fire with Fire: Adversarial Prompting to Generate a Misinformation Detection Dataset](https://arxiv.org/abs/2401.04481)                                   |                          **Misinformation Detection**&**LLM-generated Synthetic Data**                         |
| 24.01 |                                                                                 University College London                                                                                 |         arxiv          |                                                   [Hallucination Benchmark in Medical Visual Question Answering](https://arxiv.org/abs/2401.05827)                                                    |                        **Medical Visual Question Answering**&**Hallucination Benchmark**                       |
| 24.01 |                                                                                    Soochow University                                                                                     |         arxiv          |                                                             [LightHouse: A Survey of AGI Hallucination](https://arxiv.org/abs/2401.06792)                                                             |                                              **AGI Hallucination**                                             |
| 24.01 |                                                       University of Washington, Carnegie Mellon University, Allen Institute for AI                                                        |         arxiv          |                                               [Fine-grained Hallucination Detection and Editing for Language Models](https://arxiv.org/abs/2401.06855)                                                |                                       **Hallucination Detection**&**FAVA**                                     |
| 24.01 |                                                             Dartmouth College, Universit√© de Montr√©al, McGill UniversityÔºåMila                                                             |         arxiv          |                                           [Comparing GPT-4 and Open-Source Language Models in Misinformation Mitigation](https://arxiv.org/abs/2401.06920)                                            |                                      **GPT-4**&**Misinformation Detection**                                    |
| 24.01 |                                                                                    Utrecht University                                                                                     |         arxiv          |                                                              [The Pitfalls of Defining Hallucination](https://arxiv.org/abs/2401.07897)                                                               |                                                **Hallucination**                                               |
| 24.01 |                                                                                     Samsung AI Center                                                                                     |         arxiv          |                                              [Hallucination Detection and Hallucination Mitigation: An Investigation](https://arxiv.org/abs/2401.08358)                                               |                             **Hallucination Detection**&**Hallucination Mitigation**                           |
| 24.01 |                                                                      McGill UniversityÔºå Mila, Universit√© de Montr√©al                                                                      |         arxiv          |                       [Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation](https://arxiv.org/abs/2401.08694)                       |            **Misinformation Mitigation**&**Uncertainty Quantification**&**Sample-based Consistency**           |
| 24.01 |                                                                                      LY Corporation                                                                                       |         arxiv          |                                                 [On the Audio Hallucinations in Large Audio-Video Language Models](https://arxiv.org/abs/2401.09774)                                                  |                **Audio Hallucinations**&**Audio-visual Learning**&**Audio-video language Models**              |
| 24.01 |                                                                           Sun Yat-sen University Tencent AI Lab                                                                           |         arXiv          |                                       [Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment](https://arxiv.org/abs/2401.10768)                                       |                         **Hallucination Mitigation**&**Knowledge Consistent Alignment**                        |
| 24.01 |                                                                             National University of Singapore                                                                              |         arxiv          |                                            [Hallucination is Inevitable: An Innate Limitation of Large Language Models](https://arxiv.org/abs/2401.11817)                                             |                                      **Hallucination**&**Real World LLMs**                                     |
| 24.01 |                                                                       X2Robot&International Digital Economy Academy                                                                       |         arXiv          |                                  [Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for Hallucination Mitigation](https://arxiv.org/abs/2401.15449)                                  |                  **Hallucination Mitigation**&**Knowledge Probing**&**Reinforcement Learning**                 |
| 24.01 |                                                                  University of Texas at Austin, Northeastern University                                                                   |         arxiv          |                            [Diverse but Divisive: LLMs Can Exaggerate Gender Differences in Opinion Related to Harms of Misinformation](https://arxiv.org/abs/2401.16558)                             |                             **Misinformation Detection**&**Socio-Technical Systems**                           |
| 24.01 |                                                        National University of Defense Technology, National University of Singapore                                                        |         arxiv          |                                   [SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering](https://arxiv.org/abs/2401.17809)                                   |                                **Factual Knowledge Editing**&**Word Embeddings**                               |
| 24.02 |                        University of Washington, University of California Berkeley, The Hong Kong University of Science and Technology, Carnegie Mellon University                        |         arxiv          |                                       [Don‚Äôt Hallucinate Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration](https://arxiv.org/abs/2402.00367)                                       |                                  **Knowledge Gaps**&**Multi-LLM Collaboration**                                |
| 24.02 |                                                                  IT Innovation and Research Center, Huawei Technologies                                                                   |         arxiv          |                                                     [A Survey on Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2402.00253)                                                     |                   **Large Vision-Language Models**&**Hallucination**&**Mitigation Strategies**                 |
| 24.02 |                                                               Tianjin University, National University of Singapore, A*STAR                                                                |         arxiv          |                                         [SKIP \N: A SIMPLE METHOD TO REDUCE HALLUCINATION IN LARGE VISION-LANGUAGE MODELS](https://arxiv.org/abs/2402.01345)                                          |                 **Semantic Shift Bias**&**Hallucination Mitigation**&**Vision-Language Models**                |
| 24.02 |                                                                       University of Marburg, University of Mannheim                                                                       |   EACL Findings 2024   |                                       [The Queen of England is not England‚Äôs Queen: On the Lack of Factual Coherency in PLMs](https://arxiv.org/abs/2402.01453)                                       |                                    **Factual Coherency**&**Knowledge Bases**                                   |
| 24.02 |                                                                    MBZUAI, Monash University, LibrAI, Sofia University                                                                    |         arxiv          |                                                       [Factuality of Large Language Models in the Year 2024](https://arxiv.org/abs/2402.02420)                                                        |                                **Factuality**&**Evaluation**&**Multimodal LLMs**                               |
| 24.02 |                                       Institute of Information Engineering, Chinese Academy of Sciences, University of Chinese Academy of Sciences                                        |         arxiv          |                                                       [Are Large Language Models Table-based Fact-Checkers?](https://arxiv.org/abs/2402.02549)                                                        |                            **Table-based Fact Verification**&**In-context Learning**                           |
| 24.02 |                                                                              Zhejiang University, Ant Group                                                                               |         arxiv          |                                               [Unified Hallucination Detection for Multimodal Large Language Models](https://arxiv.org/abs/2402.03190)                                                |                  **Multimodal Large Language Models**&**Hallucination Detection**&**Benchmark**                |
| 24.02 |                                                                            Alibaba Cloud, Zhejiang University                                                                             |        ICLR2024        |                                             [INSIDE: LLMS‚Äô INTERNAL STATES RETAIN THE POWER OF HALLUCINATION DETECTION](https://arxiv.org/abs/2402.03744)                                             |                                    **Hallucination Detection**&**EigenScore**                                  |
| 24.02 |                           The Hong Kong University of Science and Technology, University of Illinois at Urbana-Champaign, The Hong Kong Polytechnic University                            |         arxiv          |                                               [The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs](https://arxiv.org/abs/2402.03757)                                                |                              **Multimodal Large Language Models**&**Hallucination**                            |
| 24.02 |                                              Institute of Automation Chinese Academy of Sciences, University of Chinese Academy of Sciences                                               |         arxiv          |                                                     [Can Large Language Models Detect Rumors on Social Media?](https://arxiv.org/abs/2402.03916)                                                      |                                       **Rumor Detection**&**Social Media**                                     |
| 24.02 |         CAS Key Laboratory of AI Safety, School of Computer Science and Technology University of Chinese Academy of Science, International Digital Economy Academy IDEA Research          |         arxiv          |                                            [A Survey on Large Language Model Hallucination via a Creativity Perspective](https://arxiv.org/abs/2402.06647)                                            |                                         **Creativity**&**Hallucination**                                       |
| 24.02 |                                                             University College London, Speechmatics, MATS, Anthropic, FAR AI                                                              |         arxiv          |                                                 [Debating with More Persuasive LLMs Leads to More Truthful Answers](https://arxiv.org/abs/2402.06782)                                                 |                                           **Debate**&**Truthfulness**                                          |
| 24.02 |                                               University of Illinois Urbana-Champaign, DAMO Academy Alibaba Group, Northwestern University                                                |         arxiv          |                                                 [Towards Faithful Explainable Fact-Checking via Multi-Agent Debate](https://arxiv.org/abs/2402.07401)                                                 |                                       **Fact-checking**&**Explainability**                                     |
| 24.02 |                                  Rice Universitym, Texas A&M University, Wake Forest University, New Jersey Institute of Technology, Meta Platforms Inc.                                  |         arxiv          |                                                           [Large Language Models As Faithful Explainers](https://arxiv.org/abs/2402.04678)                                                            |                                 **Explainability**&**Fidelity**&**Optimization**                               |
| 24.02 |                                                                    The Hong Kong University of Science and Technology                                                                     |         arxiv          |                                        [Do LLMs Know about Hallucination? An Empirical Investigation of LLM‚Äôs Hidden States](https://arxiv.org/abs/2402.09733)                                        |                           **Hallucination**&**Hidden States**&**Model Interpretation**                         |
| 24.02 |                                                                UC Santa Cruz, ByteDance Research, Northwestern University                                                                 |         arxiv          |                                  [MEASURING AND REDUCING LLM HALLUCINATION WITHOUT GOLD-STANDARD ANSWERS VIA EXPERTISE-WEIGHTING](https://arxiv.org/abs/2402.10412)                                   |             **Large Language Models (LLMs)**&**Hallucination**&**Factualness Evaluations**&**FEWL**            |
| 24.02 |                                                     Paul G. Allen School of Computer Science & Engineering, University of Washington                                                      |         arxiv          |                                               [Comparing Hallucination Detection Metrics for Multilingual Generation](https://arxiv.org/abs/2402.10496)                                               | **Hallucination Detection**&**Multilingual Generation**&**Lexical Metrics**&**Natural Language Inference (NLI)** |
| 24.02 |                                         Institute of Computing Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences                                         |         arxiv          |                        [Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models](https://arxiv.org/abs/2402.10612)                         |        **Large Language Models (LLMs)**&**Hallucination Mitigation**&**Retrieval Augmentation**&**Rowen**      |
| 24.02 |                                    Institute of Automation, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Nanjing University                                    |         arxiv          |                                       [Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2402.11622)                                       |                           **Object Hallucination**&**Vision-Language Models (LVLMs)**                          |
| 24.02 |                                 Institute of Mathematics and Statistics University of S√£o Paulo, Artificial Intelligence Specialist in the Banking Sector                                 |         arxiv          |                     [Hallucinations or Attention Misdirection? The Path to Strategic Value Extraction in Business Using Large Language Models](https://arxiv.org/abs/2402.14002)                      |                            **Hallucinations**&**Generative Artificial Intelligence**                           |
| 24.02 |                                                                       Stevens Institute of Technology, Peraton Labs                                                                       |         arxiv          |                                           [Can Large Language Models Detect Misinformation in Scientific News Reporting?](https://arxiv.org/abs/2402.14268)                                           |                          **Scientific Reporting**&**Misinformation**&**Explainability**                        |
| 24.02 |                                                                             Middle East Technical University                                                                              |         arxiv          |                                      [HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs](https://arxiv.org/abs/2402.16211)                                       |                                    **Hallucination**&**Benchmarking Dataset**                                  |
| 24.02 |                                                                             National University of Singapore                                                                              |         arxiv          |                              [Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding](https://arxiv.org/abs/2402.15300)                               |                      **Vision-Language Models**&**Hallucination**&**CLIP-Guided Decoding**                     |
| 24.02 |                                                                   University of California Los Angeles, Cisco Research                                                                    |         arxiv          |                                  [Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension](https://arxiv.org/abs/2402.18048)                                   |                                  **Truthfulness**&**Local Intrinsic Dimension**                                |
| 24.02 |                 Institute of Automation Chinese Academy of Sciences, School of Artificial Intelligence University of Chinese Academy of Sciences, Hunan Normal University                 |         arxiv          |                          [Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models](https://arxiv.org/abs/2402.19103)                          |                             **False Premise Hallucinations**&**Attention Mechanism**                           |
| 24.02 |        Shanghai Artificial Intelligence Laboratory, Renmin University of China, University of Chinese Academy of Sciences, Shanghai Jiao Tong University, The University of Sydney        |         arxiv          |                                 [Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models](https://arxiv.org/abs/2402.19465)                                 |                                  **Trustworthiness Dynamics**&**Pre-training**                                 |
| 24.02 |                                              AWS AI Labs&Korea Advanced Institute of Science & Technology&The University of Texas at Austin                                               |       NAACL2024        |                                        [TOFUEVAL: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization](https://arxiv.org/abs/2402.13249)                                        |                         **Hallucination Evaluation**&**LLMs**&**Dialogue Summarization**                       |
| 24.03 |                                         √âcole polytechnique f√©d√©rale de Lausanne, Carnegie Mellon University, University of Maryland College Park                                         |         arxiv          |                                           ["Flex Tape Can‚Äôt Fix That": Bias and Misinformation in Edited Language Models](https://arxiv.org/abs/2403.00180)                                           |                            **Model Editing**&**Demographic Bias**&**Misinformation**                           |
| 24.03 |                                                                               East China Normal University                                                                                |         arxiv          |                                      [DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2403.00896)                                       |                 **Dialogue-level Hallucination**&**Benchmarking**&**Human-machine Interaction**                |
| 24.03 |                                                                                     Peking University                                                                                     |         arxiv          |                            [Evaluating and Mitigating Number Hallucinations in Large Vision-Language Models: A Consistency Perspective](https://arxiv.org/abs/2403.01373)                             |                   **Number Hallucination**&**Vision-Language Models**&**Consistency Training**                 |
| 24.03 | City University of Hong Kong, National University of Singapore, Shanghai Jiao Tong University, Stanford University, Penn State University, Hong Kong University of Science and Technology |         arxiv          |                                 [IN-CONTEXT SHARPNESS AS ALERTS: AN INNER REPRESENTATION PERSPECTIVE FOR HALLUCINATION MITIGATION](https://arxiv.org/abs/2403.01548)                                  |                              **Hallucination**&**Inner Representation**&**Entropy**                            |
| 24.03 |                                                                                         Microsoft                                                                                         |         arxiv          |                                             [In Search of Truth: An Interrogation Approach to Hallucination Detection](https://arxiv.org/abs/2403.02889)                                              |                  **Hallucination Detection**&**Interrogation Technique**&**Balanced Accuracy**                 |
| 24.03 |                                                                  Mohamed bin Zayed University of Artificial Intelligence                                                                  |         arxiv          |                                               [Multimodal Large Language Models to Support Real-World Fact-Checking](https://arxiv.org/abs/2403.03627)                                                |                    **Multimodal Large Language Models**&**Fact-Checking**&**Misinformation**                   |
| 24.03 |                                                                              KAIST, Microsoft Research Asia                                                                               |         arxiv          |                         [ERBENCH: AN ENTITY-RELATIONSHIP BASED AUTOMATICALLY VERIFIABLE HALLUCINATION BENCHMARK FOR LARGE LANGUAGE MODELS](https://arxiv.org/abs/2403.05266)                          |                         **Hallucination**&**Entity-Relationship Model**&**Benchmarking**                       |
| 24.03 |                                                                University of Alberta, Platform and Content Group, Tencent                                                                 |         arxiv          |                                                 [SIFiD: Reassess Summary Factual Inconsistency Detection with LLM](https://arxiv.org/abs/2403.07557)                                                  |                                    **Factual Consistency**&**Summarization**                                   |
| 24.03 |              Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences              |         arxiv          |                     [Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts](https://arxiv.org/abs/2403.07556)                     |                                    **Truth Detection**&**Context Selection**                                   |
| 24.03 |                                                                               UC Berkeley, Google DeepMind                                                                                |         arxiv          |                                              [Unfamiliar Finetuning Examples Control How Language Models Hallucinate](https://arxiv.org/abs/2403.05612)                                               |                        **Large Language Models**&**Finetuning**&**Hallucination Control**                      |
| 24.03 |                                                                University of Alberta, Platform and Content Group, Tencent                                                                 |         arxiv          |                                                 [SIFiD: Reassess Summary Factual Inconsistency Detection with LLM](https://arxiv.org/abs/2403.07557)                                                  |                                    **Factual Consistency**&**Summarization**                                   |
| 24.03 |              Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences              |         arxiv          |                     [Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts](https://arxiv.org/abs/2403.07556)                     |                                    **Truth Detection**&**Context Selection**                                   |
| 24.03 |                                                                               UC Berkeley, Google DeepMind                                                                                |         arxiv          |                                              [Unfamiliar Finetuning Examples Control How Language Models Hallucinate](https://arxiv.org/abs/2403.05612)                                               |                        **Large Language Models**&**Finetuning**&**Hallucination Control**                      |
| 24.03 |                                                                               Google Research, UC San Diego                                                                               |      COLING 2024       |                              [Detecting Hallucination and Coverage Errors in Retrieval Augmented Generation for Controversial Topics](https://arxiv.org/abs/2403.08904)                               |                             **Conversational Systems**&**Evaluation Methodologies**                            |
| 24.03 |                                                            University of Maryland, University of Antwerp, New York University                                                             |         arxiv          |                                                     [Evaluating LLMs for Gender Disparities in Notable Persons](https://arxiv.org/abs/2403.09148)                                                     |                                     **Bias**&**Fairness**&**Hallucinations**                                   |
| 24.03 |                                                                               University of Duisburg-Essen                                                                                |         arxiv          |                   [The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions](https://arxiv.org/abs/2403.09743)                    |                                                **Hallucination**                                               |
| 24.03 |                                             Wuhan University, Beihang University, The University of Sydney, Nanyang Technological University                                              |      COLING 2024       |                              [Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in Factual Knowledge Extraction](https://arxiv.org/abs/2403.09963)                              |                                 **Factual Knowledge Extraction**&**Prompt Bias**                               |
| 24.03 |                                                                                Carnegie Mellon University                                                                                 |         arxiv          |               [Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases](https://arxiv.org/abs/2403.10446)               |             **Retrieval Augmented Generation (RAG)**&**Private Knowledge-Bases**&**Hallucinations**            |
| 24.03 |                                                                   Integrated Vision and Language Lab KAIST South Korea                                                                    |         arxiv          |                                [What if...?: Counterfactual Inception to Mitigate Hallucination Effects in Large Multimodal Models](https://arxiv.org/abs/2403.13513)                                 |                                  **Large Multimodal Models**&**Hallucination**                                 |
| 24.03 |                                                                                           UCAS                                                                                            |         arxiv          |                              [MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation](https://arxiv.org/abs/2403.14171)                               |                        **Multimodal Misinformation Detection**&**Knowledge Distillation**                      |
| 24.03 |                                                                       Seoul National University, Sogang University                                                                        |         arxiv          |                                      [Exploiting Semantic Reconstruction to Mitigate Hallucinations in Vision-Language Models](https://arxiv.org/abs/2403.16167)                                      |               **Semantic Reconstruction**&**Vision-Language Models**&**Hallucination Mitigation**              |
| 24.03 |                                                                          University of Illinois Urbana-Champaign                                                                          |         arxiv          |                    [Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art](https://arxiv.org/abs/2403.16527)                     |                      **Hallucination Detection**&**Foundation Models**&**Decision-Making**                     |
| 24.03 |                                                                               Shanghai Jiao Tong University                                                                               |         arxiv          |                            [Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback](https://arxiv.org/abs/2403.18349)                             |                      **Knowledge Feedback**&**Reliable Reward Model**&**Refusal Mechanism**                    |
| 24.03 |                                                                       Universit√§t Hamburg, The University of Sydney                                                                       |         arxiv          |                                  [Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding](https://arxiv.org/abs/2403.18715)                                  |        **Instruction Contrastive Decoding**&**Large Vision-Language Models**&**Hallucination Mitigation**      |
| 24.03 |                   AI Institute University of South Carolina, Indian Institute of Technology Kharagpur, Islamic University of Technology, Stanford University, Amazon AI                   |         arxiv          |                 [‚ÄúSorry Come Again?‚Äù Prompting ‚Äì Enhancing Comprehension and Diminishing Hallucination with [PAUSE] -injected Optimal Paraphrasing](https://arxiv.org/abs/2403.18976)                 |                    **Prompt Engineering**&**Hallucination Mitigation**&**[PAUSE] Injection**                   |
| 24.04 |                                          Beihang University, School of Computer Science and Engineering, School of Software, Shandong University                                          |         arxiv          |                                              [Exploring and Evaluating Hallucinations in LLM-Powered Code Generation](https://arxiv.org/abs/2404.00971)                                               |                                      **Code Generation**&**Hallucination**                                     |
| 24.03 |                                                                          University of Illinois Urbana-Champaign                                                                          |       NAACL2024        |                                         [Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation](https://arxiv.org/abs/2403.14952)                                         |             **Online Misinformation**&**Retrieval Augmented Response**&**Evidence-Based Countering**           |
| 24.03 |                                   Department of Electronic Engineering, Tsinghua University, Pattern Recognition Center, WeChat AI, Tencent Inc, China                                    |       NAACL 2024       |                                                [On Large Language Models‚Äô Hallucination with Regard to Known Facts](https://arxiv.org/abs/2403.20009)                                                 |                                     **Hallucination**&**Inference Dynamics**                                   |
| 24.03 |                                   Department of Electronic Engineering, Tsinghua University, Pattern Recognition Center, WeChat AI, Tencent Inc, China                                    |       NAACL 2024       |                                                [On Large Language Models‚Äô Hallucination with Regard to Known Facts](https://arxiv.org/abs/2403.20009)                                                 |                                     **Hallucination**&**Inference Dynamics**                                   |
| 24.03 |                                                                       Tsinghua University, WeChat AI, Tencent Inc.                                                                        |       NAACL2024        |                                                [On Large Language Models‚Äô Hallucination with Regard to Known Facts](https://arxiv.org/abs/2403.20009)                                                 |                        **Large Language Models**&**Hallucination**&**Inference Dynamics**                      |
| 24.04 |                                                      Technical University of Munich, University of Stavanger, University of Alberta                                                       |         arxiv          |                                   [PoLLMgraph: Unraveling Hallucinations in Large Language Models via State Transition Dynamics](https://arxiv.org/abs/2404.04722)                                    |               **Hallucination Detection**&**State Transition Dynamics**&**Large Language Models**              |
| 24.04 |                                                    University of Edinburgh, University College London, Peking University, Together AI                                                     |         arxiv          |                                [The Hallucinations Leaderboard ‚Äì An Open Effort to Measure Hallucinations in Large Language Models](https://arxiv.org/abs/2404.05904)                                 |                                   **Hallucination Detection**&**Benchmarking**                                 |
| 24.04 |                                                IIIT Hyderabad, Purdue University, Northwestern University, Indiana University Indianapolis                                                |         arxiv          |                    [Halu-NLP at SemEval-2024 Task 6: MetaCheckGPT - A Multi-task Hallucination Detection Using LLM Uncertainty and Meta-models](https://arxiv.org/abs/2404.06948)                     |                         **Hallucination Detection**&**LLM Uncertainty**&**Meta-models**                        |
| 24.04 |                                                                Technion ‚Äì Israel Institute of Technology, Google Research                                                                 |         arxiv          |                                          [Constructing Benchmarks and Interventions for Combating Hallucinations in LLMs](https://arxiv.org/abs/2404.09971)                                           |                                        **Hallucinations**&**Benchmarks**                                       |
| 24.04 |                                                                 The University of Texas at Austin, Salesforce AI Research                                                                 |         arxiv          |                                                 [MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents](https://arxiv.org/abs/2404.10774)                                                 |                                         **Fact-Checking**&**Efficiency**                                       |
| 24.04 |                                                                           Meta, Technical University of Munich                                                                            |         arxiv          |                                          [Uncertainty-Based Abstention in LLMs Improves Safety and Reduces Hallucinations](https://arxiv.org/abs/2404.10960)                                          |                                  **Safety**&**Hallucinations**&**Uncertainty**                                 |
| 24.04 |                                                                   Zhejiang University, Alibaba Group,  Fudan University                                                                   |         arxiv          |                                [Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback](https://arxiv.org/abs/2404.14233)                                |  **Large Vision Language Model**&**Hallucination Detection And Mitigating**&**Direct Preference Optimization** |
| 24.04 |                                                                            Cheriton School of Computer Science                                                                            |         arxiv          |                                                         [Rumour Evaluation with Very Large Language Models](https://arxiv.org/abs/2404.16859)                                                         |                             **Misinformation in Social Networks**&**Explainable AI**                           |
| 24.04 |                                                                            University of California, Berkeley                                                                             |       NAACL 2024       |                                                   [ALOHa: A New Measure for Hallucination in Captioning Models](https://davidmchan.github.io/aloha)                                                   |                                   **Adversarial Attack**&**AI-Text Detection**                                 |
| 24.04 |                                                                                        ServiceNow                                                                                         |       NAACL 2024       |                                          [Reducing hallucination in structured outputs via Retrieval-Augmented Generation](https://arxiv.org/abs/2404.08189)                                          |                   **Retrieval-Augmented Generation**&**Structured Outputs**&**Generative AI**                  |
| 24.04 |                                                                                    Stanford University                                                                                    |       NAACL2024        |                               [NLP Systems That Can‚Äôt Tell Use from Mention Censor Counterspeech, but Teaching the Distinction Helps](https://arxiv.org/abs/2404.01651)                               |                           **Counterspeech**&**Censorship**&**Use-Mention Distinction**                         |
| 24.04 |                                                                  Department of Computing Science, University of Aberdeen                                                                  |       NAACL2024        |                                  [Improving Factual Accuracy of Neural Table-to-Text Output by Addressing Input Problems in ToTTo](https://arxiv.org/abs/2404.04103)                                  |                         **Neural Table-to-Text**&**Factual Accuracy**&**Input Problems**                       |
| 24.04 |                                                                                 Seoul National University                                                                                 |  NAACL2024(findings)   |                                 [Mitigating Hallucination in Abstractive Summarization with Domain-Conditional Mutual Information](https://arxiv.org/abs/2404.09480)                                  |            **Hallucination**&**Abstractive Summarization**&**Domain-Conditional Mutual Information**           |
| 24.05 |      The University of Tokyo, University of California Santa Barbara, Mila - Qu√©bec AI Institute,  Universit√© de Montr√©al, Speech Lab, Alibaba Group,  Hong Kong Baptist University       |         arxiv          |                                           [CodeHalu: Code Hallucinations in LLMs Driven by Execution-based Verification](https://arxiv.org/abs/2405.00253)                                            |                             **Code Hallucination**&**Execution-based Verification**                            |
| 24.05 |                                                                Department of Computer Science, The University of Sheffield                                                                |         arxiv          |                                    [Addressing Topic Granularity and Hallucination in Large Language Models for Topic Modelling](https://arxiv.org/abs/2405.00611)                                    |                           **Topic Modelling**&**Hallucination**&**Topic Granularity**                          |
| 24.04 |                                                                        School of Computing and Information Systems                                                                        |      COLING 2024       |                             [Reinforcement Retrieval Leveraging Fine-grained Feedback for Fact Checking News Claims with Black-Box LLM](https://arxiv.org/abs/2404.17283)                             |                   **Claim Verification**&**Reinforcement Retrieval**&**Fine-Grained Feedback**                 |
| 24.05 |                                                                                         DeepMind                                                                                          |         arxiv          |                                                      [Mitigating LLM Hallucinations via Conformal Abstention](https://arxiv.org/abs/2405.01563)                                                       |                              **Conformal Prediction**&**Hallucination Mitigation**                             |
| 24.05 |                                                                        MBZUAI, Monash University, Sofia University                                                                        |         arxiv          |                                               [OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs](https://arxiv.org/abs/2405.05583)                                                |                              **Factuality Evaluation**&**Automatic Fact-Checking**                             |
| 24.05 |                                                                           Indian Institute of Technology Patna                                                                            |         arxiv          |                                [Unveiling Hallucination in Text, Image, Video, and Audio Foundation Models: A Comprehensive Review](https://arxiv.org/abs/2405.09589)                                 |                           **Hallucination Detection**&**Multimodal Models**&**Review**                         |
| 24.05 |                                                                                  Dublin City University                                                                                   |         arxiv          |                                          [Tell Me Why: Explainable Public Health Fact-Checking with Large Language Models](https://arxiv.org/abs/2405.09454)                                          |                              **Explainable AI**&**Fact-Checking**&**Public Health**                            |
| 24.05 |                                                             University of Information Technology, Vietnam National University                                                             |         arxiv          |                                          [ViWikiFC: Fact-Checking for Vietnamese Wikipedia-Based Textual Knowledge Source](https://arxiv.org/abs/2405.07615)                                          |                            **Fact Checking**&**Information Verification**&**Corpus**                           |
| 24.05 |                                                                                  Imperial College London                                                                                  |         arxiv          |                                [Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval](https://arxiv.org/abs/2405.06545)                                |                            **Hallucination Mitigation**&**Knowledge Graph Retrieval**                          |
| 24.05 |                                                                  Paul G. Allen School of Computer Science & Engineering                                                                   |         arxiv          |                             [MASSIVE Multilingual Abstract Meaning Representation: A Dataset and Baselines for Hallucination Detection](https://arxiv.org/abs/2405.19285)                             |                           **Hallucination Detection**&**Multilingual AMR**&**Dataset**                         |
| 24.05 |                                                                                   Microsoft Corporation                                                                                   |         arxiv          |                                                    [Unlearning Climate Misinformation in Large Language Models](https://arxiv.org/abs/2405.19563)                                                     |                            **Climate Misinformation**&**Unlearning**&**Fine-Tuning**                           |
| 24.05 |                                                                                     Baylor University                                                                                     |         arxiv          |                                     [Detecting Hallucinations in Large Language Model Generation: A Token Probability Approach](https://arxiv.org/abs/2405.19648)                                     |                           **Hallucinations Detection**&**Token Probability Approach**                          |
| 24.05 |                                                                                  Shanghai AI Laboratory                                                                                   |         arxiv          |                                              [ANAH: Analytical Annotation of Hallucinations in Large Language Models](https://arxiv.org/abs/2405.20315)                                               |                                   **Hallucinations**&**Analytical Annotation**                                 |
| 24.06 |                                                                                  University of Waterloo                                                                                   |         arxiv          |                                                 [TruthEval: A Dataset to Evaluate LLM Truthfulness and Reliability](https://arxiv.org/abs/2406.01855)                                                 |                                         **Truthfulness**&**Reliability**                                       |
| 24.06 |                                                                                     Peking University                                                                                     |         arxiv          |                                     [Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework](https://arxiv.org/abs/2406.03075)                                      |                       **Hallucination Detection**&**Markov Chain**&**Multi-agent Debate**                      |
| 24.06 |                                                                                  Northeastern University                                                                                  |        ACL 2024        |                                  [Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends](https://arxiv.org/abs/2406.03487)                                  |                 **Dialogue Summarization**&**Circumstantial Hallucination**&**Error Detection**                |
| 24.06 |                                                                                     McGill University                                                                                     |        ACL 2024        |                                            [Confabulation: The Surprising Value of Large Language Model Hallucinations](https://arxiv.org/abs/2406.04175)                                             |                               **Confabulation**&**Hallucinations**&**Narrativity**                             |
| 24.06 |                                                                                  University of Michigan                                                                                   |         arxiv          |                                    [3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination](https://arxiv.org/abs/2406.05132)                                     |                                   **3D-LLMs**&**Grounding**&**Hallucination**                                  |
| 24.06 |                                                                                 Arizona State University                                                                                  |         arxiv          |                                          [Investigating and Addressing Hallucinations of LLMs in Tasks Involving Negation](https://arxiv.org/abs/2406.05494)                                          |                                         **Hallucinations**&**Negation**                                        |
| 24.06 |                                                                                    Tsinghua University                                                                                    |         arxiv          |                                      [Benchmarking Trustworthiness of Multimodal Large Language Models: A Comprehensive Study](https://arxiv.org/abs/2406.07057)                                      |                                   **Trustworthiness**&**MLLMs**&**Benchmark**                                  |
| 24.06 |                                                                        Beijing Academy of Artificial Intelligence                                                                         |         arxiv          |                                     [HalluDial: A Large-Scale Benchmark for Automatic Dialogue-Level Hallucination Evaluation](https://arxiv.org/abs/2406.07070)                                      |                          **Hallucination Evaluation**&**Dialogue-Level**&**HalluDial**                         |
| 24.06 |                                                                                           KFUPM                                                                                           |         arxiv          |                                                [DefAn: Definitive Answer Dataset for LLMs Hallucination Evaluation](https://arxiv.org/abs/2406.09155)                                                 |                               **Hallucination Evaluation**&**Definitive Answers**                              |
| 24.06 |                                                                              Harbin Institute of Technology                                                                               |   ACL 2024 findings    |                               [Paying More Attention to Source Context: Mitigating Unfaithful Translations from Large Language Model](https://arxiv.org/abs/2406.07036)                               |                                  **Unfaithful Translations**&**Source Context**                                |
| 24.06 |                                                                                National Taiwan University                                                                                 |    Interspeech 2024    |                         [Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models](https://arxiv.org/abs/2406.08402)                         |              **Large audio-language models**&**Object hallucination**&**Discriminative questions**             |
| 24.06 |                                                                            University of Texas at San Antonio                                                                             |         arxiv          |                               [We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs](https://arxiv.org/abs/2406.10279)                               |              **Package Hallucinations**&**Code Generating LLMs**&**Software Supply Chain Security**            |
| 24.06 |                                                                               The University of Manchester                                                                                |         arxiv          |               [RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation Detection Using In-Context Learning based on Emotional Information](https://arxiv.org/abs/2406.11093)               |                 **RAEmoLLM**&**Cross-Domain Misinformation Detection**&**Affective Information**               |
| 24.06 |                                                                                           KAIST                                                                                           |         arxiv          |                                      [Adversarial Style Augmentation via Large Language Model for Robust Fake News Detection](https://arxiv.org/abs/2406.11260)                                       |                            **Adversarial Style Augmentation**&**Fake News Detection**                          |
| 24.06 |                                                                            The Chinese University of Hong Kong                                                                            |         arxiv          |                                              [Mitigating Large Language Model Hallucination with Faithful Finetuning](https://arxiv.org/abs/2406.11267)                                               |                                    **Hallucination**&**Faithful Finetuning**                                   |
| 24.06 |                                                           Gaoling School of Artificial Intelligence, Renmin University of China                                                           |         arxiv          |                                       [Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector](https://arxiv.org/abs/2406.11277)                                       |                       **Hallucination Detection**&**Small Language Models**&**HaluAgent**                      |
| 24.06 |                                                                       University of Science and Technology of China                                                                       |         arxiv          |                                    [CrAM: Credibility-Aware Attention Modification in LLMs for Combating Misinformation in RAG](https://arxiv.org/abs/2406.11497)                                     |                   **CrAM**&**Credibility-Aware Attention**&**Retrieval-Augmented Generation**                  |
| 24.06 |                                                                                  University of Rochester                                                                                  |         arxiv          |                                       [Do More Details Always Introduce More Hallucinations in LVLM-based Image Captioning?](https://arxiv.org/abs/2406.12663)                                        |                             **LVLMs**&**Image Captioning**&**Object Hallucination**                            |
| 24.06 |                                                                                 Xi'an Jiaotong University                                                                                 |         arxiv          |                        [AGLA: Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention](https://arxiv.org/abs/2406.12718)                         |                       **AGLA**&**Object Hallucinations**&**Large Vision-Language Models**                      |
| 24.06 |                                                                     University of Groningen, University of Amsterdam                                                                      |         arxiv          |                                      [Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation](https://arxiv.org/abs/2406.13663)                                      |                              **Retrieval-Augmented Generation**&**Trustworthy AI**                             |
| 24.06 |                                                                                 Seoul National University                                                                                 |         arxiv          |                                   [Large Language Models are Skeptics: False Negative Problem of Input-conflicting Hallucination](https://arxiv.org/abs/2406.13929)                                   |                     **False Negative Problem**&**Input-conflicting Hallucination**&**Bias**                    |
| 24.06 |                                                                                   University of Houston                                                                                   |         arxiv          |                                       [Seeing Through AI‚Äôs Lens: Enhancing Human Skepticism Towards LLM-Generated Fake News](https://arxiv.org/abs/2406.14012)                                        |                                       **Fake news**&**LLM-generated news**                                     |
| 24.06 |                                                                                   University of Oxford                                                                                    |         arxiv          |                                             [Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs](https://arxiv.org/abs/2406.15927)                                             |                           **Hallucination Detection**&**Semantic Entropy**&**Probes**                          |
| 24.06 |                                                                                       UC San Diego                                                                                        |         arxiv          |                                                     [Mitigating Hallucination in Fictional Character Role-Play](https://arxiv.org/abs/2406.17260)                                                     |                       **Hallucination Mitigation**&**Role-Play**&**Fictional Characters**                      |
| 24.06 |                                                                                          Lamini                                                                                           |         arxiv          |                                                  [Banishing LLM Hallucinations Requires Rethinking Generalization](https://arxiv.org/abs/2406.17642)                                                  |                             **Hallucinations**&**Generalization**&**Memory Experts**                           |
| 24.06 |                                                                                     Waseda University                                                                                     |         arxiv          |                              [ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models](https://arxiv.org/abs/2406.20015)                              |          **Tool-Augmented Large Language Models**&**Hallucination Diagnostic Benchmark**&**Tool Usage**        |
| 24.07 |                                                                                    Beihang University                                                                                     |         arxiv          |                              [PFME: A Modular Approach for Fine-grained Hallucination Detection and Editing of Large Language Models](https://arxiv.org/abs/2407.00488)                               |                                  **Hallucination Detection**&**Model Editing**                                 |
| 24.07 |                                                                                    Tsinghua University                                                                                    |         arxiv          |                                          [Fake News Detection and Manipulation Reasoning via Large Vision-Language Models](https://arxiv.org/abs/2407.02042)                                          |               **Large Vision-Language Models**&**Fake News Detection**&**Manipulation Reasoning**              |
| 24.07 |                                                                               Brno University of Technology                                                                               |         arxiv          |                                               [Generative Large Language Models in Automated Fact-Checking: A Survey](https://arxiv.org/abs/2407.02351)                                               |                                      **Automated Fact-Checking**&**Survey**                                    |
| 24.07 |                                                                                     SRI International                                                                                     |         arxiv          |                           [Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification](https://arxiv.org/abs/2407.02352)                            |                        **Vision-LLMs**&**Hallucination Detection**&**Claim Verification**                      |
| 24.07 |                                                                      Hong Kong University of Science and Technology                                                                       |         arxiv          |                                                 [LLM Internal States Reveal Hallucination Risk Faced With a Query](https://arxiv.org/abs/2407.03282)                                                  |                              **Hallucination Detection**&**Uncertainty Estimation**                            |
| 24.07 |                                                                              Harbin Institute of Technology                                                                               | ICLR 2024 AGI Workshop |                               [Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models](https://arxiv.org/abs/2407.00569)                               |               **Vision-Language Models**&**Multimodal Hallucination**&**Residual Visual Decoding**             |
| 24.07 |                                                                              Harbin Institute of Technology                                                                               |        ACL 2024        |                               [Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models](https://arxiv.org/abs/2407.00569)                               |                       **Multimodal Hallucinations**&**LVLMs**&**Residual Visual Decoding**                     |
| 24.07 |                                                                                  University of Amsterdam                                                                                  |         arxiv          |                                           [Leveraging Graph Structures to Detect Hallucinations in Large Language Models](https://arxiv.org/abs/2407.04485)                                           |                **Hallucination Detection**&**Graph Attention Network**&**Large Language Models**               |
| 24.07 |                                                                                      Cisco Research                                                                                       |         arxiv          |                                                                        [Code Hallucination](https://arxiv.org/abs/2407.04831)                                                                         |                           **Code Hallucination**&**Generative Models**&**HallTrigger**                         |
| 24.07 |                                                                                Beijing Jiaotong University                                                                                |         arxiv          |                              [KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge Graph-based False Premise Questions](https://arxiv.org/abs/2407.05868)                               |                   **Factuality Hallucination**&**Knowledge Graph**&**False Premise Questions**                 |
| 24.07 |                                                                          University of California, Santa Barbara                                                                          |         arxiv          |                            [DebUnc: Mitigating Hallucinations in Large Language Model Agent Communication with Uncertainty Estimations](https://arxiv.org/abs/2407.06426)                             |                      **Hallucinations**&**Uncertainty Estimations**&**Multi-agent Systems**                    |
| 24.07 |                                                                           Massachusetts Institute of Technology                                                                           |         arxiv          |                       [Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps](https://arxiv.org/abs/2407.07071)                        |                                 **Contextual Hallucinations**&**Attention Maps**                               |
| 24.07 |                                                                          University of Illinois Urbana-Champaign                                                                          |         arxiv          |                                         [Knowledge Overshadowing Causes Amalgamated Hallucination in Large Language Models](https://arxiv.org/abs/2407.08039)                                         |                                  **Knowledge Overshadowing**&**Hallucination**                                 |
| 24.07 |                                                                                        Patronus AI                                                                                        |         arxiv          |                                                        [Lynx: An Open Source Hallucination Evaluation Model](https://arxiv.org/abs/2407.08488)                                                        |                             **Hallucination Detection**&**RAG**&**Evaluation Model**                           |
| 24.07 |                                                                               Shanghai Jiao Tong University                                                                               |         arxiv          |                                                       [On the Universal Truthfulness Hyperplane Inside LLMs](https://arxiv.org/abs/2407.08582)                                                        |                                  **Truthfulness Hyperplane**&**Hallucination**                                 |
| 24.07 |                                                                                  University of Michigan                                                                                   |     ACL 2024 ALVR      |                                                 [Multi-Object Hallucination in Vision-Language Models](https://multi-object-hallucination.github.io/)                                                 |                **Multi-Object Hallucination**&**Vision-Language Models**&**Evaluation Protocol**               |
| 24.07 |                                                                                        ASAPP, Inc.                                                                                        |   ACL 2024 Findings    |                            [Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses](https://arxiv.org/abs/2407.05474)                             |                       **Hallucination Detection**&**Synthetic Data**&**System Responses**                      |
| 24.07 |                                                                                          FAR AI                                                                                           |       COLM 2024        |                                                      [Transformer Circuit Faithfulness Metrics Are Not Robust](https://arxiv.org/abs/2407.08734)                                                      |                      **Transformer Circuits**&**Ablation Studies**&**Faithfulness Metrics**                    |
| 24.07 |                                                                       University of Science and Technology of China                                                                       |         arxiv          |                              [Detect, Investigate, Judge and Determine: A Novel LLM-based Framework for Few-shot Fake News Detection](https://arxiv.org/abs/2407.08952)                               |                                             **Fake News Detection**                                            |
| 24.07 |                                                                                    Tsinghua University                                                                                    |         arxiv          |                                                  [Mitigating Entity-Level Hallucination in Large Language Models](https://arxiv.org/abs/2407.09417)                                                   |                              **Hallucination**&**Retrieval Augmented Generation**&                             |
| 24.07 |                                                                                    Amazon Web Services                                                                                    |         arxiv          |                                                   [On Mitigating Code LLM Hallucinations with API Documentation](https://arxiv.org/abs/2407.09726)                                                    |                   **API Hallucinations**&**Code LLMs**&**Documentation Augmented Generation**                  |
| 24.07 |                                                                             Technical University of Darmstadt                                                                             |         arxiv          |                                        [Fine-grained Hallucination Detection and Mitigation in Long-form Question Answering](https://arxiv.org/abs/2407.11930)                                        |                         **Hallucination Detection**&**Error Annotation**&**Factuality**                        |
| 24.07 |                                                                                   Heidelberg University                                                                                   |         arxiv          |                                                       [Truth is Universal: Robust Detection of Lies in LLMs](https://arxiv.org/abs/2407.12831)                                                        |                           **Lie Detection**&**Activation Vectors**&**Truth Direction**                         |
| 24.07 |                                                                               Shanghai Jiao Tong University                                                                               |         arxiv          |                                                            [HALU-J: Critique-Based Hallucination Judge](https://arxiv.org/abs/2407.12943)                                                             |              **Hallucination Detection**&**Critique-Based Evaluation**&**Evidence Categorization**             |
| 24.07 |                                                                         TH K√∂ln ‚Äì University of Applied Sciences                                                                          |       CLEF 2024        |                                [The Two Sides of the Coin: Hallucination Generation and Detection with LLMs as Evaluators for LLMs](https://arxiv.org/abs/2407.13757)                                 |                 **Hallucination Generation**&**Hallucination Detection**&**Multilingual Models**               |
| 24.07 |                                                                                          POSTECH                                                                                          |       ECCV 2024        |                                     [BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models](https://arxiv.org/abs/2407.13442)                                      |                                   **Hallucination**&**Vision-Language Models**                                 |
| 24.07 |                                                                                 University College London                                                                                 |         arxiv          |                            [Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models](https://arxiv.org/abs/2407.16470)                            |                               **Machine Translation**&**Hallucination Detection**                              |
| 24.07 |                                                                                    Cornell University                                                                                     |         arxiv          |                                    [WILDHALLUCINATIONS: Evaluating Long-form Factuality in LLMs with Real-World Entity Queries](https://arxiv.org/abs/2407.17468)                                     |                     **WildHallucinations**&**Factuality Evaluation**&**Real-World Entities**                   |
| 24.07 |                                                                                    Columbia University                                                                                    |       ECCV 2024        |                                           [HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning](https://arxiv.org/abs/2407.15680)                                            |                            **Hallucination**&**Vision-Language Models**&**Datasets**                           |
| 24.07 |                                                                                       IBM Research                                                                                        |   ICML 2024 Workshop   |                                                     [Generation Constraint Scaling Can Mitigate Hallucination](https://arxiv.org/abs/2407.16908)                                                      |                                  **Hallucination**&**Memory-Augmented Models**                                 |
| 24.07 |                                                                                        Harvard-MIT                                                                                        |         arxiv          | [The Need for Guardrails with Large Language Models in Medical Safety-Critical Settings: An Artificial Intelligence Application in the Pharmacovigilance Ecosystem](https://arxiv.org/abs/2407.18322) |                               **Pharmacovigilance**&**Drug Safety**&**Guardrails**                             |
| 24.07 |                                                                             Illinois Institute of Technology                                                                              |         arxiv          |                                                                   [Can Editing LLMs Inject Harm?](https://arxiv.org/abs/2407.20224)                                                                   |                      **Knowledge Editing**&**Misinformation Injection**&**Bias Injection**                     |
| 24.07 |                                                                                    Stanford University                                                                                    |         arxiv          |                                     [Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models](https://arxiv.org/abs/2407.21417)                                      |                        **Instruction Following**&**Faithfulness**&**Multi-task Learning**                      |
| 24.07 |                                                                                     Jilin University                                                                                      |      ACM MM 2024       |                                            [Harmfully Manipulated Images Matter in Multimodal Misinformation Detection](https://arxiv.org/abs/2407.19192)                                             |                                  **Social media**&**Misinformation detection**                                 |
| 24.07 |                                                                                    Zhejiang University                                                                                    |      COLING 2024       |                           [Improving Faithfulness of Large Language Models in Summarization via Sliding Generation and Self-Consistency](https://arxiv.org/abs/2407.21443)                            |                                        **Summarization**&**Faithfulness**                                      |
| 24.08 |                                                                       Huazhong University of Science and Technology                                                                       |         arxiv          |                                               [Mitigating Multilingual Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2408.00550)                                               |            **Large Vision-Language Models**&**Multilingual Hallucination**&**Supervised Fine-tuning**          |
| 24.08 |                                                                       Huazhong University of Science and Technology                                                                       |         arxiv          |                                   [Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation](https://arxiv.org/abs/2408.00555)                                    |              **Hallucination**&**Vision-Language Models (VLMs)**&**Active Retrieval Augmentation**             |
| 24.08 |                                                                                           DFKI                                                                                            | UbiComp Companion '24  |                                                 [Misinforming LLMs: Vulnerabilities, Challenges and Opportunities](https://arxiv.org/abs/2408.01168)                                                  |                                      **Misinformation**&**Trustworthy AI**                                     |
| 24.08 |                                                                                    Bar Ilan University                                                                                    |         arxiv          |                             [Mitigating Hallucinations in Large Vision-Language Models (LVLMs) via Language-Contrastive Decoding (LCD)](https://arxiv.org/abs/2408.04664)                             |           **Large Vision-Language Models**&**Object Hallucinations**&**Language-Contrastive Decoding**         |
| 24.08 |                                                                                  University of Liverpool                                                                                  |         arxiv          |                          [Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models](https://arxiv.org/abs/2408.05093)                           |                          **Hallucination**&**Reasoning Order**&**Reflexive Prompting**                         |
| 24.08 |                                                                                 The Alan Turing Institute                                                                                 |         arxiv          |                            [Large Language Models Can Consistently Generate High-Quality Content for Election Disinformation Operations](https://arxiv.org/abs/2408.06731)                            |                                 **Election Disinformation**&**DisElect Dataset**                               |
| 24.08 |                                                                                      Google DeepMind                                                                                      |       COLM 2024        |                                [Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability](https://arxiv.org/abs/2408.07852)                                |                                      **Knowledge Graph**&**Hallucinations**                                    |
| 24.08 |                                                                           The Hong Kong Polytechnic University                                                                            |         arxiv          |                                         [MegaFake: A Theory-Driven Dataset of Fake News Generated by Large Language Models](https://arxiv.org/abs/2408.11871)                                         |                                        **Fake News**&**MegaFake Dataset**                                      |
| 24.08 |                                                                                       IIT Kharagpur                                                                                       |         arxiv          |                                        [Evidence-backed Fact Checking using RAG and Few-Shot In-Context Learning with LLMs](https://arxiv.org/abs/2408.12060)                                         |                                **Fact Checking**&**RAG**&**In-Context Learning**                               |
| 24.08 |                                                                                     Fudan University                                                                                      |         arxiv          |                              [Improving Factuality in Large Language Models via Decoding-Time Hallucinatory and Truthful Comparators](https://arxiv.org/abs/2408.12325)                               |              **Factuality Improvement**&**Hallucination Mitigation**&**Decoding-Time Intervention**            |
| 24.08 |                                                                                  The University of Tokyo                                                                                  |         arxiv          |                                     [Interactive DualChecker for Mitigating Hallucinations in Distilling Large Language Models](https://arxiv.org/abs/2408.12326)                                     |                **Hallucination Mitigation**&**Knowledge Distillation**&**Large Language Models**               |
| 24.08 |                                                                                   University of Surrey                                                                                    |       IJCAI 2024       |                                               [CodeMirage: Hallucinations in Code Generated by Large Language Models](https://arxiv.org/abs/2408.08333)                                               |                                  **Code Hallucinations**&**CodeMirage Dataset**                                |
| 24.08 | Sichuan Normal University | arxiv | [Can LLM Be a Good Path Planner Based on Prompt Engineering? Mitigating the Hallucination for Path Planning](https://arxiv.org/abs/2408.13184) | **Path Planning**&**Spatial Reasoning**&**Hallucination Mitigation** |
| 24.08 | Alibaba Cloud | arxiv | [LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via Layer-wise Relevance Propagation](https://arxiv.org/abs/2408.15533) | **Hallucination Detection**&**RAG**&**Layer-wise Relevance Propagation** |
| 24.08 | Royal Holloway, University of London | arxiv | [Logic-Enhanced Language Model Agents for Trustworthy Social Simulations](https://arxiv.org/abs/2408.16081) | **Social Simulations**&**Trustworthy AI**&**Game Theory** |



## üíªPresentations & Talks



## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

## üßë‚Äçüè´Scholars