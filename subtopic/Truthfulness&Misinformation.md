# Truthfulness&Misinformation

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"

## üìëPapers

| Date  |                                                                      Institute                                                                       |  Publication   |                                                                                   Paper                                                                                   |                                     Keywords                                      |
|:-----:|:----------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------:|
| 21.09 |                                                                 University of Oxford                                                                 |    ACL2022     |                                        [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)                                        |                          **Benchmark**&**Truthfulness**                           |
| 23.07 | Microsoft Research Asia, Hong Kong University of Science and Technology, University of Science and Technology of China, Tsinghua University, Sony AI | ResearchSquare |                           [Defending ChatGPT against Jailbreak Attack via Self-Reminder](https://www.researchsquare.com/article/rs-2873090/v1)                            |              **Jailbreak Attack**&**Self-Reminder**&**AI Security**               |
| 23.10 |                                                                 University of Zurich                                                                 |     arxiv      |                                 [Lost in Translation -- Multilingual Misinformation and its Evolution](https://arxiv.org/abs/2310.18089)                                  |                        **Misinformation**&**Multilingual**                        |
| 23.10 |                                                           New York University&Javier Rando                                                           |     arxiv      |                                      [Personas as a Way to Model Truthfulness in Language Models](https://arxiv.org/abs/2310.18168)                                       |                       **Truthfulness**&**Truthful Persona**                       |
| 23.11 |                                                                  Dialpad Canada Inc                                                                  |     arxiv      |                 [Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs](https://arxiv.org/abs/2311.00681)                  |                             **Factuality Assessment**                             |
| 23.11 |                                                             The University of Manchester                                                             |     arxiv      |                                            [Emotion Detection for Misinformation: A Review](https://arxiv.org/abs/2311.00671)                                             |                    **Survey**&**Misinformation**&**Emotions**                     |
| 23.11 |                                                                University of Virginia                                                                |     arxiv      |                    [Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder with Semantics](https://arxiv.org/abs/2311.01386)                    |                              **Language Illusions**                               |
| 23.11 |                                                       University of Illinois Urbana-Champaign                                                        |     arxiv      | [Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism](https://arxiv.org/abs/2311.01041) |                     **Hallucinations**&**Refusal Mechanism**                      |
| 23.11 |                                                           University of Washington Bothell                                                           |     arxiv      |                                [Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI](https://arxiv.org/abs/2311.01463)                                |               **Healthcare**&**Trustworthiness**&**Hallucinations**               |
| 23.11 |                                                                  Intuit AI Research                                                                  |   EMNLP2023    |            [SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency](https://arxiv.org/abs/2311.01740)             |                  **Hallucination Detection**&**Trustworthiness**                  |
| 23.11 |                                                            Shanghai Jiao Tong University                                                             |     arxiv      |                 [Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation](https://arxiv.org/abs/2311.01766)                  |             **Misinformation**&**Disinformation**&**Out-of-Context**              |
| 23.11 |                                                             Hamad Bin Khalifa University                                                             |     arxiv      |                        [ArAIEval Shared Task: Persuasion Techniques and Disinformation Detection in Arabic Text](https://arxiv.org/abs/2311.03179)                        |                        **Disinformation**&**Arabic Text**                         |
| 23.11 |                                                                   UNC-Chapel Hill                                                                    |     arxiv      |                         [Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges](https://arxiv.org/abs/2311.03287)                         |                  **Hallucination**&**Benchmark**&**Multimodal**                   |
| 23.11 |                                                                  Cornell University                                                                  |     arxiv      |                                   [Adapting Fake News Detection to the Era of Large Language Models](https://arxiv.org/abs/2311.04917)                                    |           **Fake news detection**&**Generated News**&**Misinformation**           |
| 23.11 |                                                            Harbin Institute of Technology                                                            |     arxiv      |               [A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/abs/2311.05232)                |           **Hallucination**&**Factual Consistency**&**Trustworthiness**           |
| 23.11 |                                                      Korea University, KAIST AI,LG AI Research                                                       |     arXiv      |                          [VOLCANO: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision](https://arxiv.org/abs/2311.07362)                           |             **Multimodal Models**&**Hallucination**&**Self-Feedback**             |
| 23.11 |                                              Beijing Jiaotong University, Alibaba Group, Peng Cheng Lab                                              |     arXiv      |                           [AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation](https://arxiv.org/abs/2311.07397)                           |             Multi-modal Large Language Models&Hallucination&Benchmark             |
| 23.11 |                                            LMU Munich; Munich Center of Machine Learning; Google Research                                            |     arXiv      |                                        [Hallucination Augmented Recitations for Language Models](https://arxiv.org/abs/2311.07424)                                        |                   **Hallucination**&**Counterfactual Datasets**                   |
| 23.11 |                                                         Stanford University, UNC Chapel Hill                                                         |     arxiv      |                                              [Fine-tuning Language Models for Factuality](https://arxiv.org/abs/2311.08401)                                               | **Factuality**&**Reference-Free Truthfulness**&**Direct Preference Optimization** |
| 23.11 |                                                      Corporate Data and Analytics Office (CDAO)                                                      |     arxiv      |                            [Hallucination-minimized Data-to-answer Framework for Financial Decision-makers](https://arxiv.org/abs/2311.07592)                             |           **Financial Decision Making**&**Hallucination Minimization**            |
| 23.11 |                                                               Arizona State University                                                               |     arxiv      |                                    [Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey](https://arxiv.org/abs/2311.07914)                                     |                **Knowledge Graphs**&**Hallucinations**&**Survey**                 |
| 23.11 |                                    Kempelen Institute of Intelligent Technologies; Brno University of Technology                                     |     arxiv      |                                         [Disinformation Capabilities of Large Language Models](https://arxiv.org/abs/2311.08838)                                          |     **Disinformation Generation**&**Safety Filters**&**Automated Evaluation**     |
| 23.11 |                                                      UNC-Chapel Hill, University of Washington                                                       |     arxiv      |               [EVER: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification](https://arxiv.org/abs/2311.09114)                |          **Hallucination**&**Real-Time Verification**&**Rectification**           |
| 23.11 |                                                      Peking University, WeChat AI, Tencent Inc.                                                      |     arXiv      |                           [RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge](https://arxiv.org/abs/2311.08147)                           |       **External Counterfactual Knowledge**&**Benchmarking**&**Robustness**       |
| 23.11 |                                                                    PolyAI Limited                                                                    |     arXiv      |            [Dial BEINFO for Faithfulness: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning](https://arxiv.org/abs/2311.09800)             |           **Factuality**&**Behavioural Fine-Tuning**&**Hallucination**            |
| 23.11 |                             The Hong Kong University of Science and Technology, University of Illinois Urbana-Champaign                              |     arxiv      |                                 [R-Tuning: Teaching Large Language Models to Refuse Unknown Questions](https://arxiv.org/abs/2311.09677)                                  |     **Hallucination**&**Refusal-Aware Instruction Tuning**&**Knowledge Gap**      |
| 23.11 |                            University of Southern California, University of Pennsylvania, University of California Davis                             |     arxiv      |                    [Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?](https://arxiv.org/abs/2311.09702)                     |            **Hallucinations**&**Semantic Associations**&**Benchmark**             |
| 23.11 |                                              The Ohio State University, University of California Davis                                               |     arxiv      |            [How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities](https://arxiv.org/abs/2311.09447)             |     **Trustworthiness**&**Malicious Demonstrations**&**Adversarial Attacks**      |
| 23.11 |                                                               University of Sheffield                                                                |     arXiv      |         [Lighter yet More Faithful: Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization](https://arxiv.org/abs/2311.09335)         |                **Hallucinations**&&**Language Model Reliability**                 |
| 23.11 |                     Institute of Information Engineering Chinese Academy of Sciences, University of Chinese Academy of Sciences                      |     arxiv      |             [Can Large Language Models Understand Content and Propagation for Misinformation Detection: An Empirical Study](https://arxiv.org/abs/2311.12699)             |                           **Misinformation Detection**                            |
| 23.11 |                     Shanghai Jiaotong University, Amazon AWS AI, Westlake University, IGSNRR Chinese Academy of Sciences, China                      |     arXiv      |                                [Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus](https://arxiv.org/abs/2311.13230)                                | **Hallucination Detection**&**Uncertainty-Based Methods**&**Factuality Checking** |
| 23.11 |                             Institute of Software Chinese Academy of Sciences, University of Chinese Academy of Sciences                             |     arXiv      |                   [Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting](https://arxiv.org/abs/2311.13314)                    |             **Hallucinations**&**Knowledge Graphs**&**Retrofitting**              |
| 23.11 |                                                              Applied Research Quantiphi                                                              |     arxiv      |                              [Minimizing Factual Inconsistency and Hallucination in Large Language Models](https://arxiv.org/abs/2311.13878)                              |                    **Factual Inconsistency**&**Hallucination**                    |
| 23.11 |                                       Microsoft Research, Georgia Tech                                        |             arxiv             |                                    [Calibrated Language Models Must Hallucinate](https://arxiv.org/abs/2311.14648)                                    |                         **Hallucination&Calibration**&**Statistical Analysis**                         |


## üíªPresentations & Talks



## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

## üßë‚Äçüè´Scholars