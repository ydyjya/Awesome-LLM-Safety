# Truthfulness&Misinformation

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"

## üìëPapers

| Date  |                                                                                         Institute                                                                                       |            Publication             |                                                                                                 Paper                                                                                                  |                                                     Keywords                                                     |
|:-----:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------:|
| 21.09 |                                                                                   University of Oxford                                                                                  |              ACL2022               |                                                      [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)                                                       |                                          **Benchmark**&**Truthfulness**                                          |
| 23.05 |                                                                                           KAIST                                                                                         |        NAACL2024(findings)         |                                [Why So Gullible? Enhancing the Robustness of Retrieval-Augmented Models against Counterfactual Noise](https://arxiv.org/abs/2305.01579)                                |            **Retrieval-Augmented Models**&**Counterfactual Noise**&**Open-Domain Question Answering**            |
| 23.07 |                   Microsoft Research Asia, Hong Kong University of Science and Technology, University of Science and Technology of China, Tsinghua University, Sony AI                  |           ResearchSquare           |                                          [Defending ChatGPT against Jailbreak Attack via Self-Reminder](https://www.researchsquare.com/article/rs-2873090/v1)                                          |                              **Jailbreak Attack**&**Self-Reminder**&**AI Security**                              |
| 23.10 |                                                                                   University of Zurich                                                                                  |               arxiv                |                                                [Lost in Translation -- Multilingual Misinformation and its Evolution](https://arxiv.org/abs/2310.18089)                                                |                                       **Misinformation**&**Multilingual**                                        |
| 23.10 |                                                                             New York University&Javier Rando                                                                            |               arxiv                |                                                     [Personas as a Way to Model Truthfulness in Language Models](https://arxiv.org/abs/2310.18168)                                                     |                                      **Truthfulness**&**Truthful Persona**                                       |
| 23.10 |                                                   Tsinghua University, Allen Institute for AI, University of Illinois Urbana-Champaign                                                  |             NAACL2024              |                                                  [Language Models Hallucinate, but May Excel at Fact Verification](https://arxiv.org/abs/2310.14564)                                                   |                                     **Hallucination**&**Fact Verification**                                      |
| 23.10 |                                 Stanford University&University of Maryland&Carnegie Mellon University&NYU Shanghai&New York University&Microsoft Research                               |             NAACL2024              |                                   [Large Language Models Help Humans Verify Truthfulness‚ÄîExcept When They Are Convincingly Wrong](https://arxiv.org/abs/2310.12558)                                    |                                        **Fact-Checking**&**Truthfulness**                                        |
| 23.10 |                                                                                    Shandong University                                                                                  |             NAACL2024              |                                            [Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method](https://arxiv.org/abs/2310.17918)                                             |                                 **Self-Detection**&**Non-Factuality Detection**                                  |
| 23.10 |                                                                                     Fudan University                                                                                    |             CIKM 2023              |                                   [Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models](https://doi.org/10.1145/3583780.3614905)                                    |                                 **Hallucination Detection**&**Reliable Answers**                                 |
| 23.11 |                                                                                    Dialpad Canada Inc                                                                                   |               arxiv                |                                [Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs](https://arxiv.org/abs/2311.00681)                                |                                            **Factuality Assessment**                                             |
| 23.11 |                                                                               The University of Manchester                                                                              |               arxiv                |                                                           [Emotion Detection for Misinformation: A Review](https://arxiv.org/abs/2311.00671)                                                           |                                    **Survey**&**Misinformation**&**Emotions**                                    |
| 23.11 |                                                                                  University of Virginia                                                                                 |               arxiv                |                                  [Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder with Semantics](https://arxiv.org/abs/2311.01386)                                   |                                              **Language Illusions**                                              |
| 23.11 |                                                                          University of Illinois Urbana-Champaign                                                                        |               arxiv                |               [Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism](https://arxiv.org/abs/2311.01041)                |                                     **Hallucinations**&**Refusal Mechanism**                                     |
| 23.11 |                                                                             University of Washington Bothell                                                                            |               arxiv                |                                              [Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI](https://arxiv.org/abs/2311.01463)                                               |                              **Healthcare**&**Trustworthiness**&**Hallucinations**                               |
| 23.11 |                                                                                    Intuit AI Research                                                                                   |             EMNLP2023              |                           [SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency](https://arxiv.org/abs/2311.01740)                           |                                 **Hallucination Detection**&**Trustworthiness**                                  |
| 23.11 |                                                                               Shanghai Jiao Tong University                                                                             |               arxiv                |                                [Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation](https://arxiv.org/abs/2311.01766)                                |                             **Misinformation**&**Disinformation**&**Out-of-Context**                             |
| 23.11 |                                                                               Hamad Bin Khalifa University                                                                              |               arxiv                |                                      [ArAIEval Shared Task: Persuasion Techniques and Disinformation Detection in Arabic Text](https://arxiv.org/abs/2311.03179)                                       |                                        **Disinformation**&**Arabic Text**                                        |
| 23.11 |                                                                                      UNC-Chapel Hill                                                                                    |               arxiv                |                                       [Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges](https://arxiv.org/abs/2311.03287)                                        |                                  **Hallucination**&**Benchmark**&**Multimodal**                                  |
| 23.11 |                                                                                    Cornell University                                                                                   |               arxiv                |                                                  [Adapting Fake News Detection to the Era of Large Language Models](https://arxiv.org/abs/2311.04917)                                                  |                          **Fake news detection**&**Generated News**&**Misinformation**                           |
| 23.11 |                                                                              Harbin Institute of Technology                                                                             |               arxiv                |                              [A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/abs/2311.05232)                              |                          **Hallucination**&**Factual Consistency**&**Trustworthiness**                           |
| 23.11 |                                                                         Korea University, KAIST AI,LG AI Research                                                                       |               arXiv                |                                         [VOLCANO: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision](https://arxiv.org/abs/2311.07362)                                         |                            **Multimodal Models**&**Hallucination**&**Self-Feedback**                             |
| 23.11 |                                                                Beijing Jiaotong University, Alibaba Group, Peng Cheng Lab                                                               |               arXiv                |                                         [AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation](https://arxiv.org/abs/2311.07397)                                          |                            Multi-modal Large Language Models&Hallucination&Benchmark                             |
| 23.11 |                                                              LMU Munich; Munich Center of Machine Learning; Google Research                                                             |               arXiv                |                                                      [Hallucination Augmented Recitations for Language Models](https://arxiv.org/abs/2311.07424)                                                       |                                  **Hallucination**&**Counterfactual Datasets**                                   |
| 23.11 |                                                                           Stanford University, UNC Chapel Hill                                                                          |               arxiv                |                                                             [Fine-tuning Language Models for Factuality](https://arxiv.org/abs/2311.08401)                                                             |                **Factuality**&**Reference-Free Truthfulness**&**Direct Preference Optimization**                 |
| 23.11 |                                                                        Corporate Data and Analytics Office (CDAO)                                                                       |               arxiv                |                                           [Hallucination-minimized Data-to-answer Framework for Financial Decision-makers](https://arxiv.org/abs/2311.07592)                                           |                           **Financial Decision Making**&**Hallucination Minimization**                           |
| 23.11 |                                                                                 Arizona State University                                                                                |               arxiv                |                                                   [Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey](https://arxiv.org/abs/2311.07914)                                                   |                                **Knowledge Graphs**&**Hallucinations**&**Survey**                                |
| 23.11 |                                                       Kempelen Institute of Intelligent Technologies; Brno University of Technology                                                     |               arxiv                |                                                        [Disinformation Capabilities of Large Language Models](https://arxiv.org/abs/2311.08838)                                                        |                    **Disinformation Generation**&**Safety Filters**&**Automated Evaluation**                     |
| 23.11 |                                                                         UNC-Chapel Hill, University of Washington                                                                       |               arxiv                |                              [EVER: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification](https://arxiv.org/abs/2311.09114)                              |                          **Hallucination**&**Real-Time Verification**&**Rectification**                          |
| 23.11 |                                                                        Peking University, WeChat AI, Tencent Inc.                                                                       |               arXiv                |                                         [RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge](https://arxiv.org/abs/2311.08147)                                          |                      **External Counterfactual Knowledge**&**Benchmarking**&**Robustness**                       |
| 23.11 |                                                                                      PolyAI Limited                                                                                     |               arXiv                |                           [Dial BEINFO for Faithfulness: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning](https://arxiv.org/abs/2311.09800)                           |                           **Factuality**&**Behavioural Fine-Tuning**&**Hallucination**                           |
| 23.11 |                                                The Hong Kong University of Science and Technology, University of Illinois Urbana-Champaign                                              |               arxiv                |                                                [R-Tuning: Teaching Large Language Models to Refuse Unknown Questions](https://arxiv.org/abs/2311.09677)                                                |                     **Hallucination**&**Refusal-Aware Instruction Tuning**&**Knowledge Gap**                     |
| 23.11 |                                               University of Southern California, University of Pennsylvania, University of California Davis                                             |               arxiv                |                                   [Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?](https://arxiv.org/abs/2311.09702)                                   |                            **Hallucinations**&**Semantic Associations**&**Benchmark**                            |
| 23.11 |                                                                 The Ohio State University, University of California Davis                                                               |               arxiv                |                           [How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities](https://arxiv.org/abs/2311.09447)                           |                     **Trustworthiness**&**Malicious Demonstrations**&**Adversarial Attacks**                     |
| 23.11 |                                                                                  University of Sheffield                                                                                |               arXiv                |                       [Lighter yet More Faithful: Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization](https://arxiv.org/abs/2311.09335)                        |                                **Hallucinations**&&**Language Model Reliability**                                |
| 23.11 |                                        Institute of Information Engineering Chinese Academy of Sciences, University of Chinese Academy of Sciences                                      |               arxiv                |                           [Can Large Language Models Understand Content and Propagation for Misinformation Detection: An Empirical Study](https://arxiv.org/abs/2311.12699)                            |                                           **Misinformation Detection**                                           |
| 23.11 |                                        Shanghai Jiaotong University, Amazon AWS AI, Westlake University, IGSNRR Chinese Academy of Sciences, China                                      |               arXiv                |                                              [Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus](https://arxiv.org/abs/2311.13230)                                               |                **Hallucination Detection**&**Uncertainty-Based Methods**&**Factuality Checking**                 |
| 23.11 |                                               Institute of Software Chinese Academy of Sciences, University of Chinese Academy of Sciences                                              |               arXiv                |                                  [Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting](https://arxiv.org/abs/2311.13314)                                  |                             **Hallucinations**&**Knowledge Graphs**&**Retrofitting**                             |
| 23.11 |                                                                                Applied Research Quantiphi                                                                               |               arxiv                |                                            [Minimizing Factual Inconsistency and Hallucination in Large Language Models](https://arxiv.org/abs/2311.13878)                                             |                                   **Factual Inconsistency**&**Hallucination**                                    |
| 23.11 |                                                                             Microsoft Research, Georgia Tech                                                                            |               arxiv                |                                                            [Calibrated Language Models Must Hallucinate](https://arxiv.org/abs/2311.14648)                                                             |                              **Hallucination&Calibration**&**Statistical Analysis**                              |
| 23.11 |                                                                     School of Information Renmin University of China                                                                    |               arxiv                |                               [UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation](https://arxiv.org/abs/2311.15296)                                |                                    **Hallucination**&**Evaluation Benchmark**                                    |
| 23.11 |                                                          DAMO Academy Alibaba Group, Nanyang Technological University, Hupan Lab                                                        |               arxiv                |                                [Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding](https://arxiv.org/abs/2311.16922)                                |                               **Vision-Language Models**&**Object Hallucinations**                               |
| 23.11 |                                                                                  Shanghai AI Laboratory                                                                                 |               arxiv                |                                 [Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization](https://arxiv.org/abs/2311.16839)                                  |           **Multimodal Language Models**&**Hallucination Problem**&**Direct Preference Optimization**            |
| 23.11 |                                                                                 Arizona State University                                                                                |             NAACL2024              |                                                   [Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey](https://arxiv.org/abs/2311.07914)                                                   |                                 **Knowledge Graphs**&**Hallucination Reduction**                                 |
| 23.11 |                                                                  Mohamed bin Zayed University of Artificial Intelligence                                                                |             NAACL2024              |                                             [A Survey of Confidence Estimation and Calibration in Large Language Models](https://arxiv.org/abs/2311.08298)                                             |                       **Confidence Estimation**&**Calibration**&**Large Language Models**                        |
| 23.11 |                                                                              University of California, Davis                                                                            |             NAACL2024              |                                   [Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?](https://arxiv.org/abs/2311.09702)                                   |                          **Semantic Shortcuts**&**Reasoning Chains**&**Hallucination**                           |
| 23.11 |                                                                                    University of Utah                                                                                   |             NAACL2024              |                                                    [To Tell The Truth: Language of Deception and Language Models](https://arxiv.org/abs/2311.07092)                                                    |                     **Deception Detection**&**Language Models**&**Conversational Analysis**                      |
| 23.11 |                                                                                    Cornell University                                                                                   |        NAACL2024(findings)         |                                                  [Adapting Fake News Detection to the Era of Large Language Models](https://arxiv.org/abs/2311.04917)                                                  |                              **Fake News Detection**&**Machine-Generated Content**                               |
| 23.12 |                                  Singapore Management University, Beijing Forestry University, University of Electronic Science and Technology of China                                 |              MMM 2024              |                              [Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites](https://arxiv.org/abs/2312.01701)                               |                     **Vision-language Models**&**Hallucination**&**Fine-grained Evaluation**                     |
| 23.12 |                                                                                  Mila, McGill University                                                                                |        EMNLP2023(findings)         |                                 [Evaluating Dependencies in Fact Editing for Language Models: Specificity and Implication Awareness](https://arxiv.org/abs/2312.01858)                                 |                             **Knowledge Bases**&**Dataset**&**Evaluation Protocol**                              |
| 23.12 |                                                                                         MIT CSAIL                                                                                       |               arxiv                |                            [Cognitive Dissonance: Why Do Language Model Outputs Disagree with Internal Representations of Truthfulness?](https://arxiv.org/abs/2312.03729)                             |                                  **Truthfulness**&**Internal Representations**                                   |
| 23.12 |                              University of Illinois Chicago, Bosch Research North America & Bosch Center for Artificial Intelligence (BCAI), UNC Chapel-Hill                            |               arxiv                |                                             [DELUCIONQA: Detecting Hallucinations in Domain-specific Question Answering](https://arxiv.org/abs/2312.05200)                                             |                 **Hallucination Detection**&**Domain-specific QA**&**Retrieval-augmented LLMs**                  |
| 23.12 |                                                                      The University of Hong Kong, Beihang University                                                                    |              AAAI2024              |                                              [Improving Factual Error Correction by Learning to Inject Factual Errors](https://arxiv.org/abs/2312.07049)                                               |                                           **Factual Error Correction**                                           |
| 23.12 |                                                                                  Allen Institute for AI                                                                                 |               arxiv                |                                    [BARDA: A Belief and Reasoning Dataset that Separates Factual Accuracy and Reasoning Ability](https://arxiv.org/abs/2312.07527)                                     |                              **Dataset**&**Factual Accuracy**&**Reasoning Ability**                              |
| 23.12 |                                         Tsinghua University, Shanghai Jiao Tong University, Stanford University, Nanyang Technological University                                       |               arxiv                |                            [The Earth is Flat because...: Investigating LLMs‚Äô Belief towards Misinformation via Persuasive Conversation](https://arxiv.org/abs/2312.09085)                             |                       **Misinformation**&**Persuasive Conversation**&**Factual Questions**                       |
| 23.12 |                                                                              University of California Davis                                                                             |               arXiv                |                                               [A Revisit of Fake News Dataset with Augmented Fact-checking by ChatGPT](https://arxiv.org/abs/2312.11870)                                               |                                         **Fake News**&**Fact-checking**                                          |
| 23.12 |                                                                                    Amazon Web Services                                                                                  |               arxiv                |                                                 [On Early Detection of Hallucinations in Factual Question Answering](https://arxiv.org/abs/2312.14183)                                                 |                                **Hallucinations**&**Factual Question Answering**                                 |
| 23.12 |                                                                            University of California Santa Cruz                                                                          |               arxiv                |      [Don‚Äôt Believe Everything You Read: Enhancing Summarization Interpretability through Automatic Identification of Hallucinations in Large Language Models](https://arxiv.org/abs/2312.14346)       |                               **Hallucinations**&**Faithfulness**&**Token-level**                                |
| 23.12 |                                                                 Department of Radiology, The University of Tokyo Hospital                                                               |               arxiv                |                                                           [Theory of Hallucinations based on Equivariance](https://arxiv.org/abs/2312.14504)                                                           |                                       **Hallucinations**&**Equivariance**                                        |
| 23.12 |                                                                              Georgia Institute of Technology                                                                            |               arXiv                |                                                    [REDUCING LLM HALLUCINATIONS USING EPISTEMIC NEURAL NETWORKS](https://arxiv.org/abs/2312.15576)                                                     |                           **Hallucinations**&**Uncertainty Estimation**&**TruthfulQA**                           |
| 23.12 |                                    Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Tencent AI Lab                                  |               arXiv                |                                         [Alleviating Hallucinations of Large Language Models through Induced Hallucinations](https://arxiv.org/abs/2312.15710)                                         |                       **Hallucinations**&**Induce-then-Contrast Decoding**&**Factuality**                        |
| 23.12 |                        SKLOIS Institute of Information Engineering Chinese Academy of Sciences, School of Cyber Security University of Chinese Academy of Sciences                      |               arXiv                |                                         [LLM Factoscope: Uncovering LLMs‚Äô Factual Discernment through Inner States Analysis](https://arxiv.org/abs/2312.16374)                                         |                                      **Factual Detection**&**Inner States**                                      |
| 24.01 |                                                                    The Chinese University of Hong Kong, Tencent AI Lab                                                                  |               arxiv                |                                                [The Earth is Flat? Unveiling Factual Errors in Large Language Models](https://arxiv.org/abs/2401.00761)                                                |                           **Factual Errors**&**Knowledge Graph**&**Answer Assessment**                           |
| 24.01 |                                                                    NewsBreak, University of Illinois Urbana-Champaign                                                                   |               arxiv                |                                  [RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models](https://arxiv.org/abs/2401.00396)                                   |                    **Retrieval-Augmented Generation**&**Hallucination Detection**&**Dataset**                    |
| 24.01 |                                                    University of California Berkeley, Universit√© de Montr√©al, McGill UniversityÔºå Mila                                                   |               arxiv                |                                                         [Uncertainty Resolution in Misinformation Detection](https://arxiv.org/abs/2401.01197)                                                         |                                  **Misinformation**&**Uncertainty Resolution**                                   |
| 24.01 |                                                                           Yale University, Stanford University                                                                          |               arxiv                |                                           [Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models](https://arxiv.org/abs/2401.01301)                                            |                                             **Legal Hallucinations**                                             |
| 24.01 |                                        Islamic University of Technology, AI Institute University of South Carolina, Stanford University, Amazon AI                                      |               arxiv                |                                       [A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models](https://arxiv.org/abs/2401.01313)                                       |                           √ü                               **Hallucination Mitigation**                           |
| 24.01 |                                                   Renmin University of China, Renmin University of China, DIRO, Universit√© de Montr√©al                                                  |               arxiv                |                                  [The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models](https://arxiv.org/abs/2401.03205)                                  |                        **Hallucination**&**Detection and Mitigation**&**Empirical Study**                        |
| 24.01 |                                       IIT Hyderabad India, Parmonic USA, University of Glasgow UK, LDRP Institute of Technology and Research India                                      |               arxiv                |                                   [Fighting Fire with Fire: Adversarial Prompting to Generate a Misinformation Detection Dataset](https://arxiv.org/abs/2401.04481)                                    |                          **Misinformation Detection**&**LLM-generated Synthetic Data**                           |
| 24.01 |                                                                                 University College London                                                                               |               arxiv                |                                                    [Hallucination Benchmark in Medical Visual Question Answering](https://arxiv.org/abs/2401.05827)                                                    |                        **Medical Visual Question Answering**&**Hallucination Benchmark**                         |
| 24.01 |                                                                                    Soochow University                                                                                   |               arxiv                |                                                             [LightHouse: A Survey of AGI Hallucination](https://arxiv.org/abs/2401.06792)                                                              |                                              **AGI Hallucination**                                               |
| 24.01 |                                                       University of Washington, Carnegie Mellon University, Allen Institute for AI                                                      |               arxiv                |                                                [Fine-grained Hallucination Detection and Editing for Language Models](https://arxiv.org/abs/2401.06855)                                                |                                       **Hallucination Detection**&**FAVA**                                       |
| 24.01 |                                                             Dartmouth College, Universit√© de Montr√©al, McGill UniversityÔºåMila                                                           |               arxiv                |                                            [Comparing GPT-4 and Open-Source Language Models in Misinformation Mitigation](https://arxiv.org/abs/2401.06920)                                            |                                      **GPT-4**&**Misinformation Detection**                                      |
| 24.01 |                                                                                    Utrecht University                                                                                   |               arxiv                |                                                               [The Pitfalls of Defining Hallucination](https://arxiv.org/abs/2401.07897)                                                               |                                                **Hallucination**                                                 |
| 24.01 |                                                                                     Samsung AI Center                                                                                   |               arxiv                |                                               [Hallucination Detection and Hallucination Mitigation: An Investigation](https://arxiv.org/abs/2401.08358)                                               |                             **Hallucination Detection**&**Hallucination Mitigation**                             |
| 24.01 |                                                                      McGill UniversityÔºå Mila, Universit√© de Montr√©al                                                                    |               arxiv                |                       [Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation](https://arxiv.org/abs/2401.08694)                        |            **Misinformation Mitigation**&**Uncertainty Quantification**&**Sample-based Consistency**             |
| 24.01 |                                                                                      LY Corporation                                                                                     |               arxiv                |                                                  [On the Audio Hallucinations in Large Audio-Video Language Models](https://arxiv.org/abs/2401.09774)                                                  |                **Audio Hallucinations**&**Audio-visual Learning**&**Audio-video language Models**                |
| 24.01 |                                                                           Sun Yat-sen University Tencent AI Lab                                                                         |               arXiv                |                                       [Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment](https://arxiv.org/abs/2401.10768)                                        |                         **Hallucination Mitigation**&**Knowledge Consistent Alignment**                          |
| 24.01 |                                                                             National University of Singapore                                                                            |               arxiv                |                                             [Hallucination is Inevitable: An Innate Limitation of Large Language Models](https://arxiv.org/abs/2401.11817)                                             |                                      **Hallucination**&**Real World LLMs**                                       |
| 24.01 |                                                                       X2Robot&International Digital Economy Academy                                                                     |               arXiv                |                                  [Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for Hallucination Mitigation](https://arxiv.org/abs/2401.15449)                                   |                  **Hallucination Mitigation**&**Knowledge Probing**&**Reinforcement Learning**                   |
| 24.01 |                                                                  University of Texas at Austin, Northeastern University                                                                 |               arxiv                |                             [Diverse but Divisive: LLMs Can Exaggerate Gender Differences in Opinion Related to Harms of Misinformation](https://arxiv.org/abs/2401.16558)                             |                             **Misinformation Detection**&**Socio-Technical Systems**                             |
| 24.01 |                                                        National University of Defense Technology, National University of Singapore                                                      |               arxiv                |                                   [SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering](https://arxiv.org/abs/2401.17809)                                    |                                **Factual Knowledge Editing**&**Word Embeddings**                                 |
| 24.02 |                        University of Washington, University of California Berkeley, The Hong Kong University of Science and Technology, Carnegie Mellon University                      |               arxiv                |                                       [Don‚Äôt Hallucinate Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration](https://arxiv.org/abs/2402.00367)                                        |                                  **Knowledge Gaps**&**Multi-LLM Collaboration**                                  |
| 24.02 |                                                                  IT Innovation and Research Center, Huawei Technologies                                                                 |               arxiv                |                                                     [A Survey on Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2402.00253)                                                      |                   **Large Vision-Language Models**&**Hallucination**&**Mitigation Strategies**                   |
| 24.02 |                                                               Tianjin University, National University of Singapore, A*STAR                                                              |               arxiv                |                                          [SKIP \N: A SIMPLE METHOD TO REDUCE HALLUCINATION IN LARGE VISION-LANGUAGE MODELS](https://arxiv.org/abs/2402.01345)                                          |                 **Semantic Shift Bias**&**Hallucination Mitigation**&**Vision-Language Models**                  |
| 24.02 |                                                                       University of Marburg, University of Mannheim                                                                     |         EACL Findings 2024         |                                       [The Queen of England is not England‚Äôs Queen: On the Lack of Factual Coherency in PLMs](https://arxiv.org/abs/2402.01453)                                        |                                    **Factual Coherency**&**Knowledge Bases**                                     |
| 24.02 |                                                                    MBZUAI, Monash University, LibrAI, Sofia University                                                                  |               arxiv                |                                                        [Factuality of Large Language Models in the Year 2024](https://arxiv.org/abs/2402.02420)                                                        |                                **Factuality**&**Evaluation**&**Multimodal LLMs**                                 |
| 24.02 |                                       Institute of Information Engineering, Chinese Academy of Sciences, University of Chinese Academy of Sciences                                      |               arxiv                |                                                        [Are Large Language Models Table-based Fact-Checkers?](https://arxiv.org/abs/2402.02549)                                                        |                            **Table-based Fact Verification**&**In-context Learning**                             |
| 24.02 |                                                                              Zhejiang University, Ant Group                                                                             |               arxiv                |                                                [Unified Hallucination Detection for Multimodal Large Language Models](https://arxiv.org/abs/2402.03190)                                                |                  **Multimodal Large Language Models**&**Hallucination Detection**&**Benchmark**                  |
| 24.02 |                                                                            Alibaba Cloud, Zhejiang University                                                                           |              ICLR2024              |                                             [INSIDE: LLMS‚Äô INTERNAL STATES RETAIN THE POWER OF HALLUCINATION DETECTION](https://arxiv.org/abs/2402.03744)                                              |                                    **Hallucination Detection**&**EigenScore**                                    |
| 24.02 |                           The Hong Kong University of Science and Technology, University of Illinois at Urbana-Champaign, The Hong Kong Polytechnic University                          |               arxiv                |                                                [The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs](https://arxiv.org/abs/2402.03757)                                                |                              **Multimodal Large Language Models**&**Hallucination**                              |
| 24.02 |                                              Institute of Automation Chinese Academy of Sciences, University of Chinese Academy of Sciences                                             |               arxiv                |                                                      [Can Large Language Models Detect Rumors on Social Media?](https://arxiv.org/abs/2402.03916)                                                      |                                       **Rumor Detection**&**Social Media**                                       |
| 24.02 |         CAS Key Laboratory of AI Safety, School of Computer Science and Technology University of Chinese Academy of Science, International Digital Economy Academy IDEA Research        |               arxiv                |                                            [A Survey on Large Language Model Hallucination via a Creativity Perspective](https://arxiv.org/abs/2402.06647)                                             |                                         **Creativity**&**Hallucination**                                         |
| 24.02 |                                                             University College London, Speechmatics, MATS, Anthropic, FAR AI                                                            |               arxiv                |                                                 [Debating with More Persuasive LLMs Leads to More Truthful Answers](https://arxiv.org/abs/2402.06782)                                                  |                                           **Debate**&**Truthfulness**                                            |
| 24.02 |                                               University of Illinois Urbana-Champaign, DAMO Academy Alibaba Group, Northwestern University                                              |               arxiv                |                                                 [Towards Faithful Explainable Fact-Checking via Multi-Agent Debate](https://arxiv.org/abs/2402.07401)                                                  |                                       **Fact-checking**&**Explainability**                                       |
| 24.02 |                                  Rice Universitym, Texas A&M University, Wake Forest University, New Jersey Institute of Technology, Meta Platforms Inc.                                |               arxiv                |                                                            [Large Language Models As Faithful Explainers](https://arxiv.org/abs/2402.04678)                                                            |                                 **Explainability**&**Fidelity**&**Optimization**                                 |
| 24.02 |                                                                    The Hong Kong University of Science and Technology                                                                   |               arxiv                |                                        [Do LLMs Know about Hallucination? An Empirical Investigation of LLM‚Äôs Hidden States](https://arxiv.org/abs/2402.09733)                                         |                           **Hallucination**&**Hidden States**&**Model Interpretation**                           |
| 24.02 |                                                                UC Santa Cruz, ByteDance Research, Northwestern University                                                               |               arxiv                |                                   [MEASURING AND REDUCING LLM HALLUCINATION WITHOUT GOLD-STANDARD ANSWERS VIA EXPERTISE-WEIGHTING](https://arxiv.org/abs/2402.10412)                                   |             **Large Language Models (LLMs)**&**Hallucination**&**Factualness Evaluations**&**FEWL**              |
| 24.02 |                                                     Paul G. Allen School of Computer Science & Engineering, University of Washington                                                    |               arxiv                |                                               [Comparing Hallucination Detection Metrics for Multilingual Generation](https://arxiv.org/abs/2402.10496)                                                | **Hallucination Detection**&**Multilingual Generation**&**Lexical Metrics**&**Natural Language Inference (NLI)** |
| 24.02 |                                         Institute of Computing Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences                                       |               arxiv                |                         [Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models](https://arxiv.org/abs/2402.10612)                         |        **Large Language Models (LLMs)**&**Hallucination Mitigation**&**Retrieval Augmentation**&**Rowen**        |
| 24.02 |                                    Institute of Automation, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Nanjing University                                  |               arxiv                |                                       [Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2402.11622)                                        |                           **Object Hallucination**&**Vision-Language Models (LVLMs)**                            |
| 24.02 |                                 Institute of Mathematics and Statistics University of S√£o Paulo, Artificial Intelligence Specialist in the Banking Sector                               |               arxiv                |                      [Hallucinations or Attention Misdirection? The Path to Strategic Value Extraction in Business Using Large Language Models](https://arxiv.org/abs/2402.14002)                      |                            **Hallucinations**&**Generative Artificial Intelligence**                             |
| 24.02 |                                                                       Stevens Institute of Technology, Peraton Labs                                                                     |               arxiv                |                                           [Can Large Language Models Detect Misinformation in Scientific News Reporting?](https://arxiv.org/abs/2402.14268)                                            |                          **Scientific Reporting**&**Misinformation**&**Explainability**                          |
| 24.02 |                                                                             Middle East Technical University                                                                            |               arxiv                |                                       [HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs](https://arxiv.org/abs/2402.16211)                                       |                                    **Hallucination**&**Benchmarking Dataset**                                    |
| 24.02 |                                                                             National University of Singapore                                                                            |               arxiv                |                               [Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding](https://arxiv.org/abs/2402.15300)                               |                      **Vision-Language Models**&**Hallucination**&**CLIP-Guided Decoding**                       |
| 24.02 |                                                                   University of California Los Angeles, Cisco Research                                                                  |               arxiv                |                                   [Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension](https://arxiv.org/abs/2402.18048)                                   |                                  **Truthfulness**&**Local Intrinsic Dimension**                                  |
| 24.02 |                 Institute of Automation Chinese Academy of Sciences, School of Artificial Intelligence University of Chinese Academy of Sciences, Hunan Normal University               |               arxiv                |                          [Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models](https://arxiv.org/abs/2402.19103)                           |                             **False Premise Hallucinations**&**Attention Mechanism**                             |
| 24.02 |        Shanghai Artificial Intelligence Laboratory, Renmin University of China, University of Chinese Academy of Sciences, Shanghai Jiao Tong University, The University of Sydney      |               arxiv                |                                 [Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models](https://arxiv.org/abs/2402.19465)                                  |                                  **Trustworthiness Dynamics**&**Pre-training**                                   |
| 24.02 |                                              AWS AI Labs&Korea Advanced Institute of Science & Technology&The University of Texas at Austin                                             |             NAACL2024              |                                        [TOFUEVAL: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization](https://arxiv.org/abs/2402.13249)                                         |                             **Hallucination Evaluation**&**Dialogue Summarization**                              |
| 24.03 |                                         √âcole polytechnique f√©d√©rale de Lausanne, Carnegie Mellon University, University of Maryland College Park                                       |               arxiv                |                                           ["Flex Tape Can‚Äôt Fix That": Bias and Misinformation in Edited Language Models](https://arxiv.org/abs/2403.00180)                                            |                            **Model Editing**&**Demographic Bias**&**Misinformation**                             |
| 24.03 |                                                                               East China Normal University                                                                              |               arxiv                |                                       [DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2403.00896)                                       |                 **Dialogue-level Hallucination**&**Benchmarking**&**Human-machine Interaction**                  |
| 24.03 |                                                                                     Peking University                                                                                   |               arxiv                |                             [Evaluating and Mitigating Number Hallucinations in Large Vision-Language Models: A Consistency Perspective](https://arxiv.org/abs/2403.01373)                             |                   **Number Hallucination**&**Vision-Language Models**&**Consistency Training**                   |
| 24.03 | City University of Hong Kong, National University of Singapore, Shanghai Jiao Tong University, Stanford University, Penn State University, Hong Kong University of Science and Technology |               arxiv                |                                  [IN-CONTEXT SHARPNESS AS ALERTS: AN INNER REPRESENTATION PERSPECTIVE FOR HALLUCINATION MITIGATION](https://arxiv.org/abs/2403.01548)                                  |                              **Hallucination**&**Inner Representation**&**Entropy**                              |
| 24.03 |                                                                                         Microsoft                                                                                       |               arxiv                |                                              [In Search of Truth: An Interrogation Approach to Hallucination Detection](https://arxiv.org/abs/2403.02889)                                              |                  **Hallucination Detection**&**Interrogation Technique**&**Balanced Accuracy**                   |
| 24.03 |                                                                  Mohamed bin Zayed University of Artificial Intelligence                                                                |               arxiv                |                                                [Multimodal Large Language Models to Support Real-World Fact-Checking](https://arxiv.org/abs/2403.03627)                                                |                    **Multimodal Large Language Models**&**Fact-Checking**&**Misinformation**                     |
| 24.03 |                                                                              KAIST, Microsoft Research Asia                                                                             |               arxiv                |                          [ERBENCH: AN ENTITY-RELATIONSHIP BASED AUTOMATICALLY VERIFIABLE HALLUCINATION BENCHMARK FOR LARGE LANGUAGE MODELS](https://arxiv.org/abs/2403.05266)                          |                         **Hallucination**&**Entity-Relationship Model**&**Benchmarking**                         |
| 24.03 |                                                                University of Alberta, Platform and Content Group, Tencent                                                               |               arxiv                |                                                  [SIFiD: Reassess Summary Factual Inconsistency Detection with LLM](https://arxiv.org/abs/2403.07557)                                                  |                                    **Factual Consistency**&**Summarization**                                     |
| 24.03 |              Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences            |               arxiv                |                     [Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts](https://arxiv.org/abs/2403.07556)                      |                                    **Truth Detection**&**Context Selection**                                     |
| 24.03 |                                                                               UC Berkeley, Google DeepMind                                                                              |               arxiv                |                                               [Unfamiliar Finetuning Examples Control How Language Models Hallucinate](https://arxiv.org/abs/2403.05612)                                               |                                     **Finetuning**&**Hallucination Control**                                     |
| 24.03 |                                                                University of Alberta, Platform and Content Group, Tencent                                                               |               arxiv                |                                                  [SIFiD: Reassess Summary Factual Inconsistency Detection with LLM](https://arxiv.org/abs/2403.07557)                                                  |                                    **Factual Consistency**&**Summarization**                                     |
| 24.03 |              Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences            |               arxiv                |                     [Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts](https://arxiv.org/abs/2403.07556)                      |                                    **Truth Detection**&**Context Selection**                                     |
| 24.03 |                                                                               UC Berkeley, Google DeepMind                                                                              |               arxiv                |                                               [Unfamiliar Finetuning Examples Control How Language Models Hallucinate](https://arxiv.org/abs/2403.05612)                                               |                                     **Finetuning**&**Hallucination Control**                                     |
| 24.03 |                                                                               Google Research, UC San Diego                                                                             |            COLING 2024             |                               [Detecting Hallucination and Coverage Errors in Retrieval Augmented Generation for Controversial Topics](https://arxiv.org/abs/2403.08904)                               |                             **Conversational Systems**&**Evaluation Methodologies**                              |
| 24.03 |                                                            University of Maryland, University of Antwerp, New York University                                                           |               arxiv                |                                                     [Evaluating LLMs for Gender Disparities in Notable Persons](https://arxiv.org/abs/2403.09148)                                                      |                                     **Bias**&**Fairness**&**Hallucinations**                                     |
| 24.03 |                                                                               University of Duisburg-Essen                                                                              |               arxiv                |                    [The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions](https://arxiv.org/abs/2403.09743)                    |                                                **Hallucination**                                                 |
| 24.03 |                                             Wuhan University, Beihang University, The University of Sydney, Nanyang Technological University                                            |            COLING 2024             |                              [Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in Factual Knowledge Extraction](https://arxiv.org/abs/2403.09963)                               |                                 **Factual Knowledge Extraction**&**Prompt Bias**                                 |
| 24.03 |                                                                                Carnegie Mellon University                                                                               |               arxiv                |               [Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases](https://arxiv.org/abs/2403.10446)                |             **Retrieval Augmented Generation (RAG)**&**Private Knowledge-Bases**&**Hallucinations**              |
| 24.03 |                                                                   Integrated Vision and Language Lab KAIST South Korea                                                                  |               arxiv                |                                 [What if...?: Counterfactual Inception to Mitigate Hallucination Effects in Large Multimodal Models](https://arxiv.org/abs/2403.13513)                                 |                                  **Large Multimodal Models**&**Hallucination**                                   |
| 24.03 |                                                                                           UCAS                                                                                          |               arxiv                |                               [MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation](https://arxiv.org/abs/2403.14171)                               |                        **Multimodal Misinformation Detection**&**Knowledge Distillation**                        |
| 24.03 |                                                                       Seoul National University, Sogang University                                                                      |               arxiv                |                                      [Exploiting Semantic Reconstruction to Mitigate Hallucinations in Vision-Language Models](https://arxiv.org/abs/2403.16167)                                       |               **Semantic Reconstruction**&**Vision-Language Models**&**Hallucination Mitigation**                |
| 24.03 |                                                                          University of Illinois Urbana-Champaign                                                                        |               arxiv                |                     [Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art](https://arxiv.org/abs/2403.16527)                     |                      **Hallucination Detection**&**Foundation Models**&**Decision-Making**                       |
| 24.03 |                                                                               Shanghai Jiao Tong University                                                                             |               arxiv                |                             [Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback](https://arxiv.org/abs/2403.18349)                             |                      **Knowledge Feedback**&**Reliable Reward Model**&**Refusal Mechanism**                      |
| 24.03 |                                                                       Universit√§t Hamburg, The University of Sydney                                                                     |               arxiv                |                                  [Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding](https://arxiv.org/abs/2403.18715)                                   |        **Instruction Contrastive Decoding**&**Large Vision-Language Models**&**Hallucination Mitigation**        |
| 24.03 |                   AI Institute University of South Carolina, Indian Institute of Technology Kharagpur, Islamic University of Technology, Stanford University, Amazon AI                 |               arxiv                |                 [‚ÄúSorry Come Again?‚Äù Prompting ‚Äì Enhancing Comprehension and Diminishing Hallucination with [PAUSE] -injected Optimal Paraphrasing](https://arxiv.org/abs/2403.18976)                  |                    **Prompt Engineering**&**Hallucination Mitigation**&**[PAUSE] Injection**                     |
| 24.04 |                                          Beihang University, School of Computer Science and Engineering, School of Software, Shandong University                                        |               arxiv                |                                               [Exploring and Evaluating Hallucinations in LLM-Powered Code Generation](https://arxiv.org/abs/2404.00971)                                               |                                      **Code Generation**&**Hallucination**                                       |
| 24.03 |                                                                          University of Illinois Urbana-Champaign                                                                        |             NAACL2024              |                                         [Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation](https://arxiv.org/abs/2403.14952)                                          |             **Online Misinformation**&**Retrieval Augmented Response**&**Evidence-Based Countering**             |
| 24.03 |                                   Department of Electronic Engineering, Tsinghua University, Pattern Recognition Center, WeChat AI, Tencent Inc, China                                  |             NAACL 2024             |                                                 [On Large Language Models‚Äô Hallucination with Regard to Known Facts](https://arxiv.org/abs/2403.20009)                                                 |                                     **Hallucination**&**Inference Dynamics**                                     |
| 24.03 |                                   Department of Electronic Engineering, Tsinghua University, Pattern Recognition Center, WeChat AI, Tencent Inc, China                                  |             NAACL 2024             |                                                 [On Large Language Models‚Äô Hallucination with Regard to Known Facts](https://arxiv.org/abs/2403.20009)                                                 |                                     **Hallucination**&**Inference Dynamics**                                     |
| 24.03 |                                                                       Tsinghua University, WeChat AI, Tencent Inc.                                                                      |             NAACL2024              |                                                 [On Large Language Models‚Äô Hallucination with Regard to Known Facts](https://arxiv.org/abs/2403.20009)                                                 |                                     **Hallucination**&**Inference Dynamics**                                     |
| 24.04 |                                                      Technical University of Munich, University of Stavanger, University of Alberta                                                     |               arxiv                |                                    [PoLLMgraph: Unraveling Hallucinations in Large Language Models via State Transition Dynamics](https://arxiv.org/abs/2404.04722)                                    |               **Hallucination Detection**&**State Transition Dynamics**&**Large Language Models**                |
| 24.04 |                                                    University of Edinburgh, University College London, Peking University, Together AI                                                   |               arxiv                |                                 [The Hallucinations Leaderboard ‚Äì An Open Effort to Measure Hallucinations in Large Language Models](https://arxiv.org/abs/2404.05904)                                 |                                   **Hallucination Detection**&**Benchmarking**                                   |
| 24.04 |                                                IIIT Hyderabad, Purdue University, Northwestern University, Indiana University Indianapolis                                              |               arxiv                |                     [Halu-NLP at SemEval-2024 Task 6: MetaCheckGPT - A Multi-task Hallucination Detection Using LLM Uncertainty and Meta-models](https://arxiv.org/abs/2404.06948)                     |                         **Hallucination Detection**&**LLM Uncertainty**&**Meta-models**                          |
| 24.04 |                                                                Technion ‚Äì Israel Institute of Technology, Google Research                                                               |               arxiv                |                                           [Constructing Benchmarks and Interventions for Combating Hallucinations in LLMs](https://arxiv.org/abs/2404.09971)                                           |                                        **Hallucinations**&**Benchmarks**                                         |
| 24.04 |                                                                 The University of Texas at Austin, Salesforce AI Research                                                               |               arxiv                |                                                 [MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents](https://arxiv.org/abs/2404.10774)                                                  |                                         **Fact-Checking**&**Efficiency**                                         |
| 24.04 |                                                                           Meta, Technical University of Munich                                                                          |               arxiv                |                                          [Uncertainty-Based Abstention in LLMs Improves Safety and Reduces Hallucinations](https://arxiv.org/abs/2404.10960)                                           |                                  **Safety**&**Hallucinations**&**Uncertainty**                                   |
| 24.04 |                                                                   Zhejiang University, Alibaba Group,  Fudan University                                                                 |               arxiv                |                                [Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback](https://arxiv.org/abs/2404.14233)                                 |  **Large Vision Language Model**&**Hallucination Detection And Mitigating**&**Direct Preference Optimization**   |
| 24.04 |                                                                            Cheriton School of Computer Science                                                                          |               arxiv                |                                                         [Rumour Evaluation with Very Large Language Models](https://arxiv.org/abs/2404.16859)                                                          |                             **Misinformation in Social Networks**&**Explainable AI**                             |
| 24.04 |                                                                            University of California, Berkeley                                                                           |             NAACL 2024             |                                                   [ALOHa: A New Measure for Hallucination in Captioning Models](https://davidmchan.github.io/aloha)                                                    |                                   **Adversarial Attack**&**AI-Text Detection**                                   |
| 24.04 |                                                                                        ServiceNow                                                                                       |             NAACL 2024             |                                          [Reducing hallucination in structured outputs via Retrieval-Augmented Generation](https://arxiv.org/abs/2404.08189)                                           |                   **Retrieval-Augmented Generation**&**Structured Outputs**&**Generative AI**                    |
| 24.04 |                                                                                    Stanford University                                                                                  |             NAACL2024              |                               [NLP Systems That Can‚Äôt Tell Use from Mention Censor Counterspeech, but Teaching the Distinction Helps](https://arxiv.org/abs/2404.01651)                                |                           **Counterspeech**&**Censorship**&**Use-Mention Distinction**                           |
| 24.04 |                                                                  Department of Computing Science, University of Aberdeen                                                                |             NAACL2024              |                                  [Improving Factual Accuracy of Neural Table-to-Text Output by Addressing Input Problems in ToTTo](https://arxiv.org/abs/2404.04103)                                   |                         **Neural Table-to-Text**&**Factual Accuracy**&**Input Problems**                         |
| 24.04 |                                                                                 Seoul National University                                                                               |        NAACL2024(findings)         |                                  [Mitigating Hallucination in Abstractive Summarization with Domain-Conditional Mutual Information](https://arxiv.org/abs/2404.09480)                                  |            **Hallucination**&**Abstractive Summarization**&**Domain-Conditional Mutual Information**             |
| 24.05 |      The University of Tokyo, University of California Santa Barbara, Mila - Qu√©bec AI Institute,  Universit√© de Montr√©al, Speech Lab, Alibaba Group,  Hong Kong Baptist University     |               arxiv                |                                            [CodeHalu: Code Hallucinations in LLMs Driven by Execution-based Verification](https://arxiv.org/abs/2405.00253)                                            |                             **Code Hallucination**&**Execution-based Verification**                              |
| 24.05 |                                                                Department of Computer Science, The University of Sheffield                                                              |               arxiv                |                                    [Addressing Topic Granularity and Hallucination in Large Language Models for Topic Modelling](https://arxiv.org/abs/2405.00611)                                     |                           **Topic Modelling**&**Hallucination**&**Topic Granularity**                            |
| 24.04 |                                                                        School of Computing and Information Systems                                                                      |            COLING 2024             |                             [Reinforcement Retrieval Leveraging Fine-grained Feedback for Fact Checking News Claims with Black-Box LLM](https://arxiv.org/abs/2404.17283)                              |                   **Claim Verification**&**Reinforcement Retrieval**&**Fine-Grained Feedback**                   |
| 24.05 |                                                                                         DeepMind                                                                                        |               arxiv                |                                                       [Mitigating LLM Hallucinations via Conformal Abstention](https://arxiv.org/abs/2405.01563)                                                       |                              **Conformal Prediction**&**Hallucination Mitigation**                               |
| 24.05 |                                                                        MBZUAI, Monash University, Sofia University                                                                      |               arxiv                |                                                [OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs](https://arxiv.org/abs/2405.05583)                                                |                              **Factuality Evaluation**&**Automatic Fact-Checking**                               |
| 24.05 |                                                                           Indian Institute of Technology Patna                                                                          |               arxiv                |                                 [Unveiling Hallucination in Text, Image, Video, and Audio Foundation Models: A Comprehensive Review](https://arxiv.org/abs/2405.09589)                                 |                           **Hallucination Detection**&**Multimodal Models**&**Review**                           |
| 24.05 |                                                                                  Dublin City University                                                                                 |               arxiv                |                                          [Tell Me Why: Explainable Public Health Fact-Checking with Large Language Models](https://arxiv.org/abs/2405.09454)                                           |                              **Explainable AI**&**Fact-Checking**&**Public Health**                              |
| 24.05 |                                                             University of Information Technology, Vietnam National University                                                           |               arxiv                |                                          [ViWikiFC: Fact-Checking for Vietnamese Wikipedia-Based Textual Knowledge Source](https://arxiv.org/abs/2405.07615)                                           |                            **Fact Checking**&**Information Verification**&**Corpus**                             |
| 24.05 |                                                                                  Imperial College London                                                                                |               arxiv                |                                [Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval](https://arxiv.org/abs/2405.06545)                                 |                            **Hallucination Mitigation**&**Knowledge Graph Retrieval**                            |
| 24.05 |                                                                  Paul G. Allen School of Computer Science & Engineering                                                                 |               arxiv                |                             [MASSIVE Multilingual Abstract Meaning Representation: A Dataset and Baselines for Hallucination Detection](https://arxiv.org/abs/2405.19285)                              |                           **Hallucination Detection**&**Multilingual AMR**&**Dataset**                           |
| 24.05 |                                                                                   Microsoft Corporation                                                                                 |               arxiv                |                                                     [Unlearning Climate Misinformation in Large Language Models](https://arxiv.org/abs/2405.19563)                                                     |                            **Climate Misinformation**&**Unlearning**&**Fine-Tuning**                             |
| 24.05 |                                                                                     Baylor University                                                                                   |               arxiv                |                                     [Detecting Hallucinations in Large Language Model Generation: A Token Probability Approach](https://arxiv.org/abs/2405.19648)                                      |                           **Hallucinations Detection**&**Token Probability Approach**                            |
| 24.05 |                                                                                  Shanghai AI Laboratory                                                                                 |               arxiv                |                                               [ANAH: Analytical Annotation of Hallucinations in Large Language Models](https://arxiv.org/abs/2405.20315)                                               |                                   **Hallucinations**&**Analytical Annotation**                                   |
| 24.06 |                                                                                  University of Waterloo                                                                                 |               arxiv                |                                                 [TruthEval: A Dataset to Evaluate LLM Truthfulness and Reliability](https://arxiv.org/abs/2406.01855)                                                  |                                         **Truthfulness**&**Reliability**                                         |
| 24.06 |                                                                                     Peking University                                                                                   |               arxiv                |                                      [Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework](https://arxiv.org/abs/2406.03075)                                      |                       **Hallucination Detection**&**Markov Chain**&**Multi-agent Debate**                        |
| 24.06 |                                                                                  Northeastern University                                                                                |              ACL 2024              |                                  [Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends](https://arxiv.org/abs/2406.03487)                                   |                 **Dialogue Summarization**&**Circumstantial Hallucination**&**Error Detection**                  |
| 24.06 |                                                                                     McGill University                                                                                   |              ACL 2024              |                                             [Confabulation: The Surprising Value of Large Language Model Hallucinations](https://arxiv.org/abs/2406.04175)                                             |                               **Confabulation**&**Hallucinations**&**Narrativity**                               |
| 24.06 |                                                                                  University of Michigan                                                                                 |               arxiv                |                                     [3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination](https://arxiv.org/abs/2406.05132)                                     |                                   **3D-LLMs**&**Grounding**&**Hallucination**                                    |
| 24.06 |                                                                                 Arizona State University                                                                                |               arxiv                |                                          [Investigating and Addressing Hallucinations of LLMs in Tasks Involving Negation](https://arxiv.org/abs/2406.05494)                                           |                                         **Hallucinations**&**Negation**                                          |
| 24.06 |                                                                                    Tsinghua University                                                                                  |               arxiv                |                                      [Benchmarking Trustworthiness of Multimodal Large Language Models: A Comprehensive Study](https://arxiv.org/abs/2406.07057)                                       |                                   **Trustworthiness**&**MLLMs**&**Benchmark**                                    |
| 24.06 |                                                                        Beijing Academy of Artificial Intelligence                                                                       |               arxiv                |                                      [HalluDial: A Large-Scale Benchmark for Automatic Dialogue-Level Hallucination Evaluation](https://arxiv.org/abs/2406.07070)                                      |                          **Hallucination Evaluation**&**Dialogue-Level**&**HalluDial**                           |
| 24.06 |                                                                                           KFUPM                                                                                         |               arxiv                |                                                 [DefAn: Definitive Answer Dataset for LLMs Hallucination Evaluation](https://arxiv.org/abs/2406.09155)                                                 |                               **Hallucination Evaluation**&**Definitive Answers**                                |
| 24.06 |                                                                              Harbin Institute of Technology                                                                             |         ACL 2024 findings          |                               [Paying More Attention to Source Context: Mitigating Unfaithful Translations from Large Language Model](https://arxiv.org/abs/2406.07036)                                |                                  **Unfaithful Translations**&**Source Context**                                  |
| 24.06 |                                                                                National Taiwan University                                                                               |          Interspeech 2024          |                         [Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models](https://arxiv.org/abs/2406.08402)                          |              **Large audio-language models**&**Object hallucination**&**Discriminative questions**               |
| 24.06 |                                                                            University of Texas at San Antonio                                                                           |               arxiv                |                               [We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs](https://arxiv.org/abs/2406.10279)                                |              **Package Hallucinations**&**Code Generating LLMs**&**Software Supply Chain Security**              |
| 24.06 |                                                                               The University of Manchester                                                                              |               arxiv                |               [RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation Detection Using In-Context Learning based on Emotional Information](https://arxiv.org/abs/2406.11093)                |                 **RAEmoLLM**&**Cross-Domain Misinformation Detection**&**Affective Information**                 |
| 24.06 |                                                                                           KAIST                                                                                         |               arxiv                |                                       [Adversarial Style Augmentation via Large Language Model for Robust Fake News Detection](https://arxiv.org/abs/2406.11260)                                       |                            **Adversarial Style Augmentation**&**Fake News Detection**                            |
| 24.06 |                                                                            The Chinese University of Hong Kong                                                                          |               arxiv                |                                               [Mitigating Large Language Model Hallucination with Faithful Finetuning](https://arxiv.org/abs/2406.11267)                                               |                                    **Hallucination**&**Faithful Finetuning**                                     |
| 24.06 |                                                           Gaoling School of Artificial Intelligence, Renmin University of China                                                         |               arxiv                |                                       [Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector](https://arxiv.org/abs/2406.11277)                                        |                       **Hallucination Detection**&**Small Language Models**&**HaluAgent**                        |
| 24.06 |                                                                       University of Science and Technology of China                                                                     |               arxiv                |                                     [CrAM: Credibility-Aware Attention Modification in LLMs for Combating Misinformation in RAG](https://arxiv.org/abs/2406.11497)                                     |                   **CrAM**&**Credibility-Aware Attention**&**Retrieval-Augmented Generation**                    |
| 24.06 |                                                                                  University of Rochester                                                                                |               arxiv                |                                        [Do More Details Always Introduce More Hallucinations in LVLM-based Image Captioning?](https://arxiv.org/abs/2406.12663)                                        |                             **LVLMs**&**Image Captioning**&**Object Hallucination**                              |
| 24.06 |                                                                                 Xi'an Jiaotong University                                                                               |               arxiv                |                         [AGLA: Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention](https://arxiv.org/abs/2406.12718)                         |                       **AGLA**&**Object Hallucinations**&**Large Vision-Language Models**                        |
| 24.06 |                                                                     University of Groningen, University of Amsterdam                                                                    |               arxiv                |                                      [Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation](https://arxiv.org/abs/2406.13663)                                       |                              **Retrieval-Augmented Generation**&**Trustworthy AI**                               |
| 24.06 |                                                                                 Seoul National University                                                                               |               arxiv                |                                   [Large Language Models are Skeptics: False Negative Problem of Input-conflicting Hallucination](https://arxiv.org/abs/2406.13929)                                    |                     **False Negative Problem**&**Input-conflicting Hallucination**&**Bias**                      |
| 24.06 |                                                                                   University of Houston                                                                                 |               arxiv                |                                        [Seeing Through AI‚Äôs Lens: Enhancing Human Skepticism Towards LLM-Generated Fake News](https://arxiv.org/abs/2406.14012)                                        |                                       **Fake news**&**LLM-generated news**                                       |
| 24.06 |                                                                                   University of Oxford                                                                                  |               arxiv                |                                             [Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs](https://arxiv.org/abs/2406.15927)                                              |                           **Hallucination Detection**&**Semantic Entropy**&**Probes**                            |
| 24.06 |                                                                                       UC San Diego                                                                                      |               arxiv                |                                                     [Mitigating Hallucination in Fictional Character Role-Play](https://arxiv.org/abs/2406.17260)                                                      |                       **Hallucination Mitigation**&**Role-Play**&**Fictional Characters**                        |
| 24.06 |                                                                                          Lamini                                                                                         |               arxiv                |                                                  [Banishing LLM Hallucinations Requires Rethinking Generalization](https://arxiv.org/abs/2406.17642)                                                   |                             **Hallucinations**&**Generalization**&**Memory Experts**                             |
| 24.06 |                                                                                     Waseda University                                                                                   |               arxiv                |                              [ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models](https://arxiv.org/abs/2406.20015)                               |          **Tool-Augmented Large Language Models**&**Hallucination Diagnostic Benchmark**&**Tool Usage**          |
| 24.07 |                                                                                    Beihang University                                                                                   |               arxiv                |                               [PFME: A Modular Approach for Fine-grained Hallucination Detection and Editing of Large Language Models](https://arxiv.org/abs/2407.00488)                               |                                  **Hallucination Detection**&**Model Editing**                                   |
| 24.07 |                                                                                    Tsinghua University                                                                                  |               arxiv                |                                          [Fake News Detection and Manipulation Reasoning via Large Vision-Language Models](https://arxiv.org/abs/2407.02042)                                           |               **Large Vision-Language Models**&**Fake News Detection**&**Manipulation Reasoning**                |
| 24.07 |                                                                               Brno University of Technology                                                                             |               arxiv                |                                               [Generative Large Language Models in Automated Fact-Checking: A Survey](https://arxiv.org/abs/2407.02351)                                                |                                      **Automated Fact-Checking**&**Survey**                                      |
| 24.07 |                                                                                     SRI International                                                                                   |               arxiv                |                            [Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification](https://arxiv.org/abs/2407.02352)                            |                        **Vision-LLMs**&**Hallucination Detection**&**Claim Verification**                        |
| 24.07 |                                                                      Hong Kong University of Science and Technology                                                                     |               arxiv                |                                                  [LLM Internal States Reveal Hallucination Risk Faced With a Query](https://arxiv.org/abs/2407.03282)                                                  |                              **Hallucination Detection**&**Uncertainty Estimation**                              |
| 24.07 |                                                                              Harbin Institute of Technology                                                                             |       ICLR 2024 AGI Workshop       |                               [Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models](https://arxiv.org/abs/2407.00569)                                |               **Vision-Language Models**&**Multimodal Hallucination**&**Residual Visual Decoding**               |
| 24.07 |                                                                              Harbin Institute of Technology                                                                             |              ACL 2024              |                               [Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models](https://arxiv.org/abs/2407.00569)                                |                       **Multimodal Hallucinations**&**LVLMs**&**Residual Visual Decoding**                       |
| 24.07 |                                                                                  University of Amsterdam                                                                                |               arxiv                |                                           [Leveraging Graph Structures to Detect Hallucinations in Large Language Models](https://arxiv.org/abs/2407.04485)                                            |                **Hallucination Detection**&**Graph Attention Network**&**Large Language Models**                 |
| 24.07 |                                                                                      Cisco Research                                                                                     |               arxiv                |                                                                         [Code Hallucination](https://arxiv.org/abs/2407.04831)                                                                         |                           **Code Hallucination**&**Generative Models**&**HallTrigger**                           |
| 24.07 |                                                                                Beijing Jiaotong University                                                                              |               arxiv                |                               [KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge Graph-based False Premise Questions](https://arxiv.org/abs/2407.05868)                               |                   **Factuality Hallucination**&**Knowledge Graph**&**False Premise Questions**                   |
| 24.07 |                                                                          University of California, Santa Barbara                                                                        |               arxiv                |                             [DebUnc: Mitigating Hallucinations in Large Language Model Agent Communication with Uncertainty Estimations](https://arxiv.org/abs/2407.06426)                             |                      **Hallucinations**&**Uncertainty Estimations**&**Multi-agent Systems**                      |
| 24.07 |                                                                           Massachusetts Institute of Technology                                                                         |               arxiv                |                        [Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps](https://arxiv.org/abs/2407.07071)                        |                                 **Contextual Hallucinations**&**Attention Maps**                                 |
| 24.07 |                                                                          University of Illinois Urbana-Champaign                                                                        |               arxiv                |                                         [Knowledge Overshadowing Causes Amalgamated Hallucination in Large Language Models](https://arxiv.org/abs/2407.08039)                                          |                                  **Knowledge Overshadowing**&**Hallucination**                                   |
| 24.07 |                                                                                        Patronus AI                                                                                      |               arxiv                |                                                        [Lynx: An Open Source Hallucination Evaluation Model](https://arxiv.org/abs/2407.08488)                                                         |                             **Hallucination Detection**&**RAG**&**Evaluation Model**                             |
| 24.07 |                                                                               Shanghai Jiao Tong University                                                                             |               arxiv                |                                                        [On the Universal Truthfulness Hyperplane Inside LLMs](https://arxiv.org/abs/2407.08582)                                                        |                                  **Truthfulness Hyperplane**&**Hallucination**                                   |
| 24.07 |                                                                                  University of Michigan                                                                                 |           ACL 2024 ALVR            |                                                 [Multi-Object Hallucination in Vision-Language Models](https://multi-object-hallucination.github.io/)                                                  |                **Multi-Object Hallucination**&**Vision-Language Models**&**Evaluation Protocol**                 |
| 24.07 |                                                                                        ASAPP, Inc.                                                                                      |         ACL 2024 Findings          |                             [Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses](https://arxiv.org/abs/2407.05474)                             |                       **Hallucination Detection**&**Synthetic Data**&**System Responses**                        |
| 24.07 |                                                                                          FAR AI                                                                                         |             COLM 2024              |                                                      [Transformer Circuit Faithfulness Metrics Are Not Robust](https://arxiv.org/abs/2407.08734)                                                       |                      **Transformer Circuits**&**Ablation Studies**&**Faithfulness Metrics**                      |
| 24.07 |                                                                       University of Science and Technology of China                                                                     |               arxiv                |                               [Detect, Investigate, Judge and Determine: A Novel LLM-based Framework for Few-shot Fake News Detection](https://arxiv.org/abs/2407.08952)                               |                                             **Fake News Detection**                                              |
| 24.07 |                                                                                    Tsinghua University                                                                                  |               arxiv                |                                                   [Mitigating Entity-Level Hallucination in Large Language Models](https://arxiv.org/abs/2407.09417)                                                   |                              **Hallucination**&**Retrieval Augmented Generation**&                               |
| 24.07 |                                                                                    Amazon Web Services                                                                                  |               arxiv                |                                                    [On Mitigating Code LLM Hallucinations with API Documentation](https://arxiv.org/abs/2407.09726)                                                    |                   **API Hallucinations**&**Code LLMs**&**Documentation Augmented Generation**                    |
| 24.07 |                                                                             Technical University of Darmstadt                                                                           |               arxiv                |                                        [Fine-grained Hallucination Detection and Mitigation in Long-form Question Answering](https://arxiv.org/abs/2407.11930)                                         |                         **Hallucination Detection**&**Error Annotation**&**Factuality**                          |
| 24.07 |                                                                                   Heidelberg University                                                                                 |               arxiv                |                                                        [Truth is Universal: Robust Detection of Lies in LLMs](https://arxiv.org/abs/2407.12831)                                                        |                           **Lie Detection**&**Activation Vectors**&**Truth Direction**                           |
| 24.07 |                                                                               Shanghai Jiao Tong University                                                                             |               arxiv                |                                                             [HALU-J: Critique-Based Hallucination Judge](https://arxiv.org/abs/2407.12943)                                                             |              **Hallucination Detection**&**Critique-Based Evaluation**&**Evidence Categorization**               |
| 24.07 |                                                                         TH K√∂ln ‚Äì University of Applied Sciences                                                                        |             CLEF 2024              |                                 [The Two Sides of the Coin: Hallucination Generation and Detection with LLMs as Evaluators for LLMs](https://arxiv.org/abs/2407.13757)                                 |                 **Hallucination Generation**&**Hallucination Detection**&**Multilingual Models**                 |
| 24.07 |                                                                                          POSTECH                                                                                        |             ECCV 2024              |                                      [BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models](https://arxiv.org/abs/2407.13442)                                      |                                   **Hallucination**&**Vision-Language Models**                                   |
| 24.07 |                                                                                 University College London                                                                               |               arxiv                |                            [Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models](https://arxiv.org/abs/2407.16470)                             |                               **Machine Translation**&**Hallucination Detection**                                |
| 24.07 |                                                                                    Cornell University                                                                                   |               arxiv                |                                     [WILDHALLUCINATIONS: Evaluating Long-form Factuality in LLMs with Real-World Entity Queries](https://arxiv.org/abs/2407.17468)                                     |                     **WildHallucinations**&**Factuality Evaluation**&**Real-World Entities**                     |
| 24.07 |                                                                                    Columbia University                                                                                  |             ECCV 2024              |                                            [HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning](https://arxiv.org/abs/2407.15680)                                            |                            **Hallucination**&**Vision-Language Models**&**Datasets**                             |
| 24.07 |                                                                                       IBM Research                                                                                      |         ICML 2024 Workshop         |                                                      [Generation Constraint Scaling Can Mitigate Hallucination](https://arxiv.org/abs/2407.16908)                                                      |                                  **Hallucination**&**Memory-Augmented Models**                                   |
| 24.07 |                                                                                        Harvard-MIT                                                                                      |               arxiv                | [The Need for Guardrails with Large Language Models in Medical Safety-Critical Settings: An Artificial Intelligence Application in the Pharmacovigilance Ecosystem](https://arxiv.org/abs/2407.18322)  |                               **Pharmacovigilance**&**Drug Safety**&**Guardrails**                               |
| 24.07 |                                                                             Illinois Institute of Technology                                                                            |               arxiv                |                                                                   [Can Editing LLMs Inject Harm?](https://arxiv.org/abs/2407.20224)                                                                    |                      **Knowledge Editing**&**Misinformation Injection**&**Bias Injection**                       |
| 24.07 |                                                                                    Stanford University                                                                                  |               arxiv                |                                      [Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models](https://arxiv.org/abs/2407.21417)                                      |                        **Instruction Following**&**Faithfulness**&**Multi-task Learning**                        |
| 24.07 |                                                                                     Jilin University                                                                                    |            ACM MM 2024             |                                             [Harmfully Manipulated Images Matter in Multimodal Misinformation Detection](https://arxiv.org/abs/2407.19192)                                             |                                  **Social media**&**Misinformation detection**                                   |
| 24.07 |                                                                                    Zhejiang University                                                                                  |            COLING 2024             |                            [Improving Faithfulness of Large Language Models in Summarization via Sliding Generation and Self-Consistency](https://arxiv.org/abs/2407.21443)                            |                                        **Summarization**&**Faithfulness**                                        |
| 24.08 |                                                                       Huazhong University of Science and Technology                                                                     |               arxiv                |                                               [Mitigating Multilingual Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2408.00550)                                                |            **Large Vision-Language Models**&**Multilingual Hallucination**&**Supervised Fine-tuning**            |
| 24.08 |                                                                       Huazhong University of Science and Technology                                                                     |               arxiv                |                                    [Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation](https://arxiv.org/abs/2408.00555)                                    |              **Hallucination**&**Vision-Language Models (VLMs)**&**Active Retrieval Augmentation**               |
| 24.08 |                                                                                           DFKI                                                                                          |       UbiComp Companion '24        |                                                  [Misinforming LLMs: Vulnerabilities, Challenges and Opportunities](https://arxiv.org/abs/2408.01168)                                                  |                                      **Misinformation**&**Trustworthy AI**                                       |
| 24.08 |                                                                                    Bar Ilan University                                                                                  |               arxiv                |                             [Mitigating Hallucinations in Large Vision-Language Models (LVLMs) via Language-Contrastive Decoding (LCD)](https://arxiv.org/abs/2408.04664)                              |           **Large Vision-Language Models**&**Object Hallucinations**&**Language-Contrastive Decoding**           |
| 24.08 |                                                                                  University of Liverpool                                                                                |               arxiv                |                           [Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models](https://arxiv.org/abs/2408.05093)                           |                          **Hallucination**&**Reasoning Order**&**Reflexive Prompting**                           |
| 24.08 |                                                                                 The Alan Turing Institute                                                                               |               arxiv                |                            [Large Language Models Can Consistently Generate High-Quality Content for Election Disinformation Operations](https://arxiv.org/abs/2408.06731)                             |                                 **Election Disinformation**&**DisElect Dataset**                                 |
| 24.08 |                                                                                      Google DeepMind                                                                                    |             COLM 2024              |                                [Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability](https://arxiv.org/abs/2408.07852)                                 |                                      **Knowledge Graph**&**Hallucinations**                                      |
| 24.08 |                                                                           The Hong Kong Polytechnic University                                                                          |               arxiv                |                                         [MegaFake: A Theory-Driven Dataset of Fake News Generated by Large Language Models](https://arxiv.org/abs/2408.11871)                                          |                                        **Fake News**&**MegaFake Dataset**                                        |
| 24.08 |                                                                                       IIT Kharagpur                                                                                     |               arxiv                |                                         [Evidence-backed Fact Checking using RAG and Few-Shot In-Context Learning with LLMs](https://arxiv.org/abs/2408.12060)                                         |                                **Fact Checking**&**RAG**&**In-Context Learning**                                 |
| 24.08 |                                                                                     Fudan University                                                                                    |               arxiv                |                               [Improving Factuality in Large Language Models via Decoding-Time Hallucinatory and Truthful Comparators](https://arxiv.org/abs/2408.12325)                               |              **Factuality Improvement**&**Hallucination Mitigation**&**Decoding-Time Intervention**              |
| 24.08 |                                                                                  The University of Tokyo                                                                                |               arxiv                |                                     [Interactive DualChecker for Mitigating Hallucinations in Distilling Large Language Models](https://arxiv.org/abs/2408.12326)                                      |                **Hallucination Mitigation**&**Knowledge Distillation**&**Large Language Models**                 |
| 24.08 |                                                                                   University of Surrey                                                                                  |             IJCAI 2024             |                                               [CodeMirage: Hallucinations in Code Generated by Large Language Models](https://arxiv.org/abs/2408.08333)                                                |                                  **Code Hallucinations**&**CodeMirage Dataset**                                  |
| 24.08 |                                                                                 Sichuan Normal University                                                                               |               arxiv                |                             [Can LLM Be a Good Path Planner Based on Prompt Engineering? Mitigating the Hallucination for Path Planning](https://arxiv.org/abs/2408.13184)                             |                       **Path Planning**&**Spatial Reasoning**&**Hallucination Mitigation**                       |
| 24.08 |                                                                                       Alibaba Cloud                                                                                     |               arxiv                |                              [LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via Layer-wise Relevance Propagation](https://arxiv.org/abs/2408.15533)                              |                     **Hallucination Detection**&**RAG**&**Layer-wise Relevance Propagation**                     |
| 24.08 |                                                                           Royal Holloway, University of London                                                                          |               arxiv                |                                              [Logic-Enhanced Language Model Agents for Trustworthy Social Simulations](https://arxiv.org/abs/2408.16081)                                               |                            **Social Simulations**&**Trustworthy AI**&**Game Theory**                             |
| 24.09 |                                                                                Inria, University of Rennes                                                                              |               arxiv                |                                                       [LLMs hallucinate graphs too: a structural perspective](https://arxiv.org/abs/2409.00159)                                                        |                                       **Hallucination**&**Graph Analysis**                                       |
| 24.09 |                                                                                         Scale AI                                                                                        |               arxiv                |                                           [Pre-Training Multimodal Hallucination Detectors with Corrupted Grounding Data](https://arxiv.org/abs/2409.00238)                                            |                      **Multimodal Hallucination**&**Grounding Data**&**Sample Efficiency**                       |
| 24.09 |                                                                                     Fudan University                                                                                    |               arxiv                |                        [LLM-GAN: Construct Generative Adversarial Network Through Large Language Models For Explainable Fake News Detection](https://arxiv.org/abs/2409.01787)                         |                      **Explainable Fake News Detection**&**Generative Adversarial Network**                      |
| 24.09 |                                                                                    University of Oslo                                                                                   |               arxiv                |                                            [Hallucination Detection in LLMs: Fast and Memory-Efficient Fine-tuned Models](https://arxiv.org/abs/2409.02976)                                            |                      **Hallucination Detection**&**Memory Efficiency**&**Ensemble Models**                       |
| 24.09 |                                                                            Univ. Polytechnique Hauts-de-France                                                                          |               arxiv                |                                              [FIDAVL: Fake Image Detection and Attribution using Vision-Language Model](https://arxiv.org/abs/2409.03109)                                              |                **Fake Image Detection**&**Vision-Language Model**&**Synthetic Image Attribution**                |
| 24.09 |                                                                                           EPFL                                                                                          |               arxiv                |                                     [LLM Detectors Still Fall Short of Real World: Case of LLM-Generated Short News-Like Posts](https://arxiv.org/abs/2409.03291)                                      |                           **LLM Detectors**&**Disinformation**&**Adversarial Evasion**                           |
| 24.09 |                                                                  Geely Automobile Research Institute, Beihang University                                                                |               arxiv                |                                            [Alleviating Hallucinations in Large Language Models with Scepticism Modeling](https://arxiv.org/abs/2409.06601)                                            |                                    **Hallucinations**&**Scepticism Modeling**                                    |
| 24.09 |                                                                         AppCubic, Georgia Institute of Technology                                                                       |               arxiv                |                                        [Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks](https://arxiv.org/abs/2409.08087)                                         |                          **Misinformation**&**Jailbreak Attacks**&**Prompt Injection**                           |
| 24.09 |                                                                                Carnegie Mellon University                                                                               |               arxiv                |                                          [AI-LIEDAR: Examine the Trade-off Between Utility and Truthfulness in LLM Agents](https://arxiv.org/abs/2409.09013)                                           |                                   **Utility**&**Truthfulness**&**LLM Agents**                                    |
| 24.09 |                                                                                  Salesforce AI Research                                                                                 |               arxiv                |                                                            [SFR-RAG: Towards Contextually Faithful LLMs](https://arxiv.org/abs/2409.09916)                                                             |          **Retrieval Augmented Generation**&**Contextual Comprehension**&**Hallucination Minimization**          |
| 24.09 |                                                                                 University of North Texas                                                                               |               arxiv                |            [HALO: Hallucination Analysis and Learning Optimization to Empower LLMs with Retrieval-Augmented Context for Guided Clinical Decision Making](https://arxiv.org/abs/2409.10011)             |          **Hallucination Mitigation**&**Retrieval Augmented Generation**&**Medical Question Answering**          |
| 24.09 |                                                                                    Tsinghua University                                                                                  |               arxiv                |                                                [Trustworthiness in Retrieval-Augmented Generation Systems: A Survey](https://arxiv.org/abs/2409.10102)                                                 |                                           **Trustworthiness**&**RAG**                                            |
| 24.09 |                                                                         National University of Defense Technology                                                                       |               arxiv                |                          [Zero-resource Hallucination Detection for Text Generation via Graph-based Contextual Knowledge Triples Modeling](https://arxiv.org/abs/2409.11283)                           |         **Zero-resource Hallucination Detection**&**Text Generation**&**Graph-based Knowledge Triples**          |
| 24.09 |                                                                               The University of Manchester                                                                              |               arxiv                |                                            [FMDLlama: Financial Misinformation Detection based on Large Language Models](https://arxiv.org/abs/2409.16452)                                             |                    **Financial Misinformation Detection**&**Instruction Tuning**&**FMDLlama**                    |
| 24.09 |                                                                                  University of Montreal                                                                                 |               arxiv                |                                         [From Deception to Detection: The Dual Roles of Large Language Models in Fake News](https://arxiv.org/abs/2409.17416)                                          |                            **Fake News**&**Fake News Detection**&**Bias Mitigation**                             |
| 24.09 |                                                                                     Korea University                                                                                    |        EMNLP 2024 Findings         |                          [Pre-trained Language Models Return Distinguishable Probability Distributions to Unfaithfully Hallucinated Texts](https://arxiv.org/abs/2409.16658)                           |                  **Hallucination Detection**&**Unfaithful Texts**&**Uncertainty Distribution**                   |
| 24.10 |                                                                             The University of Texas at Dallas                                                                           |                TMLR                |                                      [A Unified Hallucination Mitigation Framework for Large Vision-Language Models](https://openreview.net/forum?id=ZVDWzgk6L6)                                       |                  **Hallucination Mitigation**&**Vision-Language Models**&**Reasoning Queries**                   |
| 24.09 |                                                                            The Chinese University of Hong Kong                                                                          |               arxiv                |                                                          [A Survey on the Honesty of Large Language Models](https://arxiv.org/abs/2409.18786)                                                          |                              **LLM Honesty**&**Self-knowledge**&**Self-expression**                              |
| 24.09 |                                                                                   University of Surrey                                                                                  |               arxiv                |                                        [MEDHALU: Hallucinations in Responses to Healthcare Queries by Large Language Models](https://arxiv.org/abs/2409.19492)                                         |                    **LLM Hallucinations**&**Healthcare Queries**&**Hallucination Detection**                     |
| 24.09 |                                                                                  Harvard Medical School                                                                                 |               arxiv                |                   [Wait, but Tylenol is Acetaminophen‚Ä¶ Investigating and Improving Language Models' Ability to Resist Requests for Misinformation](https://arxiv.org/abs/2409.20385)                   |                     **LLM Misinformation Resistance**&**Healthcare**&**Instruction Tuning**                      |
| 24.09 |                                                                    Nanjing University of Aeronautics and Astronautics                                                                   |               arxiv                |                          [HELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding](https://arxiv.org/abs/2409.20429)                          |                           **Hallucination Mitigation**&**LVLMs**&**Feedback Learning**                           |
| 24.09 |                                                                                  Sun Yat-sen University                                                                                 |               arxiv                |                                       [LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation](https://arxiv.org/abs/2409.20550)                                        |                       **LLM Hallucinations**&**Code Generation**&**Mitigation Strategies**                       |
| 24.10 |                                                                                         Technion                                                                                        |               arxiv                |                                        [LLMS KNOW MORE THAN THEY SHOW: ON THE INTRINSIC REPRESENTATION OF LLM HALLUCINATIONS](https://arxiv.org/abs/2410.02707)                                        |                       **LLM Hallucinations**&**Error Detection**&**Truthfulness Encoding**                       |
| 24.10 |                                                                                           Meta                                                                                          |               arxiv                |                                       [Ingest-And-Ground: Dispelling Hallucinations from Continually-Pretrained LLMs with RAG](https://arxiv.org/abs/2410.02825)                                       |                                            **RAG**&**Hallucination**                                             |
| 24.10 |                                                                                       IBM Research                                                                                      |               arxiv                |                                       [ST-WebAgentBench: A Benchmark for Evaluating Safety and Trustworthiness in Web Agents](https://arxiv.org/abs/2410.06703)                                        |                                  **Web Agents**&**Safety**&**Trustworthiness**                                   |
| 24.10 |                                                                             National University of Singapore                                                                            |               arxiv                |                      [Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study Over Open-ended Question Answering](https://arxiv.org/abs/2410.08085)                       |                                     **Knowledge Graphs**&**Trustworthiness**                                     |
| 24.10 |                                                                                     Tongji University                                                                                   |             EMNLP 2024             |                                          [DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination](https://arxiv.org/abs/2410.04514)                                           |                            **LVLM**&**Object Hallucination**&**Attention Mechanism**                             |
| 24.10 |                                                                   The University of Sydney, The University of Hong Kong                                                                 |               arxiv                |                                         [NOVO: Norm Voting Off Hallucinations with Attention Heads in Large Language Models](https://arxiv.org/abs/2410.08970)                                         |                         **Hallucination mitigation**&**Attention heads**&**Norm voting**                         |
| 24.10 |                                                                                     Purdue University                                                                                   |               arxiv                |                                           [COLLU-BENCH: A Benchmark for Predicting Language Model Hallucinations in Code](https://arxiv.org/abs/2410.09997)                                            |                     **Code hallucinations**&**Code generation**&**Automated program repair**                     |
| 24.10 |                                 National University of Sciences and Technology, Rawalpindi Medical University, King Faisal University, Sejong University                                |               arxiv                |               [Mitigating Hallucinations Using Ensemble of Knowledge Graph and Vector Store in Large Language Models to Enhance Mental Health Support](https://arxiv.org/abs/2410.10853)               |                   **Hallucination mitigation**&**Knowledge graphs**&**Mental health support**                    |
| 24.10 |                                       Renmin University of China, Kuaishou Technology Co., Ltd., University of International Business and Economics                                     |             ICLR 2025              |                                 [ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability](https://arxiv.org/abs/2410.11414)                                 |      **Retrieval-Augmented Generation (RAG)**&**Hallucination detection**&**Mechanistic interpretability**       |
| 24.10 |                                                                   Zhejiang University, National University of Singapore                                                                 |               arxiv                |                                               [MLLM CAN SEE? Dynamic Correction Decoding for Hallucination Mitigation](https://arxiv.org/abs/2410.11779)                                               |                 **Hallucination mitigation**&**Multimodal LLMs**&**Dynamic correction decoding**                 |
| 24.10 |             Vectara, Inc., Iowa State University, University of Southern California, Entropy Technologies, University of Waterloo, Funix.io, University of Wisconsin, Madison           |               arxiv                |                                           [FaithBench: A Diverse Hallucination Benchmark for Summarization by Modern LLMs](https://arxiv.org/abs/2410.13210)                                           |                    **Hallucination detection**&**Human-annotated benchmark**&**Faithfulness**                    |
| 24.10 |                                                                  Harbin Institute of Technology (Shenzhen), Huawei Cloud                                                                |               arxiv                |                                      [MEDICO: Towards Hallucination Detection and Correction with Multi-source Evidence Fusion](https://arxiv.org/abs/2410.10408)                                      |            **Hallucination detection**&**Multi-source evidence fusion**&**Hallucination correction**             |
| 24.10 |                                                                                  Independent Researchers                                                                                |       KDD 2024 RAG Workshop        |                              [Honest AI: Fine-Tuning "Small" Language Models to Say "I Don‚Äôt Know", and Reducing Hallucination in RAG](https://arxiv.org/abs/2410.09699)                               |                           **Hallucination reduction**&**Small LLMs**&**False premise**                           |
| 24.10 |                                                                              University of California Irvine                                                                            |               arxiv                |                                             [From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization](https://arxiv.org/abs/2410.13961)                                             |                     **Multi-Document Summarization**&**LLM Hallucination**&**Benchmarking**                      |
| 24.10 |                                                                                    Harvard University                                                                                   |               arxiv                |                                             [Good Parenting is All You Need: Multi-agentic LLM Hallucination Mitigation](https://arxiv.org/abs/2410.14262)                                             |                        **LLM Hallucination**&**Multi-agent Systems**&**Self-reflection**                         |
| 24.10 |                                                                       University of Science and Technology of China                                                                     |               arxiv                |                                       [Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models](https://arxiv.org/abs/2410.15116)                                       |         **Knowledge Hallucination**&**Retrieval-Augmented Language Models**&**Highlighting Techniques**          |
| 24.10 |                                                                                     McGill University                                                                                   |               arxiv                |                                       [Hallucination Detox: Sensitive Neuron Dropout (SEND) for Large Language Model Training](https://arxiv.org/abs/2410.15460)                                       |                    **Hallucination Mitigation**&**Sensitive Neurons**&**Training Protocols**                     |
| 24.10 |                                                                                National Taiwan University                                                                               |               arxiv                |                    [Can Large Audio-Language Models Truly Hear? Tackling Hallucinations with Multi-Task Assessment and Stepwise Audio Reasoning](https://arxiv.org/abs/2410.16130)                     |                  **Audio-Language Models**&**Hallucination Analysis**&**Multi-Task Evaluation**                  |
| 24.10 |                                                                                Mila - Quebec AI Institute                                                                               |               arxiv                |                                                      [Multilingual Hallucination Gaps in Large Language Models](https://arxiv.org/abs/2410.18270)                                                      |                     **Multilingual Hallucination**&**FACTSCORE**&**Low-Resource Languages**                      |
| 24.10 |                                                                                  University of Edinburgh                                                                                |               arxiv                |                                             [DECORE: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations](https://arxiv.org/abs/2410.18860)                                             |                    **Hallucination Mitigation**&**Contrastive Decoding**&**Retrieval Heads**                     |
| 24.10 |                                                                       University of Science and Technology of China                                                                     |        EMNLP 2024 Findings         |                           [Mitigating Hallucinations of Large Language Models in Medical Information Extraction via Contrastive Decoding](https://arxiv.org/abs/2410.15702)                            |             **Hallucination Mitigation**&**Medical Information Extraction**&**Contrastive Decoding**             |
| 24.10 |                                                                       University of Science and Technology of China                                                                     |             ICML 2024              |                                   [Trustworthy Alignment of Retrieval-Augmented Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2410.16843)                                    |             **Trustworthy Alignment**&**Reinforcement Learning**&**Retrieval-Augmented Generation**              |
| 24.10 |                                                                                        Intel Labs                                                                                       | NeurIPS 2024 Workshop on SafeGenAI |                                                         Debiasing Large Vision-Language Models by Ablating Protected Attribute Representations                                                         |                         **Debiasing**&**Vision-Language Models**&**Attribute Ablation**                          |
| 24.10 |                                                                             The Pennsylvania State University                                                                           |               arxiv                |                          [The Reopening of Pandora‚Äôs Box: Analyzing the Role of LLMs in the Evolving Battle Against AI-Generated Fake News](https://arxiv.org/abs/2410.19250)                          |                                **Fake News Detection**&**Human-AI Collaboration**                                |
| 24.10 |                                                                                  Stellenbosch University                                                                                |               arxiv                |                               [Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models](https://arxiv.org/abs/2410.19385)                               |                      **Hallucination Mitigation**&**Prompt Engineering**&**External Tools**                      |
| 24.10 |                                                                                   Algoverse AI Research                                                                                 |               arxiv                |                                                   [A Debate-Driven Experiment on LLM Hallucinations and Accuracy](https://arxiv.org/abs/2410.19485)                                                    |                      **LLM Hallucinations**&**Accuracy Improvement**&**Model Interaction**                       |
| 24.10 |                                                                                       Narrative BI                                                                                      |               arxiv                |                         [Beyond Fine-Tuning: Effective Strategies for Mitigating Hallucinations in Large Language Models for Data Analytics](https://arxiv.org/abs/2410.20024)                         |                      **Hallucination Mitigation**&**Data Analytics**&**Prompt Engineering**                      |
| 24.10 |                                                                                        HKUST (GZ)                                                                                       |               arxiv                |                          [Maintaining Informative Coherence: Migrating Hallucinations in Large Language Models via Absorbing Markov Chains](https://arxiv.org/abs/2410.20340)                          |                                  **Hallucination Mitigation**&**Markov Chains**                                  |
| 24.10 |                                                                                National Taiwan University                                                                               |               arxiv                |                                            [LLMs are Biased Evaluators But Not Biased for Retrieval Augmented Generation](https://arxiv.org/abs/2410.20833)                                            |                     **Bias Analysis**&**LLM Evaluation**&**Retrieval-Augmented Generation**                      |
| 24.10 |                                                                                University Hospital Leipzig                                                                              |               arxiv                |                                               [LLM Robustness Against Misinformation in Biomedical Question Answering](https://arxiv.org/abs/2410.21330)                                               |                       **Biomedical Question Answering**&**Robustness**&**Misinformation**                        |
| 24.10 |                                                                         Technion ‚Äì Israel Institute of Technology                                                                       |               arxiv                |                                                     [Distinguishing Ignorance from Error in LLM Hallucinations](https://arxiv.org/abs/2410.22071)                                                      |                     **LLM Hallucinations**&**Error Classification**&**Knowledge Detection**                      |
| 24.10 |                                                                    The Hong Kong University of Science and Technology                                                                   |               arxiv                |                                          [Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models](https://arxiv.org/abs/2410.23114)                                           |                  **Vision-Language Models**&**Hallucination Evaluation**&**Relation Analysis**                   |
| 24.10 |                                                           University of Notre Dame, MBZUAI, IBM Research, UW, Peking University                                                         |               arxiv                |                                                     [Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge](https://arxiv.org/abs/2410.02736)                                                     |                                                     **Bias**                                                     |
| 24.11 |                                                                                    New York University                                                                                  |               arxiv                |                    [Exploring the Knowledge Mismatch Hypothesis: Hallucination Propensity in Small Models Fine-tuned on Data from Larger Models](https://arxiv.org/abs/2411.00878)                     |                             **Hallucination**&**Knowledge Mismatch**&**Fine-tuning**                             |
| 24.11 |                                                                                     Nankai University                                                                                   |               arxiv                |                                         [Prompt-Guided Internal States for Hallucination Detection of Large Language Models](https://arxiv.org/abs/2411.04847)                                         |          **Hallucination Detection**&**Prompt-Guided Internal States**&**Cross-Domain Generalization**           |
| 24.11 |                                                                              Georgia Institute of Technology                                                                            |               arXiv                |                                                     [LLM Hallucination Reasoning with Zero-shot Knowledge Test](https://arxiv.org/abs/2411.09689)                                                      |                    **Hallucination Detection**&**Zero-shot Methods**&**Model Knowledge Test**                    |
| 24.11 |                                                                               Shanghai Jiao Tong University                                                                             |               arxiv                |                                     [Seeing Clearly by Layer Two: Enhancing Attention Heads to Alleviate Hallucination in LVLMs](https://arxiv.org/abs/2411.09968)                                     |                  **Multimodal Large Language Models**&**Hallucination**&**Attention Mechanism**                  |
| 24.11 |                                                                                Renmin University of China                                                                               |               arxiv                |                       [Mitigating Hallucination in Multimodal Large Language Models via Hallucination-targeted Direct Preference Optimization](https://arxiv.org/abs/2411.10436)                       |       **Multimodal Large Language Models**&**Hallucination Mitigation**&**Direct Preference Optimization**       |
| 24.11 |                                                                                           AIRI                                                                                          |               arxiv                |                               [Addressing Hallucinations in Language Models with Knowledge Graph Embeddings as an Additional Modality](https://arxiv.org/abs/2411.11531)                               |                                **Hallucination Mitigation**&**Knowledge Graphs**                                 |
| 24.11 |                                                                                University of Pennsylvania                                                                               |               arxiv                |                                  [Thinking Before Looking: Improving Multimodal LLM Reasoning via Mitigating Visual Hallucination](https://arxiv.org/abs/2411.12591)                                   |               **Multimodal Large Language Models**&**Visual Hallucination**&**Reasoning Accuracy**               |
| 24.11 |                                                                                    Tsinghua University                                                                                  |               arxiv                |                                 [CATCH: Complementary Adaptive Token-level Contrastive Decoding to Mitigate Hallucinations in LVLMs](https://arxiv.org/abs/2411.12713)                                 |              **Large Vision-Language Models**&**Hallucination Mitigation**&**Contrastive Decoding**              |
| 24.11 |                                                                                  Stony Brook University                                                                                 |               arxiv                |                                  [A Novel Approach to Eliminating Hallucinations in Large Language Model-Assisted Causal Discovery](https://arxiv.org/abs/2411.12759)                                  |                                      **Hallucination**&**Causal Discovery**                                      |
| 24.11 |                                                                                        ETH Z√ºrich                                                                                       |               arxiv                |                                          [Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models](https://arxiv.org/abs/2411.14257)                                          |                        **Hallucinations**&**Knowledge Awareness**&**Sparse Autoencoders**                        |
| 24.11 |                                                                                    Aalborg University                                                                                   |               arxiv                |                                          [Knowledge Graphs, Large Language Models, and Hallucinations: An NLP Perspective](https://arxiv.org/abs/2411.14258)                                           |                                     **Knowledge Graphs**&**Hallucinations**                                      |
| 24.11 |                                                             China Telecom Shanghai Company, Ferret Relationship Intelligence                                                            |               arxiv                |         [Enhancing Multi-Agent Consensus through Third-Party LLM Integration: Analyzing Uncertainty and Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2411.16189)          |                  **Multi-Agent Systems**&**Hallucination Mitigation**&**Uncertainty Analysis**                   |
| 24.12 |                                                                  Salah Boubnider University, Abdelhamid Mehri University                                                                |               arxiv                |                                                 [An Evolutionary Large Language Model for Hallucination Mitigation](https://arxiv.org/abs/2412.02790)                                                  |                            **Evolutionary Computation**&**Hallucination Mitigation**                             |
| 24.11 |                                                                          Adobe Research, IIT Kanpur, IIT Bombay                                                                         |               arxiv                |                                  [Beyond Logit Lens: Contextual Embeddings for Robust Hallucination Detection & Grounding in VLMs](https://arxiv.org/abs/2411.19187)                                   |                   **Hallucination Detection**&**Contextual Embeddings**&**Multimodal Models**                    |
| 24.11 |                                              Tsinghua University, Tencent, University of Science and Technology Beijing, University of Macau                                            |               arxiv                |                                  [DHCP: Detecting Hallucinations by Cross-modal Attention Patterns in Large Vision-Language Models](https://arxiv.org/abs/2411.18659)                                  |                 **Hallucination Detection**&**Cross-modal Attention**&**Vision-Language Models**                 |
| 24.12 |                                                                                       Acurai, Inc.                                                                                      |               arxiv                |                                                               [Hallucination Elimination Using Acurai](https://arxiv.org/abs/2412.05223)                                                               |                                **Hallucination Detection**&**AI Trustworthiness**                                |
| 24.12 |                                                                    National Yang Ming Chiao Tung University, Atmanity                                                                   |               arxiv                |                                [Delve into Visual Contrastive Decoding for Hallucination Mitigation of Large Vision-Language Models](https://arxiv.org/abs/2412.06775)                                 |             **Visual Contrastive Decoding**&**Hallucination Mitigation**&**Vision-Language Models**              |
| 24.12 |                                                                              Amazon Alexa AI, Cambridge, UK                                                                             |               arxiv                |                                                    [HALLUCANA: Fixing LLM Hallucination with A Canary Lookahead](https://arxiv.org/abs/2412.07965)                                                     |                      **Hallucination Detection**&**Lookahead Strategy**&**LLM Factuality**                       |
| 24.12 |                                                                                     Peking University                                                                                   |               arxiv                |                                           [Dehallucinating Parallel Context Extension for Retrieval-Augmented Generation](https://arxiv.org/abs/2412.14905)                                            |    **In-Context Hallucination**&**Retrieval-Augmented Generation (RAG)**&**Parallel Context Extension (PCE)**    |
| 24.12 |                                                                                         AE Studio                                                                                       |  NeurIPS 2024 SafeGenAI Workshop   |                                                  [Towards Safe and Honest AI Agents with Neural Self-Other Overlap](https://arxiv.org/abs/2412.16325)                                                  |                  **AI Safety**&**Neural Self-Other Overlap**&**Deceptive Behavior Mitigation**                   |
| 24.12 |                                                                                           EPFL                                                                                          |               arxiv                |                                                           [Trustworthy and Efficient LLMs Meet Databases](https://arxiv.org/abs/2412.18022)                                                            |                     **Trustworthy LLMs**&**Efficient LLM Inference**&**LLMs and Databases**                      |
| 24.12 |                                                                                   University of M√ºnster                                                                                 |               arxiv                |                          [The HalluRAG Dataset: Detecting Closed-Domain Hallucinations in RAG Applications Using an LLM‚Äôs Internal States](https://arxiv.org/abs/2412.17056)                           |             **Hallucination Detection**&**Retrieval-Augmented Generation (RAG)**&**Internal States**             |
| 24.12 |                                                                                     Purdue University                                                                                   |               arxiv                |                                   [The Reliability Paradox: Exploring How Shortcut Learning Undermines Language Model Calibration](https://arxiv.org/abs/2412.15269)                                   |                                      **Calibration**&**Shortcut Learning**                                       |
| 24.12 |                                                                                         IIT Patna                                                                                       |               arxiv                |                                       [From Hallucinations to Facts: Enhancing Language Models with Curated Knowledge Graphs](https://arxiv.org/abs/2412.18672)                                        |                                **Hallucination Mitigation**&**Knowledge Graphs**                                 |
| 25.01 |                                                                                 Brunel University London                                                                                |               arxiv                |                                       [A Review of Faithfulness Metrics for Hallucination Assessment in Large Language Models](https://arxiv.org/abs/2501.00269)                                       |                **Faithfulness Evaluation**&**Hallucination Assessment**&**Large Language Models**                |
| 25.01 |                                                                      University of Notre Dame, Deloitte & Touche LLP                                                                    |             AAAI 2025              |                                                            [Citations and Trust in LLM Generated Answers](https://arxiv.org/abs/2501.01303)                                                            |                           **User Trust**&**Citations in LLMs**&**Social Proof Theory**                           |
| 25.01 |                                                             Singapore Management University, National University of Singapore                                                           |               arxiv                |                                          [Aligning Large Language Models for Faithful Integrity Against Opposing Argument](https://arxiv.org/abs/2501.01336)                                           |               **Faithful Integrity**&**Confidence Estimation**&**Direct Preference Optimization**                |
| 25.01 |                                                                                    Harvard University                                                                                   |               arxiv                |                                               [Generalizing Trust: Weak-to-Strong Trustworthiness in Language Models](https://arxiv.org/abs/2501.00418)                                                |                **Trustworthiness Generalization**&**Weak-to-Strong Transfer**&**Language Models**                |
| 25.01 |                                                                                Carnegie Mellon University                                                                               |               arxiv                |                                            [Dynamics of Adversarial Attacks on Large Language Model-Based Search Engines](https://arxiv.org/abs/2501.00745)                                            |                         **Adversarial Attacks**&**Ranking Manipulation**&**Game Theory**                         |
| 25.01 |                                                                                  Imperial College London                                                                                |               arxiv                |                                                     [TrustRAG: Enhancing Robustness and Trustworthiness in RAG](https://arxiv.org/abs/2501.00879)                                                      |     **Robust Retrieval-Augmented Generation**&**Corpus Poisoning Defense**&**Knowledge Conflict Resolution**     |
| 25.01 |                                                                                Renmin University of China                                                                               |               arxiv                |                                 [Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process of Fast and Slow Thinking](https://arxiv.org/abs/2501.01306)                                 |                 **Hallucination Mitigation**&**Dual Process Theory**&**Monte Carlo Tree Search**                 |
| 25.01 |                                                                                    Tsinghua University                                                                                  |               arxiv                |                                        [A Survey on Responsible LLMs: Inherent Risk, Malicious Use, and Mitigation Strategy](https://arxiv.org/abs/2501.09431)                                         |                                **Responsible LLMs**&**Privacy**&**Hallucination**                                |
| 25.01 |                                                                       University of Science and Technology of China                                                                     |               arxiv                |                                                [Personality Modeling for Persuasion of Misinformation using AI Agent](https://arxiv.org/abs/2501.08985)                                                |                   **Personality Traits**&**Misinformation Dynamics**&**Agent-Based Modeling**                    |
| 25.01 |                                                                                 University of Washington                                                                                |               arxiv                |                                                    [HALOGEN: Fantastic LLM Hallucinations and Where to Find Them](https://arxiv.org/abs/2501.08292)                                                    |                      **Hallucination Benchmark**&**Automatic Verifiers**&**LLM Factuality**                      |
| 25.01 |                                                                                Chinese Academy of Sciences                                                                              |               arxiv                |                                  [Attention-guided Self-reflection for Zero-shot Hallucination Detection in Large Language Models](https://arxiv.org/abs/2501.09997)                                   |                          **Zero-shot Hallucination Detection**&**Attention Mechanisms**                          |
| 25.01 |                                                                                Carnegie Mellon University                                                                               |               arxiv                |                                          [A Hybrid Attention Framework for Fake News Detection with Large Language Models](https://arxiv.org/abs/2501.11967)                                           |                              **Fake News Detection**&**Hybrid Attention Mechanism**                              |
| 25.01 |                                                                                       Virginia Tech                                                                                     |               arxiv                |                                  [Fixing Imbalanced Attention to Mitigate In-Context Hallucination of Large Vision-Language Models](https://arxiv.org/abs/2501.12206)                                  |                 **Vision-Language Models**&**Hallucination Mitigation**&**Attention Mechanisms**                 |
| 25.01 |                                                                                     McGill University                                                                                   |               arxiv                |                                 [OnionEval: A Unified Evaluation of Fact-conflicting Hallucination for Small-Large Language Models](https://arxiv.org/abs/2501.12975)                                  |             **Fact-conflicting Hallucination**&**Small-Large Language Models (SLLMs)**&**Benchmark**             |
| 25.01 |                                                                              Harbin Institute of Technology                                                                             |               arxiv                |                                [Improving Contextual Faithfulness of Large Language Models via Retrieval Heads-Induced Optimization](https://arxiv.org/abs/2501.13573)                                 |           **Contextual Faithfulness**&**Retrieval-Augmented Models**&**Long-Form Question Answering**            |
| 25.01 |                                                                             Dresden University of Technology                                                                            |               arxiv                |                                                 [Hallucinations Can Improve Large Language Models in Drug Discovery](https://arxiv.org/abs/2501.13824)                                                 |                             **Hallucinations**&**Drug Discovery**&**SMILES Strings**                             |
| 25.01 |                                                                              China Telecom Shanghai Company                                                                             |               arxiv                |                                         [Prompt-Based Monte Carlo Tree Search for Mitigating Hallucinations in Large Models](https://arxiv.org/abs/2501.13942)                                         |                                  **Monte Carlo Tree Search**&**Hallucinations**                                  |
| 25.01 |                                                                            XCALLY, Linux Foundation AI & Data                                                                           |               arxiv                |                                            [Hallucination Mitigation Using Agentic AI Natural Language-Based Frameworks](https://arxiv.org/abs/2501.13946)                                             |                           **Agentic AI**&**LLM Hallucination**&**AI Interoperability**                           |
| 25.01 |                                                                                 Boston University, Apple                                                                                |             NAACL 2025             |                          [Mitigating Hallucinated Translations in Large Language Models with Hallucination-focused Preference Optimization](https://arxiv.org/abs/2501.17295)                          |                  **LLM Hallucinations**&**Preference Optimization**&**Contrastive Fine-tuning**                  |
| 25.01 |                                                                                  University of Waterloo                                                                                 |               arxiv                |                                              [Importing Phantoms: Measuring LLM Package Hallucination Vulnerabilities](https://arxiv.org/abs/2501.19012)                                               |                       **LLM Security**&**Package Hallucination**&**Software Supply Chain**                       |
| 25.02 |                                                                                    Shanghai University                                                                                  |               arxiv                |                              [Mitigating Hallucinations in Large Vision-Language Models with Internal Fact-based Contrastive Decoding](https://arxiv.org/abs/2502.01056)                               |                 **Hallucination Mitigation**&**Vision-Language Models**&**Contrastive Decoding**                 |
| 25.02 |                                                                                     Wuhan University                                                                                    |               arxiv                |                         [Topic-FlipRAG: Topic-Orientated Adversarial Opinion Manipulation Attacks to Retrieval-Augmented Generation Models](https://arxiv.org/abs/2502.01386)                          |               **Retrieval-Augmented Generation**&**Opinion Manipulation**&**Adversarial Attacks**                |
| 25.02 |                                                                                Leibniz University Hannover                                                                              |               arxiv                |                                     [SelfCheckAgent: Zero-Resource Hallucination Detection in Generative Large Language Models](https://arxiv.org/abs/2502.01812)                                      |                             **Hallucination Detection**&**Zero-Resource Evaluation**                             |
| 25.02 |                                                                                   University of Sydney                                                                                  |               arxiv                |                                     [Mitigating Object Hallucinations in Large Vision-Language Models via Attention Calibration](https://arxiv.org/abs/2502.01969)                                     |                  **Vision-Language Models**&**Object Hallucination**&**Attention Calibration**                   |
| 25.02 |                                                                                    Rutgers University                                                                                   |               arxiv                |                         [The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering](https://arxiv.org/abs/2502.03628)                          |                 **Vision-Language Models**&**Hallucination Mitigation**&**Token-Logit Analysis**                 |
| 25.02 |                                                                                   Qualcomm AI Research                                                                                  |               arxiv                |                                                     [Enhancing Hallucination Detection through Noise Injection](https://arxiv.org/abs/2502.03799)                                                      |                    **Hallucination Detection**&**Uncertainty Estimation**&**Noise Injection**                    |
| 25.02 |                                                                                   University of Toronto                                                                                 |               arxiv                |                               [MARAGE: Transferable Multi-Model Adversarial Attack for Retrieval-Augmented Generation Data Extraction](https://arxiv.org/abs/2502.04360)                               |                        **Retrieval-Augmented Generation**&**RAG**&**Adversarial Attack**                         |
| 25.02 |                                                                               Pennsylvania State University                                                                             |               arxiv                |                                               [TruthFlow: Truthful LLM Generation via Representation Flow Correction](https://arxiv.org/abs/2502.04556)                                                |                                 **Truthfulness**&**Representation Intervention**                                 |
| 25.02 |                                                                   National Taiwan University of Science and Technology                                                                  |               arxiv                |                                        [DELTA - Contrastive Decoding Mitigates Text Hallucinations in Large Language Models](https://arxiv.org/abs/2502.05825)                                         |                  **Contrastive Decoding**&**Text Hallucination Mitigation**&**LLM Reliability**                  |
| 25.02 |                                                                        Shanghai Artificial Intelligence Laboratory                                                                      |               arxiv                |                                   [GRAIT: Gradient-Driven Refusal-Aware Instruction Tuning for Effective Hallucination Mitigation](https://arxiv.org/abs/2502.05911)                                   |          **Refusal-Aware Instruction Tuning**&**Hallucination Mitigation**&**Gradient-Based Learning**           |
| 25.02 |                                                                    Fraunhofer Institute for Integrated Circuits (IIS)                                                                   |               arxiv                |                                                    [Transparent NLP: Using RAG and LLM Alignment for Privacy Q&A](https://arxiv.org/abs/2502.06652)                                                    |                    **Retrieval-Augmented Generation (RAG)**&**LLM Alignment**&**Privacy Q&A**                    |
| 25.02 |                                                                                   Vanderbilt University                                                                                 |               arxiv                |                                       [Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2502.06872)                                       |                   **Retrieval-Augmented Generation (RAG)**&**Trustworthy AI**&**LLM Security**                   |
| 25.02 |                                                                                    Tsinghua University                                                                                  |               arxiv                |                              [Aligning Large Language Models to Follow Instructions and Hallucinate Less via Effective Data Filtering](https://arxiv.org/abs/2502.07340)                               |                      **Instruction Tuning**&**Hallucination Reduction**&**Data Filtering**                       |
| 25.02 |                                                                                      Inha University                                                                                    |               arxiv                |                            [HUDEX: Integrating Hallucination Detection and Explainability for Enhancing the Reliability of LLM Responses](https://arxiv.org/abs/2502.08109)                            |                        **Hallucination Detection**&**Explainability**&**LLM Reliability**                        |
| 25.02 |                                                                             Huazhong Agricultural University                                                                            |               arxiv                |                                                    [Trustworthy GNNs with LLMs: A Systematic Review and Taxonomy](https://arxiv.org/abs/2502.08353)                                                    |                               **Graph Neural Networks (GNNs)**&**Trustworthiness**                               |
| 25.02 |                                                                                  Stony Brook University                                                                                 |               arxiv                |                                  [Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial Stance for Summary Evaluation](https://arxiv.org/abs/2502.08514)                                  |                       **Faithfulness Evaluation**&**Multi-Agent Debate**&**Summarization**                       |
| 25.02 |                                                                                 KAUST, University of Pisa                                                                               |               arxiv                |                                       [Hallucination Detection: A Probabilistic Framework Using Embeddings Distance Analysis](https://arxiv.org/abs/2502.08663)                                        |                      **Hallucination Detection**&**Embedding Distance**&**LLM Evaluation**                       |
| 25.02 |                                                                                University of Pennsylvania                                                                               |               arXiv                |                                              [Hallucination, Monofacts, and Miscalibration: An Empirical Investigation](https://arxiv.org/abs/2502.08666)                                              |                              **Hallucination**&**Monofact Rate**&**Miscalibration**                              |
| 25.02 |                                                                                    Tsinghua University                                                                                  |               arxiv                |                                [MIH-TCCT: Mitigating Inconsistent Hallucinations in LLMs via Event-Driven Text-Code Cyclic Training](https://arxiv.org/abs/2502.08904)                                 |                  **Hallucination Mitigation**&**Event-Driven Training**&**Logical Consistency**                  |
| 25.02 |                                                                             University of the Basque Country                                                                            |               arxiv                |                                                  [Truth Knows No Language: Evaluating Truthfulness Beyond English](https://arxiv.org/abs/2502.09387)                                                   |                        **Truthfulness Evaluation**&**Multilingual LLMs**&**Benchmarking**                        |
| 25.02 |                                                                                          Amazon                                                                                         |               arxiv                |                                        [Hallucinations and Truth: A Comprehensive Accuracy Evaluation of RAG, LoRA and DORA](https://arxiv.org/abs/2502.10497)                                         |                                            **RAG**&**LoRA**&**DoRA**                                             |
| 25.02 |                                                                             University of Southern California                                                                           |               arxiv                |                                  [Smoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed Knowledge Distillation](https://arxiv.org/abs/2502.11306)                                   |                             **Hallucination Mitigation**&**Knowledge Distillation**                              |
| 25.02 |                                                                                The University of Hong Kong                                                                              |               arxiv                |                                                     [Hallucinations are Inevitable but Statistically Negligible](https://arxiv.org/abs/2502.12187)                                                     |                                     **Hallucination**&**Probability Theory**                                     |
| 25.02 |                                                                                  University of W√ºrzburg                                                                                 |               arxiv                |                             [How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild](https://arxiv.org/abs/2502.12769)                             |                              **Hallucination**&**Multilingual LLMs**&**Factuality**                              |
| 25.02 |                                                                         Technion ‚Äì Israel Institute of Technology                                                                       |               arxiv                |                                                     [Trust Me, I‚Äôm Wrong: High-Certainty Hallucinations in LLMs](https://arxiv.org/abs/2502.12964)                                                     |                               **Hallucination**&**Model Certainty**&**LLM Safety**                               |
| 25.02 |                                                                                           Mila                                                                                          |               arXiv                |                           [Reducing Hallucinations in Language Model-based SPARQL Query Generation Using Post-Generation Memory Retrieval](https://arxiv.org/abs/2502.13369)                           |                  **SPARQL Query Generation**&**Hallucination Mitigation**&**Memory Retrieval**                   |
| 25.02 |                                                                       Huazhong University of Science and Technology                                                                     |               arXiv                |                                      [Detecting LLM Fact-conflicting Hallucinations Enhanced by Temporal-logic-based Reasoning](https://arxiv.org/abs/2502.13416)                                      |                                       **Hallucination**&**Temporal Logic**                                       |
| 25.02 |                                                                                    Columbia University                                                                                  |               arXiv                |                                    [TREECUT: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation](https://arxiv.org/abs/2502.13442)                                    |                        **Math Word Problem**&**LLM Hallucination**&**Synthetic Dataset**                         |
| 25.02 |                                                                   Pohang University of Science and Technology (POSTECH)                                                                 |               arXiv                |                                      [REFIND: Retrieval-Augmented Factuality Hallucination Detection in Large Language Models](https://arxiv.org/abs/2502.13622)                                       |                            **Hallucination Detection**&**Retrieval-Augmented Model**                             |
| 25.02 |                                                                                 Michigan State University                                                                               |               arXiv                |                                                [Multi-Faceted Studies on Data Poisoning can Advance LLM Development](https://arxiv.org/abs/2502.14182)                                                 |                              **Data Poisoning**&**LLM Security**&**Trustworthy AI**                              |
| 25.02 |                                                                               University of Texas at Austin                                                                             |               arXiv                |                                 [MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models](https://arxiv.org/abs/2502.14302)                                  |                            **Medical Hallucination**&**LLM Evaluation**&**Benchmark**                            |
| 25.02 |                                                                                    Cornell University                                                                                   |             ICLP 2024              |                                              [LP-LM: No Hallucinations in Question Answering with Logic Programming](https://doi.org/10.4204/EPTCS.416.5)                                              |                            **Logic Programming**&**Hallucination-Free QA**&**Prolog**                            |
| 25.02 |                                                                                Carnegie Mellon University                                                                               |             ICLR 2025              |                          [Self-Correcting Decoding with Generative Feedback for Mitigating Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2502.06130)                           |              **Large Vision-Language Models**&**Hallucination Mitigation**&**Generative Feedback**               |
| 25.02 |                                                                       Wroclaw University of Science and Technology                                                                      |               arxiv                |                                             [Hallucination Detection in LLMs Using Spectral Features of Attention Maps](https://arxiv.org/abs/2502.17598)                                              |                       **Hallucination Detection**&**Spectral Features**&**Attention Maps**                       |
| 25.02 |                                                                          University of Illinois Urbana-Champaign                                                                        |               arxiv                |                              [The Law of Knowledge Overshadowing: Towards Understanding, Predicting, and Preventing LLM Hallucination](https://arxiv.org/abs/2502.16143)                               |                         **Hallucination**&**Knowledge Overshadowing**&**LLM Factuality**                         |
| 25.02 |                                                                           University of California, Los Angeles                                                                         |               arxiv                |                                        [Verify when Uncertain: Beyond Self-Consistency in Black Box Hallucination Detection](https://arxiv.org/abs/2502.15845)                                         |                  **Hallucination Detection**&**Self-Consistency**&**Cross-Model Verification**                   |
| 25.02 |                                                                                   King's College London                                                                                 |               arxiv                |                                            [Hallucination Detection in Large Language Models with Metamorphic Relations](https://arxiv.org/abs/2502.15844)                                             |                    **Hallucination Detection**&**Metamorphic Relations**&**LLM Reliability**                     |
| 25.02 |                                                                                 The Ohio State University                                                                               |               arxiv                |                          [Winning Big with Small Models: Knowledge Distillation vs. Self-Training for Reducing Hallucination in QA Agents](https://arxiv.org/abs/2502.19545)                           |                     **Knowledge Distillation**&**Self-Training**&**Hallucination Reduction**                     |
| 25.02 |                                                                                    University of Seoul                                                                                  |               arxiv                |                          [Vision-Encoders (Already) Know What They See: Mitigating Object Hallucination via Simple Fine-Grained CLIPScore](https://arxiv.org/abs/2502.20034)                           |                        **Object Hallucination**&**Vision-Language Models**&**CLIPScore**                         |
| 25.02 |                                                                                         Ant Group                                                                                       |               arxiv                |                                [Bi‚Äôan: A Bilingual Benchmark and Model for Hallucination Detection in Retrieval-Augmented Generation](https://arxiv.org/abs/2502.19209)                                |              **Hallucination Detection**&**Retrieval-Augmented Generation**&**Bilingual Benchmark**              |
| 25.02 |                                                                         University of Maryland, Baltimore County                                                                        |               arxiv                |                                   [FilterRAG: Zero-Shot Informed Retrieval-Augmented Generation to Mitigate Hallucinations in VQA](https://arxiv.org/abs/2502.18536)                                   |          **Hallucination Mitigation**&**Visual Question Answering**&**Retrieval-Augmented Generation**           |
| 25.02 |                                                                         Guangzhou University, Beihang University                                                                        |               arXiv                |                               [Mitigating Hallucinations in Large Vision-Language Models by Adaptively Constraining Information Flow](https://arxiv.org/abs/2502.20750)                                |          **Vision-Language Models**&**Hallucination Mitigation**&**Variational Information Bottleneck**          |
| 25.02 |                                                     The Chinese University of Hong Kong, Shanghai Artificial Intelligence Laboratory                                                    |               arXiv                |                            [MedHallTune: An Instruction-Tuning Benchmark for Mitigating Medical Hallucination in Vision-Language Models](https://arxiv.org/abs/2502.20780)                             |             **Medical Hallucination**&**Hallucination Mitigation**&**Large Vision-Language Models**              |
| 25.03 |                                                          University of Oxford, Oxford University Hospitals NHS Foundation Trust                                                         |               arXiv                |                                        [Reducing Large Language Model Safety Risks in Women‚Äôs Health using Semantic Entropy](https://arxiv.org/abs/2503.00269)                                         |                                     **Semantic Entropy**&**Women‚Äôs Health**                                      |
| 25.03 |                                                                 Northwestern Polytechnical University, Swansea University                                                               |               arXiv                |                                                [Octopus: Alleviating Hallucination via Dynamic Contrastive Decoding](https://arxiv.org/abs/2503.00361)                                                 |              **Hallucination Mitigation**&**Contrastive Decoding**&**Large Vision-Language Models**              |
| 25.03 |                                                                          National Technical University of Athens                                                                        |               arXiv                |                        [HalCECE: A Framework for Explainable Hallucination Detection through Conceptual Counterfactuals in Image Captioning](https://arxiv.org/abs/2503.00436)                         |                **Hallucination Detection**&**Explainable Evaluation**&**Vision-Language Models**                 |
| 25.03 |                                                                        King‚Äôs College London, Cambridge University                                                                      |               arXiv                |                                    [Evaluating LLMs‚Äô Assessment of Mixed-Context Hallucination Through the Lens of Summarization](https://arxiv.org/abs/2503.01670)                                    |                  **Mixed-Context Hallucination**&**Hallucination Evaluation**&**Summarization**                  |
| 25.03 |                                                                                        Capital One                                                                                      |               arXiv                |                                 [Building Safe GenAI Applications: An End-to-End Overview of Red Teaming for Large Language Models](https://arxiv.org/abs/2503.01742)                                  |                                          **Red Teaming**&**AI Safety**                                           |
| 25.03 |                                                                     The Pennsylvania State University, GE Healthcare                                                                    |               arXiv                |                              [MedHEval: Benchmarking Hallucinations and Mitigation Strategies in Medical Large Vision-Language Models](https://arxiv.org/abs/2503.02157)                               |                **Medical Hallucination**&**Hallucination Mitigation**&**Vision-Language Models**                 |
| 25.03 |                                                                     Georgia Institute of Technology, Wuhan University                                                                   |               arXiv                |                                      [Shakespearean Sparks: The Dance of Hallucination and Creativity in LLMs‚Äô Decoding Layers](https://arxiv.org/abs/2503.02851)                                      |                               **Hallucination**&**Creativity**&**Decoding Layers**                               |
| 25.03 |                                                                               Pennsylvania State University                                                                             |               arXiv                |                         [Monitoring Decoding: Mitigating Hallucination via Evaluating the Factuality of Partial Response during Generation](https://arxiv.org/abs/2503.03106)                          |                  **Hallucination Mitigation**&**Factuality Evaluation**&**Decoding Monitoring**                  |
| 25.03 |                                                                      IIIT Hyderabad & TCS Research, Hyderabad, India                                                                    |               arXiv                |                                               [HalluCounter: Reference-free LLM Hallucination Detection in the Wild!](https://arxiv.org/abs/2503.04615)                                                |                   **Reference-free Hallucination Detection**&**LLMs**&**Response Consistency**                   |
| 25.03 |                                                                   University of Wisconsin-MadisonÔºå Zhejiang University                                                                  |     QUHFM Workshop @ ICLR 2025     |                                                       [How to Steer LLM Latents for Hallucination Detection?](https://arxiv.org/abs/2503.01917)                                                        |                      **Hallucination Detection**&**LLM Latent Space**&**Optimal Transport**                      |
| 25.03 |                                                                           Massachusetts Institute of Technology                                                                         |               arxiv                |                                             [Medical Hallucination in Foundation Models and Their Impact on Healthcare](https://arxiv.org/abs/2503.05777)                                              |                       **Medical Hallucination**&**Foundation Models**&**Clinical Safety**                        |
| 25.03 |                                                                                   Texas A&M University                                                                                  |               arxiv                |                                              [SINdex: Semantic INconsistency Index for Hallucination Detection in LLMs](https://arxiv.org/abs/2503.05980)                                              |                      **Hallucination Detection**&**Semantic Clustering**&**LLM Evaluation**                      |
| 25.03 |                                                                             University of Southern California                                                                           |               arxiv                |                                                   [Treble Counterfactual VLMs: A Causal Approach to Hallucination](https://arxiv.org/abs/2503.06169)                                                   |                        **Hallucination**&**Vision-Language Models**&**Causal Inference**                         |
| 25.03 |                                                                               Shanghai Jiao Tong University                                                                             |               arxiv                |                                                                 [Delusions of Large Language Models](https://arxiv.org/abs/2503.06709)                                                                 |                          **Delusion**&**Hallucination Detection**&**Model Uncertainty**                          |
| 25.03 |                                                                                   Texas A&M University                                                                                  |               arxiv                |                                          [HalluVerse25: Fine-grained Multilingual Benchmark Dataset for LLM Hallucinations](https://arxiv.org/abs/2503.07833)                                          |                **Hallucination Detection**&**Multilingual Benchmark**&**Fine-grained Annotation**                |
| 25.03 |                                                                                    Yangzhou University                                                                                  |               arxiv                |                                         [Is LLMs Hallucination Usable? LLM-based Negative Reasoning for Fake News Detection](https://arxiv.org/abs/2503.09153)                                         |                       **LLM Hallucination**&**Negative Reasoning**&**Fake News Detection**                       |
| 25.03 |                                                                                     Drexel University                                                                                   |               arxiv                |                                    [TruthPrInt: Mitigating LVLM Object Hallucination Via Latent Truthful-Guided Pre-Intervention](https://arxiv.org/abs/2503.10602)                                    |                   **Object Hallucination**&**Vision-Language Models**&**Latent Truthfulness**                    |
| 25.03 |                                                                          University of California, Santa Barbara                                                                        |       Finding of NAACL 2025        |                                    [Gradient-guided Attention Map Editing: Towards Efficient Contextual Hallucination Mitigation](https://arxiv.org/abs/2503.08963)                                    |                     **Contextual Hallucination**&**Attention Map Editing**&**Summarization**                     |
| 25.03 |                                                                                          Amazon                                                                                         |              WWW 2025              |                               [Uncertainty-Aware Fusion: An Ensemble Framework for Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2503.05757)                               |                             **Hallucination Detection**&**Uncertainty**&**Ensemble**                             |
| 25.03 |                                                                              University of Central Missouri                                                                             |               arxiv                |                                   [HALURust: Exploiting Hallucinations of Large Language Models to Detect Vulnerabilities in Rust](https://arxiv.org/abs/2503.10793)                                   |                            **Rust**&**LLM Hallucination**&**Vulnerability Detection**                            |
| 25.03 |                                                                                 Honda Research Institute                                                                                |               arxiv                |                                     [Graph-Grounded LLMs: Leveraging Graphical Function Calling to Minimize LLM Hallucinations](https://arxiv.org/abs/2503.10941)                                      |                       **Graph Reasoning**&**Function Calling**&**Hallucination Reduction**                       |
| 25.03 |                                                             Shanghai Advanced Research Institute, Chinese Academy of Sciences                                                           |               arxiv                |                 [HICD: Hallucination-Inducing via Attention Dispersion for Contrastive Decoding to Mitigate Hallucinations in Large Language Models](https://arxiv.org/abs/2503.12908)                 |                  **Hallucination Mitigation**&**Contrastive Decoding**&**Attention Dispersion**                  |
| 25.03 |                                                                       University of Science and Technology of China                                                                     |               arxiv                |                           [ClearSight: Visual Signal Enhancement for Object Hallucination Mitigation in Multimodal Large Language Models](https://arxiv.org/abs/2503.13107)                            |                      **Multimodal LLMs**&**Object Hallucination**&**Attention Modulation**                       |
| 25.03 |                                                                                    University of Derby                                                                                  |               arxiv                | [RAG-KG-IL: A Multi-Agent Hybrid Framework for Reducing Hallucinations and Enhancing LLM Reasoning through RAG and Incremental Knowledge Graph Learning Integration](https://arxiv.org/abs/2503.13514) |                               **RAG**&**Knowledge Graph**&**Multi-Agent Systems**                                |
| 25.03 |                                                                                   Vanderbilt University                                                                                 |               arxiv                |                                [From "Hallucination" to "Suture": Insights from Language Philosophy to Enhance Large Language Models](https://arxiv.org/abs/2503.14392)                                |                           **LLM Hallucination**&**Language Philosophy**&**Anchor-RAG**                           |
| 25.03 |                                                                                     Fudan University                                                                                    |               arxiv                |                                            [Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations](https://arxiv.org/abs/2503.14895)                                             |                  **Object Hallucination**&**Multimodal LLMs**&**Frequency-Domain Perturbation**                  |
| 25.03 |                                                                                         Skoltech                                                                                        |             AAAI 2024              |                                     [Don‚Äôt Fight Hallucinations, Use Them: Estimating Image Realism using NLI over Atomic Facts](https://arxiv.org/abs/2503.15948)                                     |                                        **LVLM**&**NLI**&**Image Realism**                                        |
| 25.03 |                                                                                      UNC Chapel Hill                                                                                    |             NAACL 2025             |                                   [MAMM-REFINE: A Recipe for Improving Faithfulness in Generation with Multi-Agent Collaboration](https://arxiv.org/abs/2503.15272)                                    |                  **Faithful Generation**&**Multi-Agent Collaboration**&**Refinement Pipeline**                   |
| 25.03 |                                                                                  Paris-Saclay University                                                                                |             ICLR 2025              |                                              [TOWARDS LIGHTER AND ROBUST EVALUATION FOR RETRIEVAL AUGMENTED GENERATION](https://arxiv.org/abs/2503.16161)                                              |                        **RAG Evaluation**&**Faithfulness Assessment**&**Quantized LLMs**                         |
| 25.03 |                                                                             University of Southern California                                                                           |               arxiv                |                                       [HDLCoRe: A Training-Free Framework for Mitigating Hallucinations in LLM-Generated HDL](https://arxiv.org/abs/2503.16528)                                        |                                **HDL Generation**&**Prompt Engineering**&**RAG**                                 |
| 25.03 |                                                       Wroc≈Çaw University of Science and Technology, University of Technology Sydney                                                     |               arxiv                |                                                [FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs](https://arxiv.org/abs/2503.17229)                                                |                       **Hallucination Detection**&**Knowledge Graph**&**Black-Box Method**                       |
| 25.03 |                                                                                    Stanford University                                                                                  |               arxiv                |                             [ShED-HD: A Shannon Entropy Distribution Framework for Lightweight Hallucination Detection on Edge Devices](https://arxiv.org/abs/2503.18242)                              |                       **Hallucination Detection**&**Entropy Analysis**&**Edge Deployment**                       |
| 25.03 |                              HausaNLP, University of Abuja, Bayero University Kano, University of Pretoria, Imperial College London, Northeastern University                            |               arxiv                |                                    [HausaNLP at SemEval-2025 Task 3: Towards a Fine-Grained Model-Aware Hallucination Detection](https://arxiv.org/abs/2503.19650)                                     |                     **Hallucination Detection**&**Model-Aware Method**&**Synthetic Dataset**                     |
| 25.03 |                                                                         Ant Group, Shanghai Jiao Tong University                                                                        |               arxiv                |                                             [Alleviating LLM-based Generative Retrieval Hallucination in Alipay Search](https://arxiv.org/abs/2503.21098)                                              |                    **Generative Retrieval**&**LLM Hallucination**&**Knowledge Distillation**                     |
| 25.03 |                                                                                ETH Z√ºrich, Google DeepMind                                                                              |               arxiv                |                                             [How do language models learn facts? Dynamics, curricula and hallucinations](https://arxiv.org/abs/2503.21676)                                             |                        **Knowledge Acquisition**&**Learning Dynamics**&**Hallucination**                         |
| 25.03 |                                                  Northwestern Polytechnical University, Alibaba Group, Zhejiang University of Technology                                                |             ICME 2025              |                                 [Instruction-Aligned Visual Attention for Mitigating Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2503.18556)                                 |                    **Visual Hallucination**&**Contrastive Decoding**&**Multimodal Attention**                    |
| 25.04 |                                                                       University of Science and Technology of China                                                                     |               arxiv                |                                 [TARAC: Mitigating Hallucination in LVLMs via Temporal Attention Real-time Accumulative Connection](https://arxiv.org/abs/2504.04099)                                  |                           **LVLM**&**Hallucination Mitigation**&**Temporal Attention**                           |
| 25.04 |                                                                             University of Southern California                                                                           |               arxiv                |                                      [Don‚Äôt Let It Hallucinate: Premise Verification via Retrieval-Augmented Logical Reasoning](https://arxiv.org/abs/2504.06438)                                      |           **Hallucination Mitigation**&**False Premise Detection**&**Retrieval-Augmented Generation**            |
| 25.04 |                                                                                        AIMon Labs                                                                                       |               arxiv                |                                       [HALLUCINOT: Hallucination Detection Through Context and Common Knowledge Verification](https://arxiv.org/abs/2504.07069)                                        |                **Hallucination Detection**&**Enterprise LLMs**&**Common Knowledge Verification**                 |
| 25.04 |                                                                                    University of Pisa                                                                                   |               arxiv                |                                   [Hybrid Retrieval for Hallucination Mitigation in Large Language Models: A Comparative Analysis](https://arxiv.org/abs/2504.05324)                                   |               **Retrieval Augmented Generation**&**Hallucination Mitigation**&**Query Expansion**                |
| 25.04 |                                                                                 University of Luxembourg                                                                                |               arxiv                |                                         [Noise Augmented Fine Tuning for Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2504.03302)                                         |                       **Noise Injection**&**Hallucination Mitigation**&**LLM Fine-Tuning**                       |
| 25.04 |                                                                                    University of Osaka                                                                                  |               arxiv                |                                                    [Hallucination Detection using Multi-View Attention Features](https://arxiv.org/abs/2504.04335)                                                     |                **Hallucination Detection**&**Attention Features**&**Token-level Classification**                 |
| 25.04 |                                                                                    Tsinghua University                                                                                  |               arxiv                |                                        [Combating LLM Hallucinations using Hypergraph-Driven Retrieval-Augmented Generation](https://arxiv.org/abs/2504.08758)                                         |                   **Large Language Models**&**Retrieval-Augmented Generation**&**Hypergraph**                    |
| 25.04 |                                                                    The Hong Kong University of Science and Technology                                                                   |               arxiv                |                           [Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection](https://arxiv.org/abs/2504.09440)                            |                   **Mathematical Reasoning**&**Hallucination Detection**&**Self-Consistency**                    |
| 25.04 |                                                                               Indian Statistical Institute                                                                              |               arxiv                |                                         [HALLUSHIFT: Measuring Distribution Shifts towards Hallucination Detection in LLMs](https://arxiv.org/abs/2504.09482)                                          |                     **Hallucination Detection**&**Distribution Shift**&**Token Probability**                     |
| 25.04 |                                                                       University of Science and Technology of China                                                                     |               arxiv                |                                [The Mirage of Performance Gains: Why Contrastive Decoding Fails to Address Multimodal Hallucination](https://arxiv.org/abs/2504.10020)                                 |                  **Multimodal Hallucination**&**Contrastive Decoding**&**MLLM Evaluation Bias**                  |
| 25.04 |                                                                       Skolkovo Institute of Science and Technology                                                                      |               arxiv                |                                           [Hallucination Detection in LLMs via Topological Divergence on Attention Graphs](https://arxiv.org/abs/2504.10063)                                           |                   **Hallucination Detection**&**Topological Divergence**&**Attention Graphs**                    |
| 25.04 |                                                                                    Newgiza University                                                                                   |               arxiv                |                                   [HalluSearch at SemEval-2025 Task 3: A Search-Enhanced RAG Pipeline for Hallucination Detection](https://arxiv.org/abs/2504.10168)                                   |                    **Hallucination Detection**&**Multilingual RAG**&**Factual Verification**                     |
| 25.04 |                                                                                        Giskard AI                                                                                       |               arxiv                |                                              [RealHarm: A Collection of Real-World Language Model Application Failures](https://arxiv.org/abs/2504.10277)                                              |                          **LLM Safety**&**Real-World Failures**&**Content Moderation**                           |
| 25.04 |                                                                                     Korea University                                                                                    |               arxiv                |                                   [Hallucination-Aware Generative Pretrained Transformer for Cooperative Aerial Mobility Control](https://arxiv.org/abs/2504.10831)                                    |                        **Reinforcement Learning**&**Hallucination**&**Mobility Control**                         |
| 25.04 |                                                                           Massachusetts Institute of Technology                                                                         |              CHI ‚Äô25               |                               [Purposefully Induced Psychosis (PIP): Embracing Hallucination as Imagination in Large Language Models](https://arxiv.org/abs/2504.12012)                                |                    **Hallucination**&**Computational Creativity**&**Human‚ÄìAI Collaboration**                     |
| 25.04 |                                                                                       Volkswagen AG                                                                                     |               arxiv                |                                             [Efficient Contrastive Decoding with Probabilistic Hallucination Detection](https://arxiv.org/abs/2504.12137)                                              |                 **Vision-Language Models**&**Hallucination Detection**&**Contrastive Decoding**                  |
| 25.04 |                                                                             University of California Berkeley                                                                           |               arxiv                |                                          [Why and How LLMs Hallucinate: Connecting the Dots with Subsequence Associations](https://arxiv.org/abs/2504.12691)                                           |                       **LLM Hallucination**&**Subsequence Association**&**Causal Tracing**                       |
| 25.04 |                                                                                  MIT Lincoln Laboratory                                                                                 |          HCXAI @ CHI 2025          |                                              [Mitigating LLM Hallucinations with Knowledge Graphs: A Case Study](https://doi.org/10.5281/zenodo.15170463)                                              |                        **LLM Hallucination**&**Knowledge Graph**&**Human-in-the-loop QA**                        |
| 25.04 |                                                                                University of West Florida                                                                               |               arxiv                |                 [HYDRA: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models](https://arxiv.org/abs/2504.14395)                  |                   **Vision-Language Models**&**Adversarial Robustness**&**Agentic Reasoning**                    |
| 25.04 |                                                                                   University of Windsor                                                                                 |               arxiv                |                                                  [ResNetVLLM-2: Addressing ResNetVLLM‚Äôs Multi-Modal Hallucinations](https://arxiv.org/abs/2504.14429)                                                  |                  **VideoLLM**&**Multi-Modal Hallucination**&**Retrieval-Augmented Generation**                   |
| 25.04 |                                                                                      Yale University                                                                                    |               arxiv                |                                           [(Im)possibility of Automated Hallucination Detection in Large Language Models](https://arxiv.org/abs/2504.17004)                                            |                **Hallucination Detection**&**Language Identification Theory**&**LLM Reliability**                |
| 25.04 |                                                                                       FAIR at Meta                                                                                      |               arxiv                |                                                               [HalluLens: LLM Hallucination Benchmark](https://arxiv.org/abs/2504.17550)                                                               |                    **LLM Hallucination**&**Benchmark**&**Intrinsic vs. Extrinsic Evaluation**                    |
| 25.04 |                                                                             University of Southern California                                                                           |               arxiv                |                                               [Evaluating Evaluation Metrics ‚Äì The Mirage of Hallucination Detection](https://arxiv.org/abs/2504.18114)                                                |                                **Hallucination Detection**&**Evaluation Metrics**                                |
| 25.04 |                                                                                      Ulm University                                                                                     |               arxiv                |                           [Hallucination Detectives at SemEval-2025 Task 3: Span-Level Hallucination Detection for LLM-Generated Answers](https://arxiv.org/abs/2504.18639)                            |                   **Hallucination Detection**&**Semantic Role Labeling**&**Multilingual NLP**                    |
| 25.04 |                                                                                  The University of Akron                                                                                |               arxiv                |                  [Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models](https://arxiv.org/abs/2504.19061)                   |                 **Medical Text Summarization**&**Hallucinations**&**Key Information Extraction**                 |
| 25.04 |                                                                                        AWS AI Labs                                                                                      |               arxiv                |                                                            [Towards Long Context Hallucination Detection](https://arxiv.org/abs/2504.19457)                                                            |              **Long-Context Processing**&**Hallucination Detection**&**Natural Language Inference**              |
| 25.04 |                                                                            RISE Research Institutes of Sweden                                                                           |               arxiv                |                                         [Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?](https://arxiv.org/abs/2504.20699)                                          |                       **Hallucination Detection**&**Machine Translation**&**Paraphrasing**                       |
| 25.04 |                                                                                           UNIST                                                                                         |               arxiv                |                                      [Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation, and Challenges](https://arxiv.org/abs/2504.20799)                                       |                     **Code Generation**&**Hallucination Taxonomy**&**Mitigation Strategies**                     |
| 25.04 |                                                                                     Jilin University                                                                                    |             IJCAI 2025             |                                             [Robust Misinformation Detection by Visiting Potential Commonsense Conflict](https://arxiv.org/abs/2504.21604)                                             |                   **Misinformation Detection**&**Commonsense Conflict**&**Data Augmentation**                    |
| 25.05 |                                                                                        Quotient AI                                                                                      |               arxiv                |                                      [HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World Hallucination Detection](https://arxiv.org/abs/2505.00506)                                      |                   **Hallucination Detection**&**Benchmark**&**Retrieval-Augmented Generation**                   |
| 25.05 |                                                                                    Kanazawa University                                                                                  |               arxiv                |                          [Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models](https://arxiv.org/abs/2505.00557)                          |                    **Prompt-Induced Hallucination**&**Conceptual Fusion**&**LLM Evaluation**                     |
| 25.04 |                                                                                       Amazon AWS AI                                                                                     |             NAACL 2025             |                              [Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models](https://arxiv.org/abs/2504.21559)                               |                **Visual Prompt Engineering**&**Object Hallucination**&**Vision Language Models**                 |
| 25.05 |                                                                                    Stanford University                                                                                  |               arxiv                |                                                  [Osiris: A Lightweight Open-Source Hallucination Detection System](https://arxiv.org/abs/2505.04844)                                                  |                        **Hallucination Detection**&**RAG Systems**&**Open-Source Models**                        |
| 25.05 |                                                                                          MBZUAI                                                                                         |               arxiv                |                [A Head to Predict and a Head to Question: Pre-trained Uncertainty Quantification Heads for Hallucination Detection in LLM Outputs](https://arxiv.org/abs/2505.08200v1)                 |                    **Hallucination Detection**&**Uncertainty Quantification**&**LLM Outputs**                    |
| 25.05 |                                                                      Lappeenranta-Lahti University of Technology LUT                                                                    |               arxiv                |                                       [EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2505.11405)                                       |           **Emotion Understanding**&**Multimodal Large Language Models**&**Hallucination Evaluation**            |
| 25.05 |                                                                              Los Alamos National Laboratory                                                                             |               arxiv                |                            [Diverging Towards Hallucination: Detection of Failures in Vision-Language Models via Multi-token Aggregation](https://arxiv.org/abs/2505.11741)                            |                  **Hallucination Detection**&**Vision-Language Models**&**Multi-token Logits**                   |
| 25.05 |                                                                                      York University                                                                                    |               arxiv                |                                      [Reasoning Large Language Model Errors Arise from Hallucinating Critical Problem Features](https://arxiv.org/abs/2505.12151)                                      |                           **Reasoning LLMs**&**Graph Coloring**&**Edge Hallucination**                           |
| 25.05 |                                                                                         HKUST(GZ)                                                                                       |               arxiv                |                            [Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation](https://arxiv.org/abs/2505.12265)                            |                 **Hallucination Detection**&**Long-Form Generation**&**Auxiliary Task Learning**                 |
| 25.05 |                                                                              De Artificial Intelligence Lab                                                                             |               arxiv                |                                 [Mitigating Hallucinations via Inter-Layer Consistency Aggregation in Large Vision-Language Models](https://arxiv.org/abs/2505.12343)                                  |                  **Hallucination Mitigation**&**Vision-Language Models**&**Layer Aggregation**                   |
| 25.05 |                                                                                Renmin University of China                                                                               |               arxiv                |                                   [Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective](https://arxiv.org/abs/2505.12886)                                   |             **Reasoning Hallucination**&**Mechanistic Interpretability**&**Reinforcement Learning**              |
| 25.05 |                                                                             University of Southern California                                                                           |               arxiv                |                                                         [The Hallucination Tax of Reinforcement Finetuning](https://arxiv.org/abs/2505.13988)                                                          |                  **Reinforcement Finetuning**&**Hallucination Mitigation**&**Refusal Behavior**                  |
| 25.05 |                                                                                    Aalborg University                                                                                   |               arxiv                |                                    [MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations](https://arxiv.org/abs/2505.14101)                                    |                       **Hallucination Evaluation**&**Knowledge Graph**&**Multilingual QA**                       |
| 25.05 |                                                                                   Chung-Ang University                                                                                  |               arxiv                |                                 [Hallucinate at the Last in Long Response Generation: A Case Study on Long Document Summarization](https://arxiv.org/abs/2505.15291v1)                                 |                           **Hallucination**&**Long-form Generation**&**Summarization**                           |
| 25.05 |                                                                    The Hong Kong University of Science and Technology                                                                   |               arxiv                |            [RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection](https://arxiv.org/abs/2505.15386v1)             |                    **Hallucination Detection**&**Uncertainty Estimation**&**Explainable QA**                     |
| 25.05 |                                                                                   Southeast University                                                                                  |               arxiv                |                                                [Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation](https://arxiv.org/abs/2505.16146v1)                                                |                        **LVLM Hallucination**&**Sparse Autoencoder**&**Latent Steering**                         |
| 25.05 |                                                                                    Politecnico di Bari                                                                                  |              ACL 2025              |                              [Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs](https://arxiv.org/abs/2505.16520v1)                              |                           **Factuality Encoding**&**LLM Probing**&**Self-Evaluation**                            |
| 25.05 |                                                                            Computer Network Information Center                                                                          |               arxiv                |                       [Shadows in the Attention: Contextual Perturbation and Representation Drift in the Dynamics of Hallucination in LLMs](https://arxiv.org/abs/2505.16894v1)                        |                        **Hallucinations**&**Representation Drift**&**Attention-Locking**                         |
| 25.05 |                                                                                  University of Virginia                                                                                 |             IJCAI 2025             |                       [Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models](https://arxiv.org/abs/2505.14599v1)                       |           **Biomedical Hypothesis Generation**&**Truthfulness Evaluation**&**Hallucination Detection**           |
| 25.05 | Chinese Academy of Sciences |         ACL 2025 Findings          | [Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2505.17061) | **Hallucination Mitigation**&**Large Vision-Language Models**&**Contrastive Decoding** |
| 25.05 | Southeast University, University of New South Wales |               arxiv                | [After Retrieval, Before Generation: Enhancing the Trustworthiness of Large Language Models in RAG](https://arxiv.org/abs/2505.17118v1) | **Trustworthiness**&**RAG**&**Soft Bias** |
| 25.05 | University of Southern California |               arxiv                | [Humans Hallucinate Too: Language Models Identify and Correct Subjective Annotation Errors With Label-in-a-Haystack Prompts](https://arxiv.org/abs/2505.17222v1) | **Subjective Annotation**&**Error Correction**&**Label-in-a-Haystack** |
| 25.05 | EPFL, Stony Brook University, University of Chicago |               arxiv                | [Unraveling Misinformation Propagation in LLM Reasoning](https://arxiv.org/abs/2505.18555) | **Misinformation Propagation**&**LLM Reasoning**&**Correction Strategies** |
| 25.05 | Zhejiang University |               arxiv                | [Causal-LLaVA: Causal Disentanglement for Mitigating Hallucination in Multimodal Large Language Models](https://arxiv.org/abs/2505.19474v1) | **Causal Disentanglement**&**Multimodal Large Language Model**&**Hallucination Mitigation** |
| 25.05 | Beihang University |               arxiv                | [Enhancing Visual Reliance in Text Generation: A Bayesian Perspective on Mitigating Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2505.19498v1) | **Visual Reliance**&**Bayesian Perspective**&**Hallucination Mitigation** |
| 25.05 | George Mason University |               arxiv                | [Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration](https://arxiv.org/abs/2505.21472v1) | **Hallucination Mitigation**&**Attention Calibration**&**Vision-Language Model** |
| 25.05 | UC Santa Cruz |               arxiv                | [More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models](https://arxiv.org/abs/2505.21523v1) | **Multimodal Reasoning**&**Hallucination**&**Visual Attention** |
| 25.05 | Hasso Plattner Institute, University of Potsdam |               arxiv                | [Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing](https://arxiv.org/abs/2505.21547v1) | **Hallucination**&**Vision-Language Model**&**Image Tokenizer** |
| 25.05 | Purdue University, University of California, Davis |               arxiv                | [Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation](https://arxiv.org/abs/2505.23657) | **Layer-Contrastive Decoding**&**Hallucination Mitigation**&**Reinforcement Learning** |
| 25.05 | Yonsei University, Onoma AI |         ACL 2025 Findings          | [Retrieval Visual Contrastive Decoding to Mitigate Object Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2505.20569v2) | **Object Hallucination**&**Vision-Language Model**&**Contrastive Decoding** |
| 25.05 | The Hong Kong University of Science and Technology (Guangzhou) |              ACL 2025              | [How does Misinformation Affect Large Language Model Behaviors and Preferences?](https://arxiv.org/abs/2505.21608v1) | **Misinformation**&**LLM Behavior**&**Benchmark** |
| 25.05 | The Hong Kong Polytechnic University |              ACL 2025              | [Removal of Hallucination on Hallucination: Debate-Augmented RAG](https://arxiv.org/abs/2505.18581v1) | **Hallucination Mitigation**&**Retrieval-Augmented Generation**&**Multi-Agent Debate** |
| 25.05 | Central South University|              ACL 2025              | [CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models](https://arxiv.org/abs/2505.19108v1) | **Cross-lingual Hallucination**&**Cross-modal Hallucination**&**Benchmark** |
| 25.05 | Hong Kong University of Science and Technology (Guangzhou) |               arxiv                | [Evaluation Hallucination in Multi-Round Incomplete Information Lateral-Driven Reasoning Tasks](https://arxiv.org/abs/2505.23843) | **Lateral Thinking**&**Multi-Round Reasoning**&**Evaluation Benchmark** |
| 25.05 | University of Illinois Urbana-Champaign |               arxiv                | [From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models](https://arxiv.org/abs/2505.24232v1) | **Hallucination**&**Jailbreak**&**Foundation Models** |
| 25.05 | National University of Singapore |               arxiv                | [The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2505.24630v1) | **Hallucination**&**Reinforcement Learning**&**Reasoning Model** |
| 25.05 | University of Arkansas |               arxiv                | [BIMA: Bijective Maximum Likelihood Learning Approach to Hallucination Prediction and Mitigation in Large Vision-Language Models](https://arxiv.org/abs/2505.24649v1) | **Vision-Language Model**&**Hallucination Mitigation**&**Normalizing Flow** |
| 25.06 | MBZUAI |               arxiv                | [HD-NDEs: Neural Differential Equations for Hallucination Detection in LLMs](https://arxiv.org/abs/2506.00088v1) | **Hallucination Detection**&**Neural Differential Equations**&**LLM Internal States** |
| 25.06 | Universit√© C√¥te d‚ÄôAzur |               arxiv                | [MMD-Flagger: Leveraging Maximum Mean Discrepancy to Detect Hallucinations](https://arxiv.org/abs/2506.01367v1) | **Hallucination Detection**&**Maximum Mean Discrepancy**&**Machine Translation** |
| 25.06 | Nanyang Technological University |               arxiv                | [Benford's Curse: Tracing Digit Bias to Numerical Hallucination in LLMs](https://arxiv.org/abs/2506.01734v1) | **Numerical Hallucination**&**Digit Bias**&**Benford‚Äôs Law** |
| 25.06 | University of Technology Sydney |               arxiv                | [Shaking to Reveal: Perturbation-Based Detection of LLM Hallucinations](https://arxiv.org/abs/2506.02696v1) | **Hallucination Detection**&**Perturbation**&**Intermediate Representation** |
| 25.06 | Fundaci√≥n Centro Tecnol√≥xico de Telecomunicaci√≥ns de Galicia |               arxiv                | [Ask a Local: Detecting Hallucinations With Specialized Model Divergence](https://arxiv.org/abs/2506.03357v1) | **Hallucination Detection**&**Specialized Model Divergence**&**Multilingual LLM** |
| 25.06 | Soochow University |               arxiv                | [Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization](https://arxiv.org/abs/2506.04039v1) | **Vision-Language Model**&**Hallucination Mitigation**&**Preference Optimization** |
| 25.06 | Tsinghua University |               arxiv                | [Joint Evaluation of Answer and Reasoning Consistency for Hallucination Detection in Large Reasoning Models](https://arxiv.org/abs/2506.04832v1) | **Hallucination Detection**&**Large Reasoning Model**&**Reasoning Consistency** |
| 25.06 | Peking University |               arxiv                | [When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models](https://arxiv.org/abs/2506.04909v1) | **LLM Deception**&**Chain-of-Thought Reasoning**&**Representation Engineering** |
| 25.06 | Institute of Automation, Chinese Academy of Sciences |              ACL 2025              | [Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis](https://arxiv.org/abs/2506.04142v1) | **Trustworthy Evaluation**&**Shortcut Neuron**&**Data Contamination** |
| 25.06 | Mohamed bin Zayed University of AI |               arxiv                | [DRAG: Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate Hallucination via Evidence and Graph-based Distillation](https://arxiv.org/abs/2506.01954v1) | **RAG Distillation**&**Small Language Models**&**Hallucination Mitigation** |
| 25.06 | City University of Hong Kong, The University of Hong Kong |               arxiv                | [GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner for Query Expansion in Information Retrieval](https://arxiv.org/abs/2506.04762) | **Query Expansion**&**Hallucination Filter**&**Information Retrieval** |
| 25.06 | New York University Abu Dhabi, Qatar Computing Research Institute, Hamad Bin Khalifa University, EURECOM |               arxiv                | [Combating Misinformation in the Arab World: Challenges & Opportunities](https://arxiv.org/abs/2506.05582) | **Misinformation**&**Disinformation**&**Arabic NLP**&**Social Correction** |
| 25.06 | Samsung AI Center Warsaw |               arxiv                | [On the Fundamental Impossibility of Hallucination Control in Large Language Models](https://arxiv.org/abs/2506.06382v1) | **Hallucination**&**LLM**&**Impossibility Theorem**&**Mechanism Design** |
| 25.06 | King Abdullah University of Science and Technology, University of Electronic Science and Technology of China, University of Copenhagen |               arxiv                | [Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images](https://arxiv.org/abs/2506.07184) | **Behavioral Hallucination**&**Multimodal LLM**&**Sequential Images**&**Hallucination Mitigation** |
| 25.06 | HKUST, Shanghai AI Lab, Peking University, HKUST(GZ), Oxford University, Imperial College London |               arxiv                | [Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning](https://arxiv.org/abs/2506.07227) | **Hallucination**&**Micro Edit Dataset**&**Fine-Grained Multimodal Learning**&**Visual Reasoning** |
| 25.06 | Purdue University, National Taiwan University |               arxiv                | [Reducing Object Hallucination in Large Audio-Language Models via Audio-Aware Decoding](https://arxiv.org/abs/2506.07233) | **Audio-Language Model**&**Object Hallucination**&**Contrastive Decoding**&**Audio-Aware Decoding** |
| 25.06 | FaCENA, Universidad Nacional del Nordeste; Legalhub S.A.; Instituto de Modelado e Innovaci√≥n Tecnol√≥gica, CONICET |               arxiv                | [The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation](https://arxiv.org/abs/2506.08827v1) | **Named Entity Recognition**&**Large Language Models**&**Legal Documents** |
| 25.06 | Xiamen University, The Hong Kong Polytechnic University, Migu Meland Co., Ltd, Soochow University, Singapore Management University, Shanghai Artificial Intelligence Laboratory |               arxiv                | [FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2506.08938v1) | **Retrieval-Augmented Generation**&**Faithfulness**&**Knowledge Conflict**&**Fact-Level Modeling** |
| 25.06 | St.Petersburg State University, Skoltech |               arxiv                | [Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs](https://arxiv.org/abs/2506.09886v1) | **Hallucination Detection**&**Attention Head Embedding**&**Deep Kernel**&**LLM** |
| 25.06 | UC Berkeley |               arxiv                | [Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers](https://arxiv.org/abs/2506.10887v1) | **Out-of-Context Reasoning**&**Generalization**&**Hallucination**&**Transformer Theory**&**Knowledge Injection** |
| 25.06 | Emory University, UIUC |              ACL 2025              | [Beyond Facts: Evaluating Intent Hallucination in Large Language Models](https://arxiv.org/abs/2506.06539v1) | **Intent Hallucination**&**Intent Alignment**&**Hallucination Evaluation**&**Benchmark**&**Constraint Score** |
| 25.06 | Massachusetts Institute of Technology |               arxiv                | [Eliminating Hallucination-Induced Errors in LLM Code Generation with Functional Clustering](https://arxiv.org/abs/2506.11021) | **LLM Code Generation**&**Hallucination Mitigation**&**Functional Clustering** |
| 25.06 | Shandong University |               arxiv                | [Mitigating Hallucination Through Theory-Consistent Symmetric Multimodal Preference Optimization](https://arxiv.org/abs/2506.11712v1) | **Hallucination Mitigation**&**Multimodal Preference Optimization**&**Direct Preference Optimization** |
| 25.06 | Beijing Institute of Technology |               arxiv                | [MALM: A Multi-Information Adapter for Large Language Models to Mitigate Hallucination](https://arxiv.org/abs/2506.12483v1) | **Hallucination Mitigation**&**Large Language Model**&**Graph Neural Networks**&**Question Answering**&**Retrieval-Augmented Generation** |
| 25.06 | Keio University |               arxiv                | [ZINA: Multimodal Fine-grained Hallucination Detection and Editing](https://arxiv.org/abs/2506.13130v1) | **Hallucination Detection**&**Multimodal Large Language Model**&**Fine-grained Editing**&**Vision-Language Model**&**Error Taxonomy** |
| 25.06 | Christian-Albrechts-Universit√§t zu Kiel |               arxiv                | [ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM](https://arxiv.org/abs/2506.14766v1) | **Hallucination**&**Multimodal Large Language Model**&**Attention Steering**&**Contrastive Decoding**&**Visual Grounding** |
| 25.06 | Harbin Institute of Technology |              ACL 2025              | [CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention](https://arxiv.org/abs/2506.11073v1) | **Multilingual Object Hallucination**&**Vision-Language Models**&**Cross-Lingual Attention**&**Inference Intervention**&**Hallucination Mitigation** |
| 25.06 | Indian Institute of Technology Delhi |               arxiv                | [HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations](https://arxiv.org/abs/2506.17748) | **Hallucination Detection**&**Language Models**&**Statistical Independence** || 25.06 | East China Normal University | arxiv | [Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation](https://arxiv.org/abs/2506.17088v1) | **Chain-of-Thought**&**Hallucination Detection**&**LLM** |
| 25.06 | Sichuan University |               arxiv                | [HalluRNN: Mitigating Hallucinations via Recurrent Cross-Layer Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2506.17587v1) | **Hallucination**&**Vision-Language Model**&**Cross-Layer Reasoning** |
| 25.06 | Wroclaw Tech |               arxiv                | [AggTruth: Contextual Hallucination Detection using Aggregated Attention Scores in LLMs](https://arxiv.org/abs/2506.18628) | **Hallucination Detection**&**Retrieval-Augmented Generation**&**Attention Map** |
| 25.06 | Technical University of Munich |          FEVER @ ACL 2025          | [Correcting Hallucinations in News Summaries: Exploration of Self-Correcting LLM Methods with External Knowledge](https://arxiv.org/abs/2506.19607v1) | **Hallucination Correction**&**News Summarization**&**Self-Correction** |
| 25.06 | dBeta Labs, The Lane Crawford Joyce Group |               arxiv                | [Hallucination Detection with Small Language Models](https://arxiv.org/abs/2506.22486v1) | **Hallucination Detection**&**SLM**&**Verification** |
| 25.07 | Java Innovations, Cognizant |          ADCAIJ Pre-Print          | [Using multi-agent architecture to mitigate the risk of LLM hallucinations](https://arxiv.org/abs/2507.01446v1) | **Hallucination Mitigation**&**Multi-Agent**&**Fuzzy Logic** |
| 25.07 | Beijing University of Posts and Telecommunications |               arxiv                | [Loki's Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models](https://arxiv.org/abs/2507.02870v1) | **Hallucination**&**LLM**&**Survey** |
| 25.07 | Stony Brook University |               arxiv                | [Token-Level Hallucination Detection via Variance in Language Models](https://arxiv.org/abs/2507.04137v1) | **Hallucination Detection**&**Token Variance**&**LLM** |
| 25.07 | University of Sydney |            ACM MM 2025             | [Identify, Isolate, and Purge: Mitigating Hallucinations in LVLMs via Self-Evolving Distillation](https://arxiv.org/abs/2507.04680v1) | **Hallucinations**&**LVLMs**&**Knowledge Distillation** |
| 25.07 | University of Bristol |               arxiv                | [ReLoop: "Seeing Twice and Thinking Backwards" via Closed-loop Training to Mitigate Hallucinations in Multimodal understanding](https://arxiv.org/abs/2507.04943v1) | **Hallucination Mitigation**&**Multimodal**&**Consistency** |
| 25.07 | University of Chinese Academy of Sciences |               arxiv                | [INTER: Mitigating Hallucination in Large Vision-Language Models by Interaction Guidance Sampling](https://arxiv.org/abs/2507.05056v1) | **Hallucination**&**LVLMs**&**Interaction** |
| 25.07 | Stanford University |               arxiv                | [Hallucination Stations: On Some Basic Limitations of Transformer-Based Language Models](https://arxiv.org/abs/2507.07505v1) | **Hallucination**&**Complexity**&**Agentic AI** |
| 25.07 | BITS Pilani |               arxiv                | [AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters](https://arxiv.org/abs/2507.10586v1) | **RAG**&**Hallucination**&**LoRA** |
| 25.07 | The University of Hong Kong |               arxiv                | [MAD-Spear: A Conformity-Driven Prompt Injection Attack on Multi-Agent Debate Systems](https://arxiv.org/abs/2507.13038v1) | **Prompt Injection**&**MAD**&**Security** |
| 25.07 | Institute for Infocomm Research (I2R), A*STAR |               arxiv                | [CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation](https://arxiv.org/abs/2507.14239v1) | **Hallucination**&**Cross-lingual**&**Contrastive Learning** |
| 25.07 | Shanghai Jiao Tong University |               arxiv                | [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660v2) | **Multi-Agent**&**Collusion**&**Safety** |
| 25.07 | Nankai University |               arxiv                | [Towards Mitigation of Hallucination for LLM-empowered Agents: Progressive Generalization Bound Exploration and Watchdog Monitor](https://arxiv.org/abs/2507.15903v1) | **Hallucination**&**Generalization Bound**&**Agent** |
| 25.07 | Konkuk University |              ACL 2025              | [Exploring the Impact of Instruction-Tuning on LLM‚Äôs Susceptibility to Misinformation](https://arxiv.org/abs/2507.18203v1) | **Instruction-Tuning**&**Misinformation**&**Hallucination** |
| 25.07 | Peking University | ACL 2025 | [ICR Probe: Tracking Hidden State Dynamics for Reliable Hallucination Detection in LLMs](https://arxiv.org/abs/2507.16488v1) | **Hallucination Detection**&**LLM**&**Interpretability** |
| 25.07 | Pegasi AI | arxiv | [FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models](https://arxiv.org/abs/2507.20930) | **Hallucination**&**Detection**&**Finance** |
| 25.07 | University of California, Berkeley | arxiv | [MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them](https://arxiv.org/abs/2507.21017) | **Hallucination**&**Agent**&**Benchmark** |
| 25.07 | Xidian University | arxiv | [Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM Intermediate Thought Processes](https://arxiv.org/abs/2507.22940) | **Reasoning**&**Factuality**&**Interpretability** |
| 25.07 | Chalmers University of Technology | arxiv | [Uncovering the Fragility of Trustworthy LLMs through Chinese Textual Ambiguity](https://arxiv.org/abs/2507.23121) | **Ambiguity**&**Trustworthiness**&**LLM** |
| 25.07 | Tsinghua University | arxiv | [Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning](https://arxiv.org/abs/2507.19586) | **Geospatial**&**Hallucination**&**Factuality** |
| 25.07 | The University of New South Wales | arxiv | [Multi-Stage Verification-Centric Framework for Mitigating Hallucination in Multi-Modal RAG](https://arxiv.org/abs/2507.20136) | **Hallucination**&**Verification**&**RAG** |
| 25.07 | University of Sana‚Äôa | arxiv | [Theoretical Foundations and Mitigation of Hallucination in Large Language Models](https://arxiv.org/abs/2507.22915) | **Hallucination**&**Theory**&**Mitigation** |
| 25.08 |                                         University of Chinese Academy of Sciences, Peking University, Amazon.com, Inc.                                         |      arxiv      |      [Hacking Hallucinations of MLLMs with Causal Sufficiency and Necessity](https://arxiv.org/abs/2508.04182)      |      **Multimodal Large Language Models**&**Hallucination**&**Causal Inference** |
| 25.08 |                                         Renmin University of China, University of California, San Diego, DataCanvas Alaya NeW                                         |      arxiv      |      [Analyzing and Mitigating Object Hallucination: A Training Bias Perspective](https://arxiv.org/abs/2508.04567)      |      **Large Vision-Language Models**&**Object Hallucination**&**Training Bias**&**Unlearning** |
| 25.08 |                                         University of Melbourne, Imperial College London                                          |      arxiv      |      [Incident Response Planning Using a Lightweight Large Language Model with Reduced Hallucination](https://arxiv.org/abs/2508.05188)      |      **Incident Response**&**Large Language Models**&**Hallucination Mitigation** |
| 25.08 | Stanford University, University of California, Berkeley | arxiv | [Toward Verifiable Misinformation Detection: A Multi-Tool LLM Agent Framework](https://arxiv.org/abs/2508.03054) | **Misinformation Detection**&**LLM Agent**&**Verifiable Reasoning**|
| 25.08 | Huaiyin Institute of Technology | arxiv | [MM-FusionNet: Context-Aware Dynamic Fusion for Multi-modal Fake News Detection with Large Vision-Language Models](https://arxiv.org/abs/2508.05684v1) | **Multi-modal Fake News Detection**&**Large Vision-Language Models**&**Dynamic Fusion** |
| 25.08 | Xi‚Äôan Jiaotong-Liverpool University | arxiv | [SEVADE: Self-Evolving Multi-Agent Analysis with Decoupled Evaluation for Hallucination-Resistant Irony Detection](https://arxiv.org/abs/2508.06803v1) | **Sarcasm Detection**&**Multi-Agent Systems**&**Hallucination Mitigation** |
| 25.08 | Wroclaw University of Science and Technology | arxiv | [The Illusion of Progress: Re-evaluating Hallucination Detection in LLMs](https://arxiv.org/abs/2508.08285v2) | **Hallucination Detection**&**Evaluation Metrics**&**LLM-as-Judge** |
| 25.08 | Simon Fraser University | AIES 2025 | [An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs](https://arxiv.org/abs/2508.10010v1) | **Jailbreak Attacks**&**Health Misinformation**&**LLM Safety** |
| 25.08 | Fidelity Investments | arxiv | [Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models](https://arxiv.org/abs/2508.10192v1) | **Faithfulness Hallucination**&**Semantic Divergence Metrics**&**Misalignment Detection** |
| 25.08 | Xi‚Äôan Jiaotong University | arxiv | [DiFaR: Enhancing Multimodal Misinformation Detection with Diverse, Factual, and Relevant Rationales](https://arxiv.org/abs/2508.10444v1) | **Multimodal Misinformation Detection**&**Chain-of-Thought Rationales**&**Post-hoc Filtering** |
| 25.08 | Peking University | CIKM 2025 | [Exploring Causal Effect of Social Bias on Faithfulness Hallucinations in Large Language Models](https://arxiv.org/abs/2508.07753v1) | **Faithfulness Hallucinations**&**Causal Inference**&**Social Bias** |
| 25.08 | Technical University of Munich | arxiv | [Hallucination in LLM-Based Code Generation: An Automotive Case Study](https://arxiv.org/abs/2508.11257) | **Code Generation**&**LLM Hallucination**&**Automotive Software** |
| 25.08 | University of Southern California | arxiv | [Mitigating Hallucinations in Large Language Models via Causal Reasoning](https://arxiv.org/abs/2508.12495v1) | **Hallucination Mitigation**&**Causal Reasoning**&**Supervised Fine-Tuning** |
| 25.08 | Chongqing University | arxiv | [Prompt-Induced Linguistic Fingerprints for LLM-Generated Fake News Detection](https://arxiv.org/abs/2508.12632v1) | **Fake News Detection**&**Linguistic Fingerprints**&**LLM Security** |
| 25.08 | Harbin Institute of Technology, Zhejiang University, Tsinghua University | arxiv | [Mitigating Hallucinations in LM-Based TTS Models via Distribution Alignment Using GFlowNets](https://arxiv.org/abs/2508.15442) | **Text-to-Speech**&**Hallucination Mitigation**&**GFlowNets** |
| 25.08 | JP Morgan AI Research | arxiv | [QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting](https://arxiv.org/abs/2508.16697v1) | **Hallucination Mitigation**&**Query Rewriting**&**Contextual Bandits** |
| 25.08 | Yonsei University | arxiv | [Being Kind Isn't Always Being Safe: Diagnosing Affective Hallucination in LLMs](https://arxiv.org/abs/2508.16921v1) | **Affective Hallucination**&**Emotional Safety**&**Benchmarking** |
| 25.08 | University of Illinois Urbana-Champaign | arxiv | [Principled Detection of Hallucinations in Large Language Models via Multiple Testing](https://arxiv.org/abs/2508.18473v2) | **Hallucination Detection**&**Multiple Testing**&**OOD Methods** |
| 25.08 | North Carolina State University | arxiv | [Decoding Memories: An Efficient Pipeline for Self-Consistency Hallucination Detection](https://arxiv.org/abs/2508.21228v1) | **Large Language Models**&**Hallucination Detection**&**Efficient Inference** |
| 25.09 | Beijing Institute of Technology | arxiv | [Exploring and Mitigating Fawning Hallucinations in Large Language Models](https://arxiv.org/abs/2509.00869v1) | **Fawning Hallucination**&**Contrastive Decoding**&**Large Language Models** |
| 25.09 | NVIDIA | arxiv | [Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection](https://arxiv.org/abs/2509.03113v1) | **Multimodal Hallucination**&**Gradient-based Self-Reflection**&**Contrastive Decoding** |
| 25.09 | ETH Z√ºrich | arxiv | [Real-Time Detection of Hallucinated Entities in Long-Form Generation](https://arxiv.org/abs/2509.03531v1) | **Hallucination Detection**&**Long-Form Generation**&**Linear/LoRA Probes** |
| 25.09 | Emory University | arxiv | [Improving Factuality in LLMs via Inference-Time Knowledge Graph Construction](https://arxiv.org/abs/2509.03540v1) | **Factuality**&**Knowledge Graph Construction**&**Retrieval-Augmented Generation** |
| 25.09 | Artefact Research Center, CentraleSup√©lec Universit√© Paris-Saclay | arxiv | [Learned Hallucination Detection in Black-Box LLMs using Token-level Entropy Production Rate](https://arxiv.org/abs/2509.04492v1) | **Hallucination Detection**&**Entropy Production Rate**&**Black-Box LLMs** |
| 25.09 | People‚Äôs Public Security University of China | arxiv | [HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models](https://arxiv.org/abs/2509.06596v1) | **Hallucination Mitigation**&**Attention Mechanisms**&**Faithful Decoding** |
| 25.09 | The Chinese University of Hong Kong | arxiv | [MESH -- Understanding Videos Like Human: Measuring Hallucinations in Large Video Models](https://arxiv.org/abs/2509.08538v2) | **Large Video Models**&**Video Hallucination**&**Benchmark** |



## üíªPresentations & Talks| 25.08 | University of Modena and Reggio Emilia | BMVC 2025 | [Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization](https://arxiv.org/abs/2508.20181v1) | **Multimodal LLMs**&**Hallucination Mitigation**&**Preference Optimization** |




## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

## üßë‚Äçüè´Scholars
