# Jailbreaks&Attack

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"

## üìëPapers

| Date  |                                                                                     Institute                                                                                     |          Publication          |                                                                                      Paper                                                                                       |                                                             Keywords                                                              |
|:-----:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------:|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------------------:|
| 20.12 |                                                                                      Google                                                                                       |     USENIX Security 2021      |                    [Extracting Training Data from Large Language Models](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting)                     |                                          **Verbatim Text Sequences**&**Rank Likelihood**                                          |
| 22.11 |                                                                                     AE Studio                                                                                     | NIPS2022(ML Safety Workshop)  |                                        [Ignore Previous Prompt: Attack Techniques For Language Models](https://arxiv.org/abs/2211.09527)                                         |                                                **Prompt Injection**&**Misaligned**                                                |
| 23.02 |                                                                                Saarland University                                                                                |             arxiv             |              [Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/abs/2302.12173)               |                      **Adversarial Prompting**&**Indirect Prompt Injection**&**LLM-Integrated Applications**                      |
| 23.04 |                                                                  Hong Kong University of Science and Technology                                                                   |      EMNLP2023(findings)      |                                              [Multi-step Jailbreaking Privacy Attacks on ChatGPT](https://arxiv.org/abs/2304.05197)                                              |                                                    **Privacy**&**Jailbreaks**                                                     |
| 23.04 |                                                              University of Michigan&Arizona State University&NVIDIA                                                               |           NAACL2024           |                      [ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger](https://arxiv.org/abs/2304.14475)                       |                          **Textual Backdoor Attack**&**Blackbox Generative Model**&**Trigger Detection**                          |
| 23.05 |                              Jinan University, Hong Kong University of Science and Technology, Nanyang Technological University, Zhejiang University                              |          EMNLP 2023           |                            [Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models](https://arxiv.org/abs/2305.01219)                            |                                                       **Backdoor Attacks**                                                        |
| 23.05 |                                                  Nanyang Technological University, University of New South Wales, Virginia Tech                                                   |             arXiv             |                                       [Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study](https://arxiv.org/abs/2305.13860)                                        |                                            Large **Jailbreak**&**Prompt Engineering**                                             |
| 23.06 |                                                                               Princeton University                                                                                |           AAAI 2024           |                                     [Visual Adversarial Examples Jailbreak Aligned Large Language Models](https://arxiv.org/abs/2306.13213)                                      |                                **Visual Language Models**&**Adversarial Attacks**&**AI Alignment**                                |
| 23.06 | Nanyang Technological University, University of New South Wales, Huazhong University of Science and Technology, Southern University of Science and Technology, Tianjin University |             arxiv             |                                         [Prompt Injection attack against LLM-integrated Applications](https://arxiv.org/abs/2306.05499)                                          |                         **&LLM-integrated Applications**&**Security Risks**&**Prompt Injection Attacks**                          |
| 23.06 |                                                                                      Google                                                                                       |             arxiv             |                                              [Are aligned neural networks adversarially aligned?](https://arxiv.org/abs/2306.15447)                                              |                                                   **Multimodal**&**Jailbreak**                                                    |
| 23.07 |                                                                                        CMU                                                                                        |             arxiv             |                                  [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)                                   |                                   **Jailbreak**&**Transferable Attack**&**Adversarial Attack**                                    |
| 23.07 |                                                            Language Technologies Institute Carnegie Mellon University                                                             |             arXiv             |                       [Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success](https://arxiv.org/abs/2307.06865)                       |                           **Prompt Extraction**&**Attack Success Measurement**&**Defensive Strategies**                           |
| 23.07 |                                                                         Nanyang Technological University                                                                          |           NDSS2023            |                                 [MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots](https://arxiv.org/abs/2307.08715)                                 |                                  **Jailbreak**&**Reverse-Engineering**&**Automatic Generation**                                   |
| 23.07 |                                                                                   Cornell Tech                                                                                    |             arxiv             |                               [Abusing Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs](https://arxiv.org/abs/2307.10490)                               |                       **Multi-Modal LLMs**&**Indirect Instruction Injection**&**Adversarial Perturbations**                       |
| 23.07 |                                                                   UNC Chapel Hill, Google DeepMind, ETH Zurich                                                                    | AdvML Frontiers Workshop 2023 |                                        [Backdoor Attacks for In-Context Learning with Language Models](https://arxiv.org/abs/2307.14692)                                         |                                           **Backdoor Attacks**&**In-Context Learning**                                            |
| 23.07 |                                                                                  Google DeepMind                                                                                  |             arXiv             |                               [Large language models (LLMs) are now highly capable at a diverse range of tasks](https://arxiv.org/abs/2307.15008)                                |                              **Adversarial Machine Learning**&**AI-Guardian**&**Defense Robustness**                              |
| 23.08 |                                                              CISPA Helmholtz Center for Information Security; NetApp                                                              |             arxiv             |                   [‚ÄúDo Anything Now‚Äù: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models](https://arxiv.org/abs/2308.03825)                    |                               **Jailbreak Prompts**&**Adversarial Prompts**&**Proactive Detection**                               |
| 23.09 |                                                                          Ben-Gurion University, DeepKeep                                                                          |             arxiv             |                                    [OPEN SESAME! UNIVERSAL BLACK BOX JAILBREAKING OF LARGE LANGUAGE MODELS](https://arxiv.org/abs/2309.01446)                                    |                               **Genetic Algorithm**&**Adversarial Prompt**&**Black Box Jailbreak**                                |
| 23.10 |                                                      Princeton University, Virginia Tech, IBM Research, Stanford University                                                       |             arxiv             |                           [FINE-TUNING ALIGNED LANGUAGE MODELS COMPROMISES SAFETY EVEN WHEN USERS DO NOT INTEND TO!](https://arxiv.org/abs/2310.03693)                           |                                     **Fine-tuning****Safety Risks**&**Adversarial Training**                                      |
| 23.10 |                                                 University of California Santa Barbara, Fudan University, Shanghai AI Laboratory                                                  |             arxiv             |                                   [SHADOW ALIGNMENT: THE EASE OF SUBVERTING SAFELY-ALIGNED LANGUAGE MODELS](https://arxiv.org/abs/2310.02949)                                    |                                          **AI Safety**&**Malicious Use**&**Fine-tuning**                                          |
| 23.10 |                                                                                 Peking University                                                                                 |             arxiv             |                             [Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations](https://arxiv.org/abs/2310.06387)                              |                           **In-Context Learning**&**Adversarial Attacks**&**In-Context Demonstrations**                           |
| 23.10 |                                                                            University of Pennsylvania                                                                             |             arxiv             |                                        [Jailbreaking Black Box Large Language Models in Twenty Queries](https://arxiv.org/abs/2310.08419)                                        |                                      **Prompt Automatic Iterative Refinement**&**Jailbreak**                                      |
| 23.10 |                                                                University of Maryland College Park, Adobe Research                                                                |             arxiv             |                              [AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2310.15140)                              |                                   **Adversarial Attacks**&**Interpretabilty**&**Jailbreaking**                                    |
| 23.11 |                                                                                      MBZUAI                                                                                       |             arxiv             |                             [Robustness Tests for Automatic Machine Translation Metrics with Adversarial Attacks](https://arxiv.org/abs/2311.00508)                              |                             **Adversarially-synthesized Texts**&**Word-level Attacks**&**Evaluation**                             |
| 23.11 |                                                                                 Palisade Research                                                                                 |             arxiv             |                                     [BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B](https://arxiv.org/abs/2311.00117)                                      |                                                   **Remove Safety Fine-tuning**                                                   |
| 23.11 |                                                                               University of Twente                                                                                |          ICNLSP 2023          |                                       [Efficient Black-Box Adversarial Attacks on Neural Text Detectors](https://arxiv.org/abs/2311.01873)                                       |                                           **Misclassification**&**Adversarial attacks**                                           |
| 23.11 |                                                                  PRISM AI&Harmony Intelligenc&Leap Laboratories                                                                   |             arxiv             |                          [Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation](https://arxiv.org/abs/2311.03348 )                          |                                **Persona-modulation Attacks**&**Jailbreaks**&**Automated Prompt**                                 |
| 23.11 |                                                                                Tsinghua University                                                                                |             arxiv             |                                   [Jailbreaking Large Vision-Language Models via Typographic Visual Prompts](https://arxiv.org/abs/2311.05608)                                   |                                   **Typographic Attack**&**Multi-modal**&**Safety Evaluation**                                    |
| 23.11 |                                                        Huazhong University of Science and Technology, Tsinghua University                                                         |             arxiv             |                 [Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration](https://arxiv.org/abs/2311.06062)                  |                                     **Membership Inference Attacks**&**Privacy and Security**                                     |
| 23.11 |                                                                          Nanjing University, Meituan Inc                                                                          |             arxiv             |                    [A Wolf in Sheep‚Äôs Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily](https://arxiv.org/abs/2311.08268)                    |                              **Jailbreak Prompts**&**Safety Alignment**&**Safeguard Effectiveness**                               |
| 23.11 |                                                                                  Google DeepMind                                                                                  |             arxiv             |               [Frontier Language Models Are Not Robust to Adversarial Arithmetic or "What Do I Need To Say So You Agree 2+2=5?"](https://arxiv.org/abs/2311.07587)               |                              **Adversarial Arithmetic**&**Model Robustness**&**Adversarial Attacks**                              |
| 23.11 |                                                               University of Illinois Chicago, Texas A&M University                                                                |             arxiv             |                         [DALA: A Distribution-Aware LoRA-Based Adversarial Attack against Pre-trained Language Models](https://arxiv.org/abs/2311.08598)                         |                                **Adversarial Attack**&**Distribution-Aware**&**LoRA-Based Attack**                                |
| 23.11 |                                                                         Illinois Institute of Technology                                                                          |             arxiv             |                   [Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment](https://arxiv.org/abs/2311.09433)                    |              Backdoor Activation Attack&Large Language Models&AI Safety&Activation Steering&Trojan Steering Vectors               |
| 23.11 |                                                                              Wayne State University                                                                               |             arXiv             |                                     [Hijacking Large Language Models via Adversarial In-Context Learning](https://arxiv.org/abs/2311.09948)                                      |                         **Adversarial Attacks**&**Gradient-Based Prompt Search**&**Adversarial Suffixes**                         |
| 23.11 |                                   Hong Kong Baptist University, Shanghai Jiao Tong University, Shanghai AI Laboratory, The University of Sydney                                   |             arXiv             |                                       [DeepInception: Hypnotize Large Language Model to Be Jailbreaker](https://arxiv.org/abs/2311.03191)                                        |                                                  **Jailbreak**&**DeepInception**                                                  |
| 23.11 |                                                                        Xi‚Äôan Jiaotong-Liverpool University                                                                        |             arxiv             |                                 [Generating Valid and Natural Adversarial Examples with Large Language Models](https://arxiv.org/abs/2311.11861)                                 |                                         **Adversarial examples**&**Text classification**                                          |
| 23.11 |                                                                             Michigan State University                                                                             |             arxiv             |                               [Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems](https://arxiv.org/abs/2311.11796)                                |                                  **Transferable Attacks**&**AI Systems**&**Adversarial Attacks**                                  |
| 23.11 |                                                                     Tsinghua University & Kuaishou Technology                                                                     |             arxiv             |                                         [Evil Geniuses: Delving into the Safety of LLM-based Agents](https://arxiv.org/abs/2311.11855v1)                                         |                                       **LLM-based Agents**&**Safety**&**Malicious Attacks**                                       |
| 23.11 |                                                                                Cornell University                                                                                 |             arxiv             |                                                           [Language Model Inversion](https://arxiv.org/abs/2311.13647)                                                           |                                     **Model Inversion**&**Prompt Reconstruction**&**Privacy**                                     |
| 23.11 |                                                                                    ETH Zurich                                                                                     |             arxiv             |                                          [Universal Jailbreak Backdoors from Poisoned Human Feedback](https://arxiv.org/abs/2311.14455)                                          |                                                   **RLHF**&**Backdoor Attacks**                                                   |
| 23.11 |                                                                          UC Santa Cruz, UNC-Chapel Hill                                                                           |             arxiv             |                                  [How Many Are in This Image? A Safety Evaluation Benchmark for Vision LLMs](https://arxiv.org/abs/2311.16101)                                   |                         **Vision Large Language Models**&**Safety Evaluation**&A**dversarial Robustness**                         |
| 23.11 |                                                                               Texas Tech University                                                                               |             arxiv             |                        [Exploiting Large Language Models (LLMs) through Deception Techniques and Persuasion Principles](https://arxiv.org/abs/2311.14876)                        |                                    **Social Engineering**&**Security**&**Prompt Engineering**                                     |
| 23.11 |                                                                             Johns Hopkins University                                                                              |             arxiv             |                                        [Instruct2Attack: Language-Guided Semantic Adversarial Attacks](https://arxiv.org/abs/2311.15551)                                         |                          **Language-guided Attacks**&**Latent Diffusion Models**&**Adversarial Attack**                           |
| 23.11 |                                                 Google DeepMind, University of Washington, Cornell, CMU, UC Berkeley, ETH Zurich                                                  |             arxiv             |                                    [Scalable Extraction of Training Data from (Production) Language Models](https://arxiv.org/abs/2311.17035)                                    |                              **Extractable Memorization**&**Data Extraction**&**Adversary Attacks**                               |
| 23.11 |                                    University of Maryland, Mila, Towards AI, Stanford, Technical University of Sofia, University of Milan, NYU                                    |             arxiv             |        [Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition](https://arxiv.org/abs/2311.16119)        |                                              **Prompt Hacking**&**Security Threats**                                              |
| 23.11 |                                               University of Washington, UIUC, Pennsylvania State University, University of Chicago                                                |             arxiv             |                                  [IDENTIFYING AND MITIGATING VULNERABILITIES IN LLM-INTEGRATED APPLICATIONS](https://arxiv.org/abs/2311.16153)                                   |                                        **LLM-Integrated Applications**&**Attack Surfaces**                                        |
| 23.11 |                                      Jinan University, Guangzhou Xuanyuan Research Institute Co. Ltd., The Hong Kong Polytechnic University                                       |             arxiv             |                            [TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4](https://arxiv.org/abs/2311.17429)                            |                                           **Prompt-based Learning**&**Backdoor Attack**                                           |
| 23.11 |                                                                          Nanjing University&Meituan Inc.                                                                          |           NAACL2024           |                    [A Wolf in Sheep‚Äôs Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily](https://arxiv.org/abs/2311.08268)                    |                                  **Jailbreak Prompts**&**LLM Security**&**Automated Framework**                                   |
| 23.11 |                                                                         University of Southern California                                                                         |      NAACL2024(findings)      |                           [Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking](https://arxiv.org/abs/2311.09827)                            |                                 **Jailbreaking**&**Large Language Models**&**Cognitive Overload**                                 |
| 23.12 |                                                                         The Pennsylvania State University                                                                         |             arxiv             |                             [Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections](https://arxiv.org/abs/2312.00027)                             |                                            **Backdoor Injection**&**Safety Alignment**                                            |
| 23.12 |                                                                                 Drexel University                                                                                 |             arXiv             |                          [A Survey on Large Language Model (LLM) Security and Privacy: The Good the Bad and the Ugly](https://arxiv.org/abs/2312.02003)                          |                                               **Security**&**Privacy**&**Attacks**                                                |
| 23.12 |                                                                       Yale University, Robust Intelligence                                                                        |             arXiv             |                                          [Tree of Attacks: Jailbreaking Black-Box LLMs Automatically](https://arxiv.org/abs/2312.02119)                                          |                           **Tree of Attacks with Pruning (TAP)**&**Jailbreaking**&**Prompt Generation**                           |
| 23.12 |                                                                       Independent (Now at Google DeepMind)                                                                        |             arXiv             |                                      [Scaling Laws for Adversarial Attacks on Language Model Activations](https://arxiv.org/abs/2312.02780)                                      |                              **Adversarial Attacks**&**Language Model Activations**&**Scaling Laws**                              |
| 23.12 |                                                                          Harbin Institute of Technology                                                                           |             arxiv             |                          [Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak](https://arxiv.org/abs/2312.04127)                          |                           **Jailbreak Attack**&**Inherent Response Tendency**&**Affirmation Tendency**                            |
| 23.12 |                                                                          University of Wisconsin-Madison                                                                          |             arxiv             |                      [DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial Natural Language Instructions](https://arxiv.org/abs/2312.04730)                       |                                   **Code Generation**&**Adversarial Attacks**&**Cybersecurity**                                   |
| 23.12 |                                                                      Carnegie Melon University, IBM Research                                                                      |             arxiv             |                              [Forcing Generative Models to Degenerate Ones: The Power of Data Poisoning Attacks](https://arxiv.org/abs/2312.04748)                               |                           **Data Poisoning Attacks**&**Natural Language Generation**&**Cybersecurity**                            |
| 23.12 |                                                                                 Purdue University                                                                                 |      NIPS2023ÔºàWorkshopÔºâ       |                               [Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs](https://arxiv.org/abs/2312.04782)                                |                              **Knowledge Extraction**&**Interrogation Techniques**&**Cybersecurity**                              |
| 23.12 |                                                                 Sungkyunkwan University, University of Tennessee                                                                  |             arXiv             |     [Poisoned ChatGPT Finds Work for Idle Hands: Exploring Developers‚Äô Coding Practices with Insecure Suggestions from Poisoned AI Models](https://arxiv.org/abs/2312.06227)     |                                          **Poisoning Attacks**&**Software Development**                                           |
| 23.12 |                                                     North Carolina State University, New York University, Stanford University                                                     |             arXiv             |            [BEYOND GRADIENT AND PRIORS IN PRIVACY ATTACKS: LEVERAGING POOLER LAYER INPUTS OF LANGUAGE MODELS IN FEDERATED LEARNING](https://arxiv.org/abs/2312.05720)            |                                            **Federated Learning**&P**rivacy Attacks**                                             |
| 23.12 |                                                            Korea Advanced Institute of Science, Graduate School of AI                                                             |             arxiv             |                                                [Hijacking Context in Large Multi-modal Models](https://arxiv.org/abs/2312.07553)                                                 |                                        **Large Multi-modal Models**&**Context Hijacking**                                         |
| 23.12 |                                           Xi‚Äôan Jiaotong University, Nanyang Technological University, Singapore Management University                                            |             arXiv             |                                    [A Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection](https://arxiv.org/abs/2312.10766)                                     |                                            **Jailbreaking Detection**&**Multi-Modal**                                             |
| 23.12 |                                                               Logistic and Supply Chain MultiTech R&D Centre (LSCM)                                                               |          UbiSec-2023          |                [A Comprehensive Survey of Attack Techniques Implementation and Mitigation Strategies in Large Language Models](https://arxiv.org/abs/2312.10982)                 |                                         **Cybersecurity Attacks**&**Defense Strategies**                                          |
| 23.12 |                                                             University of Illinois Urbana-Champaign, VMware Research                                                              |             arXiv             |                                    [BYPASSING THE SAFETY TRAINING OF OPEN-SOURCE LLMS WITH PRIMING ATTACKS](https://arxiv.org/abs/2312.12321)                                    |                                              **Safety Training**&**Priming Attacks**                                              |
| 23.12 |                                                                          Delft University of Technology                                                                           |           ICSE 2024           |                                           [Traces of Memorisation in Large Language Models for Code](https://arxiv.org/abs/2312.11658)                                           |                                         **Code Memorisation**&**Data Extraction Attacks**                                         |
| 23.12 |                                     University of Science and Technology of China, Hong Kong University of Science and Technology, Microsoft                                      |             arxiv             |                        [Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models](https://arxiv.org/abs/2312.14197)                         |                               **Indirect Prompt Injection Attacks**&**BIPIA Benchmark**&**Defense**                               |
| 23.12 |                                                                Nanjing University of Aeronautics and Astronautics                                                                 |           NLPCC2023           |                                      [Punctuation Matters! Stealthy Backdoor Attack for Language Models](https://arxiv.org/abs/2312.15867)                                       |                                        **Backdoor Attack**&**PuncAttack**&**Stealthiness**                                        |
| 23.12 |                                                             FAR AI, McGill University, MILA, Jagiellonian University                                                              |             arXiv             |                                                         [Exploiting Novel GPT-4 APIs](https://arxiv.org/abs/2312.14302)                                                          |                               **Fine-Tuning**&**Knowledge Retrieval**&**Security Vulnerabilities**                                |
| 23.12 |                                                                                       EPFL                                                                                        |                               |                                        [Adversarial Attacks on GPT-4 via Simple Random Search](https://www.andriushchenko.me/gpt4adv.pdf)                                        |                                      **Adversarial Attacks**&**Random Search**&**Jailbreak**                                      |
| 24.01 |                                                               Logistic and Supply Chain MultiTech R&D Centre (LSCM)                                                               |           CSDE2023            |               [A Novel Evaluation Framework for Assessing Resilience Against Prompt Injection Attacks in Large Language Models](https://arxiv.org/abs/2401.00991)                |                                      **Evaluation**&**Prompt Injection**&**Cyber Security**                                       |
| 24.01 |                                                                         University of Southern California                                                                         |             arxiv             |              [The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance](https://arxiv.org/abs/2401.03729)              |                                   **Prompt Engineering**&**Text Classification**&**Jailbreaks**                                   |
| 24.01 |                                                     Virginia Tech, Renmin University of China, UC Davis, Stanford University                                                      |             arxiv             |                  [How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs](https://arxiv.org/abs/ÈìæÊé•ÂæÖÂÆö)                   |                                  **AI Safety**&**Persuasion Adversarial Prompts**&**Jailbreak**                                   |
| 24.01 |                                                    Anthropic, Redwood Research, Mila Quebec AI Institute, University of Oxford                                                    |             arxiv             |                                 [SLEEPER AGENTS: TRAINING DECEPTIVE LLMS THAT PERSIST THROUGH SAFETY TRAINING](https://arxiv.org/abs/2401.05566)                                 |                    **Deceptive Behavior**&**Safety Training**&**Backdoored Behavior**&**Adversarial Training**                    |
| 24.01 |                                          Jinan University,Nanyang Technological University, Beijing Institute of Technology, Pazhou Lab                                           |             arxiv             |                           [UNIVERSAL VULNERABILITIES IN LARGE LANGUAGE MODELS: IN-CONTEXT LEARNING BACKDOOR ATTACKS](https://arxiv.org/abs/2401.05949)                           |                                     **In-context Learning**&**Security**&**Backdoor Attacks**                                     |
| 24.01 |                                                                            Carnegie Mellon University                                                                             |             arxiv             |                                            [Combating Adversarial Attacks with Multi-Agent Debate](https://arxiv.org/abs/2401.05998)                                             |                                    **Adversarial Attacks**&**Multi-Agent Debate**&**Red Team**                                    |
| 24.01 |                                                                                 Fudan University                                                                                  |             arxiv             |                             [Open the Pandora‚Äôs Box of LLMs: Jailbreaking LLMs through Representation Engineering](https://arxiv.org/abs/2401.06824)                             |                                          **LLM Security**&**Representation Engineering**                                          |
| 24.01 |                                             Northwestern University, New York University, University of Liverpool, Rutgers University                                             |             arxiv             |                        [AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models](https://arxiv.org/abs/2401.09002)                         |                              **Jailbreak Attack**&**Evaluation Frameworks**&**Ground Truth Dataset**                              |
| 24.01 |                                                                          Kyushu Institute of Technology                                                                           |             arxiv             |                                   [ALL IN HOW YOU ASK FOR IT: SIMPLE BLACK-BOX METHOD FOR JAILBREAK ATTACKS](https://arxiv.org/abs/2401.09798)                                   |                                            **Jailbreak Attacks**&**Black-box Method**                                             |
| 24.01 |                                                                                        MIT                                                                                        |             arXiv             |                         [Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning](https://arxiv.org/abs/2401.10862)                          |                                                 **Jailbreaking**&**Model Safety**                                                 |
| 24.01 |                                                                                Aalborg University                                                                                 |             arxiv             |                                       [Text Embedding Inversion Attacks on Multilingual Language Models](https://arxiv.org/abs/2401.12192)                                       |                             **Text Embedding**&**Inversion Attacks**&**Multilingual Language Models**                             |
| 24.01 |                                         University of Illinois Urbana-Champaign, University of Washington, Western Washington University                                          |             arxiv             |                                   [BADCHAIN: BACKDOOR CHAIN-OF-THOUGHT PROMPTING FOR LARGE LANGUAGE MODELS](https://arxiv.org/abs/2401.12242)                                    |                                        **Chain-of-Thought Prompting**&**Backdoor Attacks**                                        |
| 24.01 |                                                                 The University of Hong Kong, Zhejiang University                                                                  |             arxiv             |                                                      [Red Teaming Visual Language Models](https://arxiv.org/abs/2401.12915)                                                      |                                            **Vision-Language Models**&**Red Teaming**                                             |
| 24.01 |                                              University of California Santa Barbara,Sea AI Lab Singapore, Carnegie Mellon University                                              |             arxiv             |                                             [Weak-to-Strong Jailbreaking on Large Language Models](https://arxiv.org/abs/2401.17256)                                             |                                      **Jailbreaking**&**Adversarial Prompts**&**AI Safety**                                       |
| 24.02 |                                                                                 Boston University                                                                                 |             arxiv             |                                   [Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks](https://arxiv.org/abs/2402.00626)                                    |                        **Large Vision-Language Models**&**Typographic Attacks**&**Self-Generated Attacks**                        |
| 24.02 |                                                                   Copenhagen Business School, Temple University                                                                   |             arxiv             |                                 [An Early Categorization of Prompt Injection Attacks on Large Language Models](https://arxiv.org/abs/2402.00898)                                 |                                              **Prompt Injection**&**Categorization**                                              |
| 24.02 |                                                   Michigan State University, Okinawa Institute of Science and Technology (OIST)                                                   |             arxiv             |                                                    [Data Poisoning for In-context Learning](https://arxiv.org/abs/2402.02160)                                                    |                                      **In-context learning**&**Data poisoning**&**Security**                                      |
| 24.02 |                                                                  CISPA Helmholtz Center for Information Security                                                                  |             arxiv             |                                            [Conversation Reconstruction Attack Against GPT Models](https://arxiv.org/abs/2402.02987)                                             |                               **Conversation Reconstruction Attack**&**Privacy risks**&**Security**                               |
| 24.02 |                                 University of Illinois Urbana-Champaign, Center for AI Safety, Carnegie Mellon University, UC Berkeley, Microsoft                                 |             arxiv             |                         [HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal](https://arxiv.org/abs/2402.04249)                          |                                           **Automated Red Teaming**&**Robust Refusal**                                            |
| 24.02 |                                           University of Washington, University of Virginia, Allen Institute for Artificial Intelligence                                           |             arxiv             |                                        [Do Membership Inference Attacks Work on Large Language Models?](https://arxiv.org/abs/2402.07841)                                        |                                     **Membership Inference Attacks**&**Privacy**&**Security**                                     |
| 24.02 |                                                 Pennsylvania State University, Wuhan University, Illinois Institute of Technology                                                 |             arxiv             |                     [PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2402.07867)                      |                                **Knowledge Poisoning Attacks**&**Retrieval-Augmented Generation**                                 |
| 24.02 |                                                             Purdue University, University of Massachusetts at Amherst                                                             |             arxiv             |                            [RAPID OPTIMIZATION FOR JAILBREAKING LLMS VIA SUBCONSCIOUS EXPLOITATION AND ECHOPRAXIA](https://arxiv.org/abs/2402.05467)                             |                                               **Jailbreaking LLM**&**Optimization**                                               |
| 24.02 |                                                                  CISPA Helmholtz Center for Information Security                                                                  |             arxiv             |                                          [Comprehensive Assessment of Jailbreak Attacks Against LLMs](https://arxiv.org/abs/2402.05668)                                          |                                   **Jailbreak Attacks**&**Attack Methods**&**Policy Alignment**                                   |
| 24.02 |                                                                                    UC Berkeley                                                                                    |             arxiv             |                                      [StruQ: Defending Against Prompt Injection with Structured Queries](https://arxiv.org/abs/2402.06363)                                       |                            **Prompt Injection Attacks**&**Structured Queries**&**Defense Mechanisms**                             |
| 24.02 |                                  Nanyang Technological University, Huazhong University of Science and Technology, University of New South Wales                                   |             arxiv             |                                     [PANDORA: Jailbreak GPTs by Retrieval Augmented Generation Poisoning](https://arxiv.org/abs/2402.08416)                                      |                                  **Jailbreak Attacks**&**Retrieval Augmented Generation (RAG)**                                   |
| 24.02 |                                                                          Sea AI Lab, Southern University                                                                          |             arxiv             |                                        [Test-Time Backdoor Attacks on Multimodal Large Language Models](https://arxiv.org/abs/2402.08577)                                        |                   **Backdoor Attacks**&**Multimodal Large Language Models (MLLMs)**&**Adversarial Test Images**                   |
| 24.02 |                                      University of Illinois at Urbana‚ÄìChampaign, University of California, San Diego, Allen Institute for AI                                      |             arxiv             |                                     [COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability](https://arxiv.org/abs/2402.08679)                                     |                                         **Jailbreaks**&**Controllable Attack Generation**                                         |
| 24.02 |                                                                                    ISCAS, NTU                                                                                     |             arxiv             |                                  [Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues](https://arxiv.org/abs/2402.09091)                                  |                                       **Jailbreak Attacks**&**Indirect Attack**&**Puzzler**                                       |
| 24.02 |                                                     √âcole Polytechnique F√©d√©rale de Lausanne, University of Wisconsin-Madison                                                     |             arxiv             |                               [Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks](https://arxiv.org/abs/2402.09177)                               |                         **Jailbreaking Attacks**&**Contextual Interaction**&**Multi-Round Interactions**                          |
| 24.02 |                                 University of Electronic Science and Technology of China, CISPA Helmholtz Center for Information Security, NetApp                                 |             arxiv             |                             [Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization](https://arxiv.org/abs/2402.09179)                              |                                    **Customization**&**Instruction Backdoor Attacks**&**GPTs**                                    |
| 24.02 |                                                                    Shanghai Artificial Intelligence Laboratory                                                                    |             arxiv             |                                   [Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey](https://arxiv.org/abs/2402.09283)                                    |                                       **LLM Conversation Safety**&**Attacks**&**Defenses**                                        |
| 24.02 |                                                                         UC Berkeley, New York University                                                                          |             arxiv             |                                         [PAL: Proxy-Guided Black-Box Attack on Large Language Models](https://arxiv.org/abs/2402.09674)                                          |                                       **Black-Box Attack**&**Proxy-Guided Attack**&**PAL**                                        |
| 24.02 |                                                                    Center for Human-Compatible AI, UC Berkeley                                                                    |             arxiv             |                                                     [A STRONGREJECT for Empty Jailbreaks](https://arxiv.org/abs/2402.10260)                                                      |                                         **Jailbreaks**&**Benchmarking**&**StrongREJECT**                                          |
| 24.02 |                                                                             Arizona State University                                                                              |             arxiv             |                                [Jailbreaking Proprietary Large Language Models using Word Substitution Cipher](https://arxiv.org/abs/2402.10601)                                 |                                **Jailbreak**&**Word Substitution Cipher**&**Attack Success Rate**                                 |
| 24.02 |                                                         Renmin University of China, Beijing, Peking University, WeChat AI                                                         |             arxiv             |                                [Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents](https://arxiv.org/abs/2402.11208)                                 |                                        **Backdoor Attacks**&**Agent Safety**&**Framework**                                        |
| 24.02 |                                               University of Washington, UIUC, Western Washington University, University of Chicago                                                |             arxiv             |                                      [ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs](https://arxiv.org/abs/2402.11753)                                       |                                     **ASCII Art**&**Jailbreak Attacks**&**Safety Alignment**                                      |
| 24.02 |      Jinan University, Nanyang Technological University, Zhejiang University, Hong Kong University of Science and Technology, Beijing Institute of Technology, Sony Research      |             arxiv             |                           [Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.12168)                            | **Weight-Poisoning Backdoor Attacks**&**Parameter-Efficient Fine-Tuning (PEFT)**&**Poisoned Sample Identification Module (PSIM)** |
| 24.02 |                                                                  CISPA Helmholtz Center for Information Security                                                                  |             arxiv             |                                            [Prompt Stealing Attacks Against Large Language Models](https://arxiv.org/abs/2402.12959)                                             |                                                **Prompt Engineering**&**Security**                                                |
| 24.02 |                        University of New South Wales Australia, Delft University of Technology The Netherlands&Nanyang Technological University Singapore                         |             arxiv             |                                    [LLM Jailbreak Attack versus Defense Techniques - A Comprehensive Study](https://arxiv.org/abs/2402.13457)                                    |                                           **Jailbreak Attacks**&**Defense Techniques**                                            |
| 24.02 |                                                               Wayne State University, University of Michigan-Flint                                                                |             arxiv             |                                      [Learning to Poison Large Language Models During Instruction Tuning](https://arxiv.org/abs/2402.13459)                                      |                                              **Data Poisoning**&**Backdoor Attacks**                                              |
| 24.02 |                                            Nanyang Technological University, Zhejiang University, The Chinese University of Hong Kong                                             |             arxiv             |                                [Backdoor Attacks on Dense Passage Retrievers for Disseminating Misinformation](https://arxiv.org/abs/2402.13532)                                 |                                **Dense Passage Retrieval**&**Backdoor Attacks**&**Misinformation**                                |
| 24.02 |                                                                              University of Michigan                                                                               |             arxiv             |                             [PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails](https://arxiv.org/abs/2402.15911)                              |                                        **Universal Adversarial Prefixes**&**Guard Models**                                        |
| 24.02 |                                                                                       Meta                                                                                        |             arxiv             |                                    [Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts](https://arxiv.org/abs/2402.16822)                                     |                                     **Adversarial Prompts**&**Quality-Diversity**&**Safety**                                      |
| 24.02 |                                                                                 Fudan University                                                                                  |             arxiv             |                           [CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models](https://arxiv.org/abs/2402.16717)                            |                                         **Personalized Encryption**&**Safety Mechanisms**                                         |
| 24.02 |                                                                            Carnegie Mellon University                                                                             |             arxiv             |                                            [Attacking LLM Watermarks by Exploiting Their Strengths](https://arxiv.org/abs/2402.16187)                                            |                                            **LLM Watermarks**&**Adversarial Attacks**                                             |
| 24.02 |                                                                                Beihang University                                                                                 |             arxiv             |         [From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings](https://arxiv.org/abs/2402.16006)          |                                       **Adversarial Suffix**&**Text Embedding Translation**                                       |
| 24.02 |                                                                        University of Maryland College Park                                                                        |             arxiv             |                                        [Fast Adversarial Attacks on Language Models In One GPU Minute](https://arxiv.org/abs/2402.15570)                                         |                                  **Adversarial Attacks**&**BEAST**&**Computational Efficiency**                                   |
| 24.02 |                                                                    Shanghai Artificial Intelligence Laboratory                                                                    |             arxiv             |                                    [Attacks Defenses and Evaluations for LLM Conversation Safety: A Survey](https://arxiv.org/abs/2402.09283)                                    |                                                **Conversation Safety**&**Survey**                                                 |
| 24.02 |                                                    Beijing University of Posts and Telecommunications, University of Michigan                                                     |             arxiv             |                           [Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue](https://arxiv.org/abs/2402.17262)                            |                                         **Multi-turn Dialogue**&**Safety Vulnerability**                                          |
| 24.02 |                                        University of California, The Hongkong University of Science and Technology, University of Maryland                                        |             arxiv             |                              [DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers](https://arxiv.org/abs/2402.16914)                               |                                         **Jailbreaking Attacks**&**Prompt Decomposition**                                         |
| 24.02 |                                                           Massachusetts Institute of Technology, MIT-IBM Watson AI Lab                                                            |             arxiv             |                                            [CURIOSITY-DRIVEN RED-TEAMING FOR LARGE LANGUAGE MODELS](https://arxiv.org/abs/2402.19464)                                            |                                         **Curiosity-Driven Exploration**&**Red Teaming**                                          |
| 24.02 |       SKLOIS Institute of Information Engineering Chinese Academy of Science, School of Cyber Security University of Chinese Academy of Sciences,Tsinghua University,RealAI       |             arxiv             |                [Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction](https://arxiv.org/abs/2402.18104)                 |                                **Jailbreaking**&**Large Language Models**&**Adversarial Attacks**                                 |
| 24.03 |                                                                   Rice University, Samsung Electronics America                                                                    |             arxiv             |                                   [LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario](https://arxiv.org/abs/2403.00108)                                   |                              **Low-Rank Adaptation (LoRA)**&**Backdoor Attacks**&**Model Security**                               |
| 24.03 |                                                                            The University of Hong Kong                                                                            |             arxiv             |                                        [ImgTrojan: Jailbreaking Vision-Language Models with ONE Image](https://arxiv.org/abs/2403.02910)                                         |                               **Vision-Language Models**&**Data Poisoning**&**Jailbreaking Attack**                               |
| 24.03 |                                                                                  SPRING Lab EPFL                                                                                  |             arxiv             |                          [Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks](https://arxiv.org/abs/2403.03792)                           |                             **Prompt Injection Attacks**&**Optimization-Based Approach**&**Security**                             |
| 24.03 |                                            Shanghai University of Finance and Economics, Southern University of Science and Technology                                            |             arxiv             |                                    [Tastle: Distract Large Language Models for Automatic Jailbreak Attack](https://arxiv.org/abs/2403.08424)                                     |                                           **Jailbreak Attack**&**Black-box Framework**                                            |
| 24.03 |                                                 Google DeepMind, ETH Zurich, University of Washington, OpenAI, McGill University                                                  |             arxiv             |                                                 [Stealing Part of a Production Language Model](https://arxiv.org/abs/2403.06634)                                                 |                                        **Model Stealing**&**Language Models**&**Security**                                        |
| 24.03 |                                                                              University of Edinburgh                                                                              |             arxiv             |                      [Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks](https://arxiv.org/abs/2403.09832)                       |                             **Prompt Injection Attacks**&**Machine Translation**&**Inverse Scaling**                              |
| 24.03 |                                                                         Nanyang Technological University                                                                          |             arxiv             |                                         [BADEDIT: BACKDOORING LARGE LANGUAGE MODELS BY MODEL EDITING](https://arxiv.org/abs/2403.13355)                                          |                                        **Backdoor Attacks**&**Model Editing**&**Security**                                        |
| 24.03 |                                                                     Fudan University, Shanghai AI Laboratory                                                                      |             arxiv             |                                  [EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models](https://arxiv.org/abs/2403.12171)                                   |                                         **Jailbreak Attacks**&**Security**&**Framework**                                          |
| 24.03 |                          Shanghai Key Lab of Intelligent Information Processing, Fudan University, Shanghai Engineering Research Center of AI & Robotics                          |             arxiv             |          [Improving Adversarial Transferability of Visual-Language Pre-training Models through Collaborative Multimodal Interaction](https://arxiv.org/abs/2403.10883)           |                    **Vision-Language Pre-trained Model**&**Adversarial Transferability**&**Black-Box Attack**                     |
| 24.03 |                                                                                     Microsoft                                                                                     |             arxiv             |                             [Securing Large Language Models: Threats, Vulnerabilities, and Responsible Practices](https://arxiv.org/abs/2403.12503)                              |                                              **Security Risks**&**Vulnerabilities**                                               |
| 24.03 |                                                                            Carnegie Mellon University                                                                             |             arxiv             |                                                  [Jailbreaking is Best Solved by Definition](https://arxiv.org/abs/2403.14725)                                                   |                                            **Jailbreak Attacks**&**Adaptive Attacks**                                             |
| 24.03 |                                                                              ShanghaiTech University                                                                              |             arxiv             |                            [LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models](https://arxiv.org/abs/2403.16432)                             |                     **Universal Adversarial Triggers**&**Prompt-based Learning**&**Natural Language Attack**                      |
| 24.03 |                                   Huazhong University of Science and Technology, Lehigh University, University of Notre Dame & Duke University                                    |             arxiv             |                                         [Optimization-based Prompt Injection Attack to LLM-as-a-Judge](https://arxiv.org/abs/2403.17710)                                         |                                  **Prompt Injection Attack**&**LLM-as-a-Judge**&**Optimization**                                  |
| 24.03 |                                           Washington University in St. Louis, University of Wisconsin - Madison, John Burroughs School                                            |     USENIX Security 2024      |                          [Don‚Äôt Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models](https://arxiv.org/abs/2403.19260)                          |                                                **Jailbreak Prompts**&**Security**                                                 |
| 24.03 |                                                       School of Information Science and Technology, ShanghaiTech University                                                       |           NAACL2024           |                            [LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models](https://arxiv.org/abs/2403.16432)                             |                 **Prompt-based Language Models**&**Universal Adversarial Triggers**&**Natural Language Attacks**                  |
| 24.04 |                                                               University of Pennsylvania, ETH Zurich, EPFL, Sony AI                                                               |             arxiv             |                             [JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models](https://arxiv.org/abs/2404.01318)                              |                                         **Jailbreaking Attacks**&**Robustness Benchmark**                                         |
| 24.04 |                                                                  Microsoft Azure, Microsoft, Microsoft Research                                                                   |             arxiv             |                                                [The Crescendo Multi-Turn LLM Jailbreak Attack](https://arxiv.org/abs/2404.01833)                                                 |                                         **Jailbreak Attacks**&**Multi-Turn Interaction**                                          |
| 24.04 |                                                                                       EPFL                                                                                        |             arxiv             |                                    [Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks](https://arxiv.org/abs/2404.02151)                                     |                                               **Adaptive Attacks**&**Jailbreaking**                                               |
| 24.04 |                                                            The Ohio State University, University of Wisconsin-Madison                                                             |             arxiv             |            [JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks](https://arxiv.org/abs/2404.03027)            |                             **Multimodal Large Language Models**&**Jailbreak Attacks**&**Benchmark**                              |
| 24.04 |                                                                                    Enkrypt AI                                                                                     |             arxiv             |                                       [INCREASED LLM VULNERABILITIES FROM FINE-TUNING AND QUANTIZATION](https://arxiv.org/abs/2404.04392)                                        |                                     **Fine-tuning**&**Quantization**&**LLM Vulnerabilities**                                      |
| 24.04 |                                                           The Pennsylvania State University, Carnegie Mellon University                                                           |             arxiv             |               [Hidden You Malicious Goal Into Benigh Narratives: Jailbreak Large Language Models through Logic Chain Injection](https://arxiv.org/abs/2404.04849)                |                                            **LLM**&**Jailbreak**&**Prompt Injection**                                             |
| 24.04 |                                                                Technical University of Darmstadt, Google Research                                                                 |             arxiv             |                                     [Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data](https://arxiv.org/abs/2404.05530)                                      |              **Reinforcement Learning from Human Feedback**&**Poisoned Preference Data**&**Language Model Security**              |
| 24.04 |                                                                                 Purdue University                                                                                 |             arxiv             |                                             [Rethinking How to Evaluate Language Model Jailbreak](https://arxiv.org/abs/2404.06407)                                              |                                               **Jailbreak**&**Evaluation Metrics**                                                |
| 24.04 |                                                 Xi‚Äôan Jiaotong-Liverpool University, Rutgers University, University of Liverpool                                                  |             arxiv             |                                   [Goal-guided Generative Prompt Injection Attack on Large Language Models](https://arxiv.org/abs/2404.07234)                                    |                                   **Prompt Injection**&**Robustness**&**Mahalanobis Distance**                                    |
| 24.04 |                                                                              University of New Haven                                                                              |             arxiv             |                                       [SANDWICH ATTACK: MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS](https://arxiv.org/abs/2404.07242)                                        |                                  **Multi-language Mixture**&**Adaptive Attack**&**LLM Security**                                  |
| 24.04 |                                                                             The Ohio State University                                                                             |             arxiv             |     [AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs](https://arxiv.org/abs/2404.07921)      |                                                 **Adversarial Suffix Generation**                                                 |
| 24.04 |                                                                  Renmin University of China, Microsoft Research                                                                   |             arxiv             |                                [Uncovering Safety Risks in Open-source LLMs through Concept Activation Vector](https://arxiv.org/abs/2404.12038)                                 |                                                   **Safety**&**Attack Methods**                                                   |
| 24.04 |                                                                                Zhejiang University                                                                                |             arxiv             |                              [JailbreakLens: Visual Analysis of Jailbreak Attacks Against Large Language Models](https://arxiv.org/abs/2404.08793)                               |                                            **Jailbreak Attacks**&**Visual Analytics**                                             |
| 24.04 |                                                                              Shanghaitech University                                                                              |             arxiv             |                                            [Don‚Äôt Say No: Jailbreaking LLM by Suppressing Refusal](https://arxiv.org/abs/2404.16369)                                             |                                         **Jailbreaking Attacks**&**Adversarial Attacks**                                          |
| 24.04 |                                            University of Electronic Science and Technology of China,  Chengdu University of Technology                                            |             arxiv             |                                       [TALK TOO MUCH: Poisoning Large Language Models under Token Limit](https://arxiv.org/abs/2404.14795)                                       |                                             **Token Limitation**&**Poisoning Attack**                                             |
| 24.04 |                                                     ETH Zurich,  EPFL, University of Twente, Georgia Institute of Technology                                                      |             arxiv             |                                  [Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs](https://arxiv.org/abs/2404.14461)                                   |                             **Aligned LLMs**&**Universal Jailbreak Backdoors**&**Poisoning Attacks**                              |
| 24.04 |                                                                                        N/A                                                                                        |             arxiv             |                           [Trojan Detection in Large Language Models: Insights from The Trojan Detection Challenge](https://arxiv.org/abs/2404.13660)                            |                                             **Trojan Detection**&**Model Robustness**                                             |
| 24.04 |                                                                           Shanghai Jiao Tong University                                                                           |             arxiv             |                              [Physical Backdoor Attack can Jeopardize Driving with Vision-Large-Language Models](https://arxiv.org/abs/2404.12916)                               |                               **Vision-Large-Language Models**&**Autonomous Driving**&**Security**                                |
| 24.04 |                                                          Max-Planck-Institute for Intelligent Systems, AI at Meta (FAIR)                                                          |             arxiv             |                                          [AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs](https://arxiv.org/abs/2404.16873)                                           |                                            **Adversarial Prompting**&**Safety in AI**                                             |
| 24.04 |                                           Singapore Management University, Shanghai Institute for Advanced Study of Zhejiang University                                           |             arxiv             |                                 [Evaluating and Mitigating Linguistic Discrimination in Large Language Models](https://arxiv.org/abs/2404.18534)                                 |                                     =**Linguistic Discrimination**&**Jailbreak**&**Defense**                                      |
| 24.04 |                              University of Louisiana at Lafayette, Beijing Electronic Science and Technology Institute, The Johns Hopkins University                              |             arxiv             |                                    [Assessing Cybersecurity Vulnerabilities in Code Large Language Models](https://arxiv.org/abs/2404.18567)                                     |                                                           **Code LLMs**                                                           |
| 24.04 |                                       University College London, The University of Melbourne, Macquarie University, University of Edinburgh                                       |             arxiv             |                   [Transferring Troubles: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction Tuning](https://arxiv.org/abs/2404.19597)                   |                           **Cross-Lingual Transferability**&**Backdoor Attacks**&**Instruction Tuning**                           |
| 24.04 |                     University of Cambridge, Indian Institute of Technology Bombay, University of Melbourne, University College London, Macquarie University                      |      ICLR 2024 Workshop       |                                             [Attacks on Third-Party APIs of Large Language Models](https://arxiv.org/abs/2404.16891)                                             |                                                 **Third-Party API**&**Security**                                                  |
| 24.04 |                                                                           Purdue University, Fort Wayne                                                                           |           NAACL2024           |                                     [Vert Attack: Taking advantage of Text Classifiers‚Äô horizontal vision](https://arxiv.org/abs/2404.08538)                                     |                                    **Text Classifiers**&**Adversarial Attacks**&**VertAttack**                                    |
| 24.04 |                                                    The University of Melbourne&Macquarie University&University College London                                                     |           NAACL2024           |                                             [Backdoor Attacks on Multilingual Machine Translation](https://arxiv.org/abs/2404.02393)                                             |                              **Multilingual Machine Translation**&**Security**&**Backdoor Attacks**                               |
| 24.05 |                                                         Institute of Information Engineering, Chinese Academy of Sciences                                                         |             arxiv             |                    [Can LLMs Deeply Detect Complex Malicious Queries? A Framework for Jailbreaking via Obfuscating Intent](https://arxiv.org/abs/2405.03654)                     |                                   **Prompt Jailbreak Attack**&**Red Team**&**Black-box Attack**                                   |
| 24.05 |                                                                           University of Texas at Austin                                                                           |             arxiv             |                                            [Mitigating Exaggerated Safety in Large Language Models](https://arxiv.org/abs/2405.05418)                                            |                                        **Model Safety**&**Utility**&**Exaggerated Safety**                                        |
| 24.05 |                                                         Institute of Information Engineering, Chinese Academy of Sciences                                                         |             arxiv             |                                  [Chain of Attack: A Semantic-Driven Contextual Multi-Turn Attacker for LLM](https://arxiv.org/abs/2405.05610)                                   |                       **Multi-Turn Dialogue Attack**&**LLM Security**&**Semantic-Driven Contextual Attack**                       |
| 24.05 |                                                                                 Peking University                                                                                 |      ICLR 2024 Workshop       |                                                   [BOOSTING JAILBREAK ATTACK WITH MOMENTUM](https://arxiv.org/abs/2405.01229)                                                    |                                             **Jailbreak Attack**&**Momentum Method**                                              |
| 24.05 |                                                                     √âcole Polytechnique F√©d√©rale de Lausanne                                                                      |           ICML 2024           |                                                [Revisiting character-level adversarial attacks](https://arxiv.org/abs/2405.04346)                                                |                                       **Character-level Adversarial Attack**&**Robustness**                                       |
| 24.05 |                                                                             Johns Hopkins University                                                                              |           CCS 2024            |                                   [PLeak: Prompt Leaking Attacks against Large Language Model Applications](https://arxiv.org/abs/2405.06823)                                    |                                        **Prompt Leaking Attacks**&**Adversarial Queries**                                         |
| 24.05 |                                                                            IT University of Copenhagen                                                                            |             arxiv             |                                                [Hacc-Man: An Arcade Game for Jailbreaking LLMs](https://arxiv.org/abs/2405.15902)                                                |                                           **creative problem solving**&**jailbreaking**                                           |
| 24.05 |                                                                       The Hong Kong Polytechnic University                                                                        |             arxiv             |                                  [No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning Attacks](https://arxiv.org/abs/2405.16229)                                   |                           **Fine-tuning Attacks**&**LLM Safeguarding**&**Mechanistic Interpretability**                           |
| 24.05 |                                                                                       KAIST                                                                                       |             arxiv             |                                      [Automatic Jailbreaking of the Text-to-Image Generative AI Systems](https://arxiv.org/abs/2405.16567)                                       |                                       **Jailbreaking**&**Text-to-Image**&**Generative AI**                                        |
| 24.05 |                                                                                 Fudan University                                                                                  |             arxiv             |                                     [White-box Multimodal Jailbreaks Against Large Vision-Language Models](https://arxiv.org/abs/2405.17894)                                     |                             **Fine-tuning Attacks**&**Multimodal Models**&**Adversarial Robustness**                              |
| 24.05 |                                                                          Singapore Management University                                                                          |             arxiv             |                             [Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing](https://arxiv.org/abs/2405.18166)                             |                               **Jailbreak Attacks**&**Layer-specific Editing**&**LLM Safeguarding**                               |
| 24.05 |                                                                            Mila ‚Äì Qu√©bec AI Institute                                                                             |             arxiv             |                          [Learning Diverse Attacks on Large Language Models for Robust Red-Teaming and Safety Tuning](https://arxiv.org/abs/2405.18540)                          |                                    **Red-Teaming**&**Safety Tuning**&**GFlowNet Fine-tuning**                                     |
| 24.05 |                                                                  CISPA Helmholtz Center for Information Security                                                                  |             arxiv             |                                                    [Voice Jailbreak Attacks Against GPT-4o](https://arxiv.org/abs/2405.19103)                                                    |                                          **Jailbreak Attacks**&**Voice Mode**&**GPT-4o**                                          |
| 24.05 |                                                                         Nanyang Technological University                                                                          |             arxiv             |                                 [ART: Automatic Red-teaming for Text-to-Image Models to Protect Benign Users](https://arxiv.org/abs/2405.19360)                                  |                                 **Red-Teaming**&**Text-to-Image Models**&**Generative AI Safety**                                 |
| 24.05 |                                                                                 Xidian University                                                                                 |             arxiv             |                                          [Efficient LLM-Jailbreaking by Introducing Visual Modality](https://arxiv.org/abs/2405.20015)                                           |                                    **Jailbreaking**&**Multimodal Models**&**Visual Modality**                                     |
| 24.05 |                                                         Institute of Information Engineering, Chinese Academy of Sciences                                                         |             arxiv             |                                              [Context Injection Attacks on Large Language Models](https://arxiv.org/abs/2405.20234)                                              |                                       **Context Injection Attacks**&**Misleading Context**                                        |
| 24.05 |                                                                    University of Illinois at Urbana-Champaign                                                                     |             arxiv             |                            [Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters](https://arxiv.org/abs/2405.20413)                            |                                   **Jailbreak**&**Moderation Guardrails**&**Cipher Characters**                                   |
| 24.05 |                                                                              Northeastern University                                                                              |             arxiv             |                                 [Phantom: General Trigger Attacks on Retrieval Augmented Language Generation](https://arxiv.org/abs/2405.20485)                                  |                               **Trigger Attacks**&**Retrieval Augmented Generation**&**Poisoning**                                |
| 24.05 |                                                                              Northwestern University                                                                              |             arxiv             |                                [Enhancing Jailbreak Attack Against Large Language Models through Silent Tokens](https://arxiv.org/abs/2405.20653)                                |                                              **Jailbreak Attack**&**Silent Tokens**                                               |
| 24.05 |                                                                                 Peking University                                                                                 |             arxiv             |               [Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character](https://arxiv.org/abs/2405.20773)               |                            **Jailbreak Attack**&**MultiModal Large Language Models**&**Role-playing**                             |
| 24.05 |                                                                              Northwestern University                                                                              |             arxiv             |                                [Exploring Backdoor Attacks against Large Language Model-based Decision Making](https://arxiv.org/abs/2405.20774)                                 |                                             **Backdoor Attacks**&**Decision Making**                                              |
| 24.05 |                                                                                Beihang University                                                                                 |             arxiv             |                         [Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models](https://arxiv.org/abs/2405.20775)                          |                              **Jailbreak**&**Multimodal Large Language Models**&**Medical Contexts**                              |
| 24.05 |                                                                          Harbin Institute of Technology                                                                           |             arxiv             |                                   [Improved Generation of Adversarial Examples Against Safety-aligned LLMs](https://arxiv.org/abs/2405.20778)                                    |                            **Adversarial Examples**&**Safety-aligned LLMs**&**Gradient-based Methods**                            |
| 24.05 |                                                                         Nanyang Technological University                                                                          |             arxiv             |                               [Improved Techniques for Optimization-Based Jailbreaking on Large Language Models](https://arxiv.org/abs/2405.21018)                               |                                           **Jailbreaking**&**Optimization Techniques**                                            |
| 24.06 |                                                                           University of Central Florida                                                                           |             arxiv             |                        [BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models](https://arxiv.org/abs/2406.00083)                        |                                     **Retrieval-Augmented Generation**&**Poisoning Attacks**                                      |
| 24.06 |                                                                                   Zscaler, Inc.                                                                                   |             arxiv             |                                 [Exploring Vulnerabilities and Protections in Large Language Models: A Survey](https://arxiv.org/abs/2406.00240)                                 |                                       **Prompt Hacking**&**Adversarial Attacks**&**Suvery**                                       |
| 24.06 |                                                                          Singapore Management University                                                                          |             arxiv             |                           [Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses](https://arxiv.org/abs/2406.01288)                           |                           **Few-Shot Jailbreaking**&**Aligned Language Models**&**Adversarial Attacks**                           |
| 24.06 |                                                                              Capgemini Invent, Paris                                                                              |             arxiv             |                                         [QROA: A Black-Box Query-Response Optimization Attack on LLMs](https://arxiv.org/abs/2406.02044)                                         |                                       **Query-Response Optimization Attack**&**Black-Box**                                        |
| 24.06 |                                                                   Huazhong University of Science and Technology                                                                   |             arxiv             |                              [AutoJailbreak: Exploring Jailbreak Attacks and Defenses through a Dependency Lens](https://arxiv.org/abs/2406.03805)                               |                                           **Jailbreak Attacks**&**Dependency Analysis**                                           |
| 24.06 |                                                                                Beihang University                                                                                 |             arxiv             |                                       [Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt](https://arxiv.org/abs/2406.04031)                                       |                         **Jailbreak Attacks**&**Vision Language Models**&**Bi-Modal Adversarial Prompt**                          |
| 24.06 |                                                                               Zhengzhou University                                                                                |           ACL 2024            |                                      [BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents](https://arxiv.org/abs/2406.03007)                                       |                                      **Backdoor Attacks**&**LLM Agents**&**Data Poisoning**                                       |
| 24.06 |                                                                           Ludwig-Maximilians-University                                                                           |             arxiv             |                          [Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models](https://arxiv.org/abs/2406.09289)                          |                                          **Jailbreak Success**&**Latent Space Dynamics**                                          |
| 24.06 |                                                                                   Alibaba Group                                                                                   |             arxiv             |                           [How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States](https://arxiv.org/abs/2406.05644)                            |                                            **LLM Safety**&**Alignment**&**Jailbreak**                                             |
| 24.06 |                                                                                Beihang University                                                                                 |             arxiv             |                                  [Unveiling the Safety of GPT-4O: An Empirical Study Using Jailbreak Attacks](https://arxiv.org/abs/2406.06302)                                  |                                      **GPT-4O**&**Jailbreak Attacks**&**Safety Evaluation**                                       |
| 24.06 |                                                                         Nanyang Technological University                                                                          |             arxiv             |                    [A Survey of Backdoor Attacks and Defenses on Large Language Models: Implications for Security Measures](https://arxiv.org/abs/2406.06852)                    |                                           **Backdoor Attacks**&**Defenses**&**Survey**                                            |
| 24.06 |                                                                                   Anomalee Inc.                                                                                   |             arxiv             |                                                    [On Trojans in Refined Language Models](https://arxiv.org/abs/2406.07778)                                                     |                                    **Trojans**&**Refined Language Models**&**Data Poisoning**                                     |
| 24.06 |                                                                                 Purdue University                                                                                 |             arxiv             |                                 [When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-Guided Search](https://arxiv.org/abs/2406.08705)                                  |                                         **Jailbreaking**&**Deep Reinforcement Learning**                                          |
| 24.06 |                                                                                 Xidian University                                                                                 |             arxiv             |              [StructuralSleight: Automated Jailbreak Attacks on Large Language Models Utilizing Uncommon Text-Encoded Structure](https://arxiv.org/abs/2406.08754)               |                                            **Jailbreak Attacks**&**StructuralSleight**                                            |
| 24.06 |                                                                                Tsinghua University                                                                                |             arxiv             |                     [JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models](https://arxiv.org/abs/2406.09321)                     |                                           **Jailbreak Attempts**&**Evaluation Toolkit**                                           |
| 24.06 |                                                          The Hong Kong University of Science and Technology (Guangzhou)                                                           |             arxiv             |                                           [Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs](https://arxiv.org/abs/2406.09324)                                           |                                              **Jailbreak Attacks**&**Benchmarking**                                               |
| 24.06 |                                                                           Pennsylvania State University                                                                           |          NAACL 2024           |                                      [PromptFix: Few-shot Backdoor Removal via Adversarial Prompt Tuning](https://arxiv.org/abs/2406.04478)                                      |                             **Backdoor Removal**&**Adversarial Prompt Tuning**&**Few-shot Learning**                              |
| 24.06 |                                                     Shanghai Jiao Tong University, Peking University, Shanghai AI Laboratory                                                      |             arxiv             |                         [Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models](https://arxiv.org/abs/2406.10630)                          |                                  **Federated Instruction Tuning**&**Safety Attack**&**Defense**                                   |
| 24.06 |                                                                             Michigan State University                                                                             |             arxiv             |                               [Towards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis](https://arxiv.org/abs/2406.10794)                               |                                      **Jailbreak Attacks**&**Representation Space Analysis**                                      |
| 24.06 |                                                                            Chinese Academy of Sciences                                                                            |             arxiv             |                   [‚ÄúNot Aligned‚Äù is Not ‚ÄúMalicious‚Äù: Being Careful about Hallucinations of Large Language Models‚Äô Jailbreak](https://arxiv.org/abs/2406.11668)                   |                                             **Jailbreak**&**Hallucinations**&**LLMs**                                             |
| 24.06 |                                                                                Tsinghua University                                                                                |             arxiv             |                                         [Knowledge-to-Jailbreak: One Knowledge Point Worth One Attack](https://arxiv.org/abs/2406.11682)                                         |                            **Knowledge-to-Jailbreak**&**Jailbreak Attacks**&**Domain-Specific Safety**                            |
| 24.06 |                                                                              University of Maryland                                                                               |             arxiv             |                                  [Is Poisoning a Real Threat to LLM Alignment? Maybe More So Than You Think](https://arxiv.org/abs/2406.12091)                                   |                **Poisoning Attacks**&**Direct Policy Optimization**&**Reinforcement Learning with Human Feedback**                |
| 24.06 |                                                                            Carnegie Mellon University                                                                             |             arxiv             |                                                [Jailbreak Paradox: The Achilles‚Äô Heel of LLMs](https://arxiv.org/abs/2406.12702)                                                 |                                                **Jailbreak Paradox**&**Security**                                                 |
| 24.06 |                                                                            Carnegie Mellon University                                                                             |             arxiv             |                                                   [Adversarial Attacks on Multimodal Agents](https://arxiv.org/abs/2406.12814)                                                   |                             **Adversarial Attacks**&**Multimodal Agents**&**Vision-Language Models**                              |
| 24.06 |                                                                 University of Washington, Allen Institute for AI                                                                  |             arxiv             |                                  [ChatBug: A Common Vulnerability of Aligned LLMs Induced by Chat Templates](https://arxiv.org/abs/2406.12935)                                   |                              **LLM Vulnerabilities**&**Jailbreak Attacks**&**Adversarial Training**                               |
| 24.06 |                                  University of Notre Dame, Huazhong University of Science and Technology, Tsinghua University, Lehigh University                                  |             arxiv             |                                     [ObscurePrompt: Jailbreaking Large Language Models via Obscure Input](https://arxiv.org/abs/2406.13662)                                      |                               **Jailbreaking**&**Adversarial Attacks**&**Out-of-Distribution Data**                               |
| 24.06 |                                                                The University of Hong Kong, Huawei Noah‚Äôs Ark Lab                                                                 |             arxiv             |                                              [Jailbreaking as a Reward Misspecification Problem](https://arxiv.org/abs/2406.14393)                                               |                               **Jailbreaking**&**Reward Misspecification**&**Adversarial Attacks**                                |
| 24.06 |                                                                                    UC Berkeley                                                                                    |             arxiv             |                                              [Adversaries Can Misuse Combinations of Safe Models](https://arxiv.org/abs/2406.14595)                                              |                                       **Model Misuse**&**AI Safety**&**Task Decomposition**                                       |
| 24.06 |                                                                                 UC Santa Barbara                                                                                  |             arxiv             |                  [MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations](https://arxiv.org/abs/2406.14711)                   |                                       **MultiAgent Collaboration**&**Adversarial Attacks**                                        |
| 24.06 |                                                                         University of Southern California                                                                         |             arxiv             |                                    [From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking](https://arxiv.org/abs/2406.14859)                                    |                                        **Multimodal Jailbreaking**&**MLLMs**&**Security**                                         |
| 24.06 |                                                                                       KAIST                                                                                       |             arxiv             |                                [CSRT: Evaluation and Analysis of LLMs using Code-Switching Red-Teaming Dataset](https://arxiv.org/abs/2406.15481)                                |                                      **Code-Switching**&**Red-Teaming**&**Multilingualism**                                       |
| 24.06 |                                                                          China University of Geosciences                                                                          |             arxiv             |                                [Large Language Models for Link Stealing Attacks Against Graph Neural Networks](https://arxiv.org/abs/2406.16963)                                 |                              **Link Stealing Attacks**&**Graph Neural Networks**&**Privacy Attacks**                              |
| 24.06 |                                                                       The Hong Kong Polytechnic University                                                                        |             arxiv             |                              [CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference](https://arxiv.org/abs/2406.17626)                               |                                   **Safety Evaluation**&**Dialogue Coreference**&**LLM Safety**                                   |
| 24.06 |                                                                              Imperial College London                                                                              |             arxiv             |                                [Inherent Challenges of Post-Hoc Membership Inference for Large Language Models](https://arxiv.org/abs/2406.17975)                                |                          **Membership Inference Attacks**&**Post-Hoc Evaluation**&**Distribution Shift**                          |
| 24.06 |                                                                                 Hubei University                                                                                  |             arxiv             |                                               [Poisoned LangChain: Jailbreak LLMs by LangChain](https://arxiv.org/abs/2406.18122)                                                |                                  **Jailbreak**&**Retrieval-Augmented Generation**&**LangChain**                                   |
| 24.06 |                                                                           University of Central Florida                                                                           |             arxiv             |                                          [Jailbreaking LLMs with Arabic Transliteration and Arabizi](https://arxiv.org/abs/2406.18725)                                           |                                      **Jailbreaking**&**Arabic Transliteration**&**Arabizi**                                      |
| 24.06 |                                                                                 Hubei University                                                                                  |      TRAC 2024 Workshop       |                      [SEEING IS BELIEVING: BLACK-BOX MEMBERSHIP INFERENCE ATTACKS AGAINST RETRIEVAL AUGMENTED GENERATION](https://arxiv.org/abs/2406.19234)                      |                                **Membership Inference Attacks**&**Retrieval-Augmented Generation**                                |
| 24.06 |                                                                   Huazhong University of Science and Technology                                                                   |             arxiv             |                                  [Virtual Context: Enhancing Jailbreak Attacks with Special Token Injection](https://arxiv.org/abs/2406.19845)                                   |                                             **Jailbreak Attacks**&**Special Tokens**                                              |
| 24.06 |                                                                                    UC Berkeley                                                                                    |             arxiv             |                                    [Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation](https://arxiv.org/abs/2406.20053)                                    |                                                    **AI Safety**&**Backdoors**                                                    |
| 24.07 |                                                                          University of Illinois Chicago                                                                           |             arxiv             |                    [Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks](https://arxiv.org/abs/2407.00869)                     |                                          **Jailbreak Attacks**&**Fallacious Reasoning**                                           |
| 24.07 |                                                                                 Palisade Research                                                                                 |             arxiv             |                                        [Badllama 3: Removing Safety Finetuning from Llama 3 in Minutes](https://arxiv.org/abs/2407.01376)                                        |                                            **Safety Finetuning**&**Jailbreak Attacks**                                            |
| 24.07 |                                                                      University of Illinois Urbana-Champaign                                                                      |             arxiv             |                   [JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large Language and Vision-Language Models](https://arxiv.org/abs/2407.01599)                   |                                            **Jailbreaking**&**Vision-Language Models**                                            |
| 24.07 |                                                                   Shanghai University of Finance and Economics                                                                    |             arxiv             |                                 [SoP: Unlock the Power of Social Facilitation for Automatic Jailbreak Attack](https://arxiv.org/abs/2407.01902)                                  |                              **Jailbreak Attacks**&**Large Language Models**&**Social Facilitation**                              |
| 24.07 |                                                                               University of Exeter                                                                                |             arxiv             |                                   [Image-to-Text Logic Jailbreak: Your Imagination can Help You Do Anything](https://arxiv.org/abs/2407.02534)                                   |                                        **Machine Learning**&**ICML**&**Jailbreak Attacks**                                        |
| 24.07 |                                                                  Hong Kong University of Science and Technology                                                                   |             arxiv             |       [JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational Datasets](https://arxiv.org/abs/2407.03045)        |                                            **Visual Analytics**&**Jailbreak Prompts**                                             |
| 24.07 |                                                                  CISPA Helmholtz Center for Information Security                                                                  |             arxiv             |                                      [SOS! Soft Prompt Attack Against Open-Source Large Language Models](https://arxiv.org/abs/2407.03160)                                       |                                           **Soft Prompt Attack**&**Open-Source Models**                                           |
| 24.07 |                                                                         National University of Singapore                                                                          |             arxiv             |                                              [Single Character Perturbations Break LLM Alignment](https://arxiv.org/abs/2407.03232)                                              |                                             **Jailbreak Attacks**&**Model Alignment**                                             |
| 24.07 |                                                              Deutsches Forschungszentrum f√ºr K√ºnstliche Intelligenz                                                               |             arxiv             |            [Soft Begging: Modular and Efficient Shielding of LLMs against Prompt Injection and Jailbreaking based on Prompt Tuning](https://arxiv.org/abs/2407.03391)            |                                      **Prompt Injection**&**Jailbreaking**&**Soft Prompts**                                       |
| 24.07 |                                                                                     UC Davis                                                                                      |             arxiv             |                           [Securing Multi-turn Conversational Language Models Against Distributed Backdoor Triggers](https://arxiv.org/abs/2407.04151)                           |                                **Multi-turn Conversation**&**Backdoor Triggers**&**LLM Security**                                 |
| 24.07 |                                                                                Tsinghua University                                                                                |             arxiv             |                                    [Jailbreak Attacks and Defenses Against Large Language Models: A Survey](https://arxiv.org/abs/2407.04295)                                    |                                                **Jailbreak Attacks**&**Defenses**                                                 |
| 24.07 |                                                                                Zhejiang University                                                                                |             arxiv             |                               [TAPI: Towards Target-Specific and Adversarial Prompt Injection against Code LLMs](https://arxiv.org/abs/2407.09164)                               |                    **Target-Specific Attacks**&**Adversarial Prompt Injection**&**Malicious Code Generation**                     |
| 24.07 |                                                                              Northwestern University                                                                              |             arxiv             |                        [CEIPA: Counterfactual Explainable Incremental Prompt Attack Analysis on Large Language Models](https://arxiv.org/abs/2407.09292)                         |                    **Counterfactual Explanation**&**Prompt Attack Analysis**&**Incremental Prompt Injection**                     |
| 24.07 |                                                                                       EPFL                                                                                        |             arxiv             |                                         [Does Refusal Training in LLMs Generalize to the Past Tense?](https://arxiv.org/abs/2407.11969)                                          |                             **Refusal Training**&**Past Tense Reformulation**&**Adversarial Attacks**                             |
| 24.07 |                                                                               University of Chicago                                                                               |             arxiv             |                                 [AGENTPOISON: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases](https://arxiv.org/abs/2407.12784)                                  |                                           **Red-teaming**&**LLM Agents**&**Poisoning**                                            |
| 24.07 |                                                                                 Wuhan University                                                                                  |             arxiv             |                      [Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2407.13757)                       |                                      **Black-box Attacks**&**RAG**&**Opinion Manipulation**                                       |
| 24.07 |                                                                           University of New South Wales                                                                           |             arxiv             |                            [Continuous Embedding Attacks via Clipped Inputs in Jailbreaking Large Language Models](https://arxiv.org/abs/2407.13796)                             |                                             **Continuous Embedding**&**Jailbreaking**                                             |
| 24.07 |                                                                                     Bloomberg                                                                                     |             arxiv             |                                 [Operationalizing a Threat Model for Red-Teaming Large Language Models (LLMs)](https://arxiv.org/abs/2407.14937)                                 |                                                 **Threat Model**&**Red-Teaming**                                                  |
| 24.07 |                                                                                Stanford University                                                                                |             arxiv             |                                 [When Do Universal Image Jailbreaks Transfer Between Vision-Language Models?](https://arxiv.org/abs/2407.15211)                                  |                           **Universal Image Jailbreaks**&**Vision-Language Models**&**Transferability**                           |
| 24.07 |                                                                             Michigan State University                                                                             |             arxiv             |              [Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal Mechanisms and the Superficial Hypothesis](https://arxiv.org/abs/2407.15286)              |                                        **Moral Self-Correction**&**Intrinsic Mechanisms**                                         |
| 24.07 |                                                                                  Meetyou AI Lab                                                                                   |             arxiv             |                        [Imposter.AI: Adversarial Attacks with Hidden Intentions towards Aligned Large Language Models](https://arxiv.org/abs/2407.15399)                         |                                           **Adversarial Attacks**&**Hidden Intentions**                                           |
| 24.07 |                                                                           Zhejiang Gongshang University                                                                           |             arxiv             |                                   [Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models](https://arxiv.org/abs/2407.16205)                                   |                                        **Jailbreak Attack**&**Analyzing-based Jailbreak**                                         |
| 24.07 |                                                                                Zhejiang University                                                                                |             arxiv             |                           [RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent](https://arxiv.org/abs/2407.16667)                           |                                  **Red Teaming**&**Jailbreak Attacks**&**Context-aware Prompts**                                  |
| 24.07 |                                                                                   Confirm Labs                                                                                    |             arxiv             |                                                      [Fluent Student-Teacher Redteaming](https://arxiv.org/abs/2407.17447)                                                       |                                   **Fluent Student-Teacher Redteaming**&**Adversarial Attacks**                                   |
| 24.07 |                                                                           City University of Hong Kong                                                                            |          ACM MM 2024          |                    [Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts](https://arxiv.org/abs/2407.15050)                     |                               **Large Vision Language Model**&**Red Teaming**&**Jailbreak Attack**                                |
| 24.07 |                                                                   Huazhong University of Science and Technology                                                                   |      NAACL 2024 Workshop      |                                          [Can Large Language Models Automatically Jailbreak GPT-4V?](https://arxiv.org/abs/2407.16686)                                           |                                  **Jailbreak**&**Multimodal Information**&**Facial Recognition**                                  |
| 24.07 |                                                                         Illinois Institute of Technology                                                                          |             arxiv             |                                                        [Can Editing LLMs Inject Harm?](https://arxiv.org/abs/2407.20224)                                                         |                               **Knowledge Editing**&**Misinformation Injection**&**Bias Injection**                               |
| 24.07 |                                                                                       KAIST                                                                                       |             arxiv             |                                   [Prompt-Driven Contrastive Learning for Transferable Adversarial Attacks](https://arxiv.org/abs/2407.20657)                                    |                             **Adversarial Attack**&**Vision-Language Model**&**Contrastive Learning**                             |
| 24.07 |                                                                  CISPA Helmholtz Center for Information Security                                                                  |             arxiv             |                            [Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification](https://arxiv.org/abs/2407.20859)                             |                                 **LLM Agents**&**Security Vulnerability**&**Autonomous Systems**                                  |
| 24.08 |                                                                  CISPA Helmholtz Center for Information Security                                                                  |             arxiv             |                                                   [Vera Verto: Multimodal Hijacking Attack](https://arxiv.org/abs/2408.00129)                                                    |                                        **Multimodal Hijacking Attack**&**Model Hijacking**                                        |
| 24.08 |                                                                                Shandong University                                                                                |             arxiv             |                                           [Jailbreaking Text-to-Image Models with LLM-Based Agents](https://arxiv.org/abs/2408.00523)                                            |                         **Jailbreak Attacks**&**Vision-Language Models (VLMs)**&**Generative AI Safety**                          |
| 24.08 |                                                                          Technological University Dublin                                                                          |             arxiv             |                              [Pathway to Secure and Trustworthy 6G for LLMs: Attacks, Defense, and Opportunities](https://arxiv.org/abs/2408.00722)                              |                                   **6G Networks**&**Security**&**Membership Inference Attacks**                                   |
| 24.08 |                                                                                     Microsoft                                                                                     |             arxiv             |                                   [WHITE PAPER: A Brief Exploration of Data Exfiltration using GCG Suffixes](https://arxiv.org/abs/2408.00925)                                   |                      **Cross-Prompt Injection Attack**&**Greedy Coordinate Gradient**&**Data Exfiltration**                       |
| 24.08 |                                                                                NYU & Meta AI, FAIR                                                                                |             arxiv             |                                      [Mission Impossible: A Statistical Perspective on Jailbreaking LLMs](https://arxiv.org/abs/2408.01420)                                      |                                  **Jailbreaking**&**Reinforcement Learning with Human Feedback**                                  |
| 24.08 |                                                                                Beihang University                                                                                 |             arxiv             |                                        [Compromising Embodied Agents with Contextual Backdoor Attacks](https://arxiv.org/abs/2408.02882)                                         |                     **Embodied Agents**&**Contextual Backdoor Attacks**&**Adversarial In-Context Generation**                     |
| 24.08 |                                                                                      FAR AI                                                                                       |             arxiv             |                                                   [Scaling Laws for Data Poisoning in LLMs](https://arxiv.org/abs/2408.02946)                                                    |                                                **Data Poisoning**&**Scaling Laws**                                                |
| 24.08 |                                                                        The University of Western Australia                                                                        |             arxiv             |                               [A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic Systems](https://arxiv.org/abs/2408.03515)                               |                                               **Mobile Robot**&**Prompt Injection**                                               |
| 24.08 |                                                                                 Fudan University                                                                                  |             arxiv             |                                              [EnJa: Ensemble Jailbreak on Large Language Models](https://arxiv.org/abs/2408.03603)                                               |                                                   **Jailbreaking**&**Security**                                                   |
| 24.08 |                                                                                Bocconi University                                                                                 |             arxiv             |                           [Compromesso! Italian Many-Shot Jailbreaks Undermine the Safety of Large Language Models](https://arxiv.org/abs/2408.04522)                            |                                             **Jailbreaking**&**Multilingual Safety**                                              |
| 24.08 |                                                                                 Xidian University                                                                                 |             arxiv             |                              [Multi-Turn Context Jailbreak Attack on Large Language Models From First Principles](https://arxiv.org/abs/2408.04686)                              |                                   **Multi-Turn Jailbreak Attack**&**Contextual Fusion Attack**                                    |
| 24.08 |                                                                                   Cornell Tech                                                                                    |             arxiv             |                [A Jailbroken GenAI Model Can Cause Substantial Harm: GenAI-powered Applications are Vulnerable to PromptWares](https://arxiv.org/abs/2408.05061)                 |                            **Jailbroken GenAI Models**&**PromptWares**&**GenAI-powered Applications**                             |
| 24.08 |                                                                          University of California Irvine                                                                          |             arxiv             |                                 [Using Retriever Augmented Large Language Models for Attack Graph Generation](https://arxiv.org/abs/2408.05855)                                  |                              **Retriever Augmented Generation**&**Attack Graphs**&**Cybersecurity**                               |
| 24.08 |                                                                       University of California, Los Angeles                                                                       |           CCS 2024            |                                              [BadMerging: Backdoor Attacks Against Model Merging](https://arxiv.org/abs/2408.07362)                                              |                                       **Backdoor Attack**&**Model Merging**&**AI Security**                                       |
| 24.08 |                                                                                Stanford University                                                                                |             arxiv             |                   [Kov: Transferable and Naturalistic Black-Box LLM Attacks using Markov Decision Processes and Tree Search](https://arxiv.org/abs/2408.08899)                   |                          **Black-Box Attacks**&**Markov Decision Processes**&**Monte Carlo Tree Search**                          |
| 24.08 |                                                                           Shanghai Jiao Tong University                                                                           |             arxiv             |                                [Transferring Backdoors between Large Language Models by Knowledge Distillation](https://arxiv.org/abs/2408.09878)                                |                                          **Backdoor Attacks**&**Knowledge Distillation**                                          |
| 24.08 |                                                                                Tsinghua University                                                                                |             arxiv             |                      [Probing the Safety Response Boundary of Large Language Models via Unsafe Decoding Path Generation](https://arxiv.org/abs/2408.10668)                       |                                       **Safety Response Boundary**&**Unsafe Decoding Path**                                       |
| 24.08 |                                                                   Singapore University of Technology and Design                                                                   |             arxiv             |                            [FERRET: Faster and Effective Automated Red Teaming with Reward-Based Scoring Technique](https://arxiv.org/abs/2408.10701)                            |                            **Automated Red Teaming**&**Adversarial Prompts**&**Reward-Based Scoring**                             |
| 24.08 |                                                                           Shanghai Jiao Tong University                                                                           |             arxiv             |                                    [MEGen: Generative Backdoor in Large Language Models via Model Editing](https://arxiv.org/abs/2408.10722)                                     |                                              **Backdoor Attacks**&**Model Editing**                                               |
| 24.08 |                                                                            Chinese Academy of Sciences                                                                            |             arxiv             |         [DiffZOO: A Purely Query-Based Black-Box Attack for Red-Teaming Text-to-Image Generative Model via Zeroth Order Optimization](https://arxiv.org/abs/2408.11071)          |                       **Black-Box Attack**&**Text-to-Image Generative Model**&**Zeroth Order Optimization**                       |
| 24.08 |                                                                         The Pennsylvania State University                                                                         |             arxiv             |               [Hide Your Malicious Goal Into Benign Narratives: Jailbreak Large Language Models through Neural Carrier Articles](https://arxiv.org/abs/2408.11182)               |                                            **Jailbreak Attacks**&**Prompt Injection**                                             |
| 24.08 |                                                                             Xi'an Jiaotong University                                                                             |             arxiv             |         [Unlocking Adversarial Suffix Optimization Without Affirmative Phrases: Efficient Black-box Jailbreaking via LLM as Optimizer](https://arxiv.org/abs/2408.11313)         |                                          **Jailbreak Attacks**&**Adversarial Suffixes**                                           |
| 24.08 |                                                             Nanjing University of Information Science and Technology                                                              |             arxiv             |                          [Large Language Models are Good Attackers: Efficient and Stealthy Textual Backdoor Attacks](https://arxiv.org/abs/2408.11587)                           |                                         **Textual Backdoor Attacks**&**Sample Selection**                                         |
| 24.08 |                                                                                 Nankai University                                                                                 |             arxiv             |                                        [RT-Attack: Jailbreaking Text-to-Image Models via Random Token](https://arxiv.org/abs/2408.13896)                                         |                                      **Jailbreak**&**Text-to-Image**&**Adversarial Attacks**                                      |
| 24.08 |                                                                     Harbin Institute of Technology, Shenzhen                                                                      |             arxiv             |                                [TF-Attack: Transferable and Fast Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2408.13985)                                 |                                     **Adversarial Attack**&**Transferability**&**Efficiency**                                     |
| 24.08 |                                                                      Shenzhen Research Institute of Big Data                                                                      |             arxiv             |                               [Detecting AI Flaws: Target-Driven Attacks on Internal Faults in Language Models](https://arxiv.org/abs/2408.14853)                                |                             **Target-Driven Attacks**&**Internal Faults**&**Reinforcement Learning**                              |
| 24.08 |                                                                         National University of Singapore                                                                          |             arxiv             |                               [Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models](https://arxiv.org/abs/2408.14866)                                |                                   **Adversarial Suffixes**&**Transfer Learning**&**Jailbreak**                                    |
| 24.08 |                                                                                     Scale AI                                                                                      |             arxiv             |                                        [LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet](https://arxiv.org/abs/2408.15221)                                        |                                  **Multi-Turn Jailbreaks**&**LLM Defense**&**Human Red Teaming**                                  |
| 24.09 |                                                                        University of California, Berkeley                                                                         |             arxiv             |                                  [Emerging Vulnerabilities in Frontier Models: Multi-Turn Jailbreak Attacks](https://arxiv.org/abs/2409.00137)                                   |                                   **Multi-Turn Jailbreak**&**Frontier Models**&**LLM Security**                                   |
| 24.09 |                                                                         University of Southern California                                                                         |             arxiv             |                                         [Rethinking Backdoor Detection Evaluation for Language Models](https://arxiv.org/abs/2409.00399)                                         |                               **Backdoor Attacks**&**Detection Robustness**&**Training Intensity**                                |
| 24.09 |                                                                             Michigan State University                                                                             |             arxiv             |                               [The Dark Side of Human Feedback: Poisoning Large Language Models via User Inputs](https://arxiv.org/abs/2409.00787)                               |                                   **User-Guided Poisoning**&**RLHF**&**Toxicity Manipulation**                                    |
| 24.09 |                                                                              University of Cambridge                                                                              |             arxiv             |                                    [Conversational Complexity for Assessing Risk in Large Language Models](https://arxiv.org/abs/2409.01247)                                     |                                         **Conversational Complexity**&**Risk Assessment**                                         |
| 24.09 |                                                                                    Independent                                                                                    |             arxiv             |                                    [Well, that escalated quickly: The Single-Turn Crescendo Attack (STCA)](https://arxiv.org/abs/2409.03131)                                     |                                     **Single-Turn Crescendo Attack**&**Adversarial Attacks**                                      |
| 24.09 |                                                                  CISPA Helmholtz Center for Information Security                                                                  |           CCS 2024            |                                           [Membership Inference Attacks Against In-Context Learning](https://arxiv.org/abs/2409.01380)                                           |                                     **Membership Inference Attacks**&**In-Context Learning**                                      |
| 24.09 |                                                                    Radboud University, Ikerlan Research Centre                                                                    |             arxiv             |                            [Context is the Key: Backdoor Attacks for In-Context Learning with Vision Transformers](https://arxiv.org/abs/2409.04142)                             |                               **Backdoor Attacks**&**In-Context Learning**&**Vision Transformers**                                |
| 24.09 |                                                         Institute of Information Engineering, Chinese Academy of Sciences                                                         |             arxiv             |                                 [AdaPPA: Adaptive Position Pre-Fill Jailbreak Attack Approach Targeting LLMs](https://arxiv.org/abs/2409.07503)                                  |                                       **Jailbreak Attacks**&**Adaptive Position Pre-Fill**                                        |
| 24.09 |                                                          Technion - Israel Institute of Technology, Intuit, Cornell Tech                                                          |             arxiv             | [Unleashing Worms and Extracting Data: Escalating the Outcome of Attacks against RAG-based Inference in Scale and Severity Using Jailbreaking](https://arxiv.org/abs/2409.08045) |                                      **Jailbreaking**&**RAG Inference**&**Data Extraction**                                       |
| 24.09 |                                                                        University of Texas at San Antonio                                                                         |             arxiv             |                                         [Jailbreaking Large Language Models with Symbolic Mathematics](https://arxiv.org/abs/2409.11445)                                         |                                             **Jailbreaking**&**Symbolic Mathematics**                                             |
| 24.09 |                                                                                Beihang University                                                                                 |             arxiv             |                  [PathSeeker: Exploring LLM Security Vulnerabilities with a Reinforcement Learning-Based Jailbreak Approach](https://arxiv.org/abs/2409.14177)                   |                         **LLM Security Vulnerabilities**&**Jailbreak Attack**&**Reinforcement Learning**                          |
| 24.09 |                                                                                      AWS AI                                                                                       |             arxiv             |                                           [Order of Magnitude Speedups for LLM Membership Inference](https://arxiv.org/abs/2409.14513)                                           |                                         **Membership Inference**&**Quantile Regression**                                          |
| 24.09 |                                                                         Nanyang Technological University                                                                          |             arxiv             |                                 [Effective and Evasive Fuzz Testing-Driven Jailbreaking Attacks against LLMs](https://arxiv.org/abs/2409.14866)                                  |                                    **Jailbreaking Attacks**&**Fuzz Testing**&**LLM Security**                                     |
| 24.09 |                                                                                  Hippocratic AI                                                                                   |             arxiv             |                           [RED QUEEN: Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking](https://arxiv.org/abs/2409.17458)                            |                                   **Jailbreaking**&**Multi-Turn Attacks**&**Concealed Attacks**                                   |
| 24.09 |                                                                         Nanyang Technological University                                                                          |             arxiv             |                               [Weak-to-Strong Backdoor Attacks for LLMs with Contrastive Knowledge Distillation](https://arxiv.org/abs/2409.17946)                               |                  **Backdoor Attacks**&**Contrastive Knowledge Distillation**&**Parameter-Efficient Fine-Tuning**                  |
| 24.09 |                                                                          Georgia Institute of Technology                                                                          |             arxiv             |                                 [Harmful Fine-tuning Attacks and Defenses for Large Language Models: A Survey](https://arxiv.org/abs/2409.18169)                                 |                                     **Harmful Fine-tuning**&**LLM Attacks**&**LLM Defenses**                                      |
| 24.09 |                                                                          Institut Polytechnique de Paris                                                                          |             arxiv             |                     [Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with ASCII Art to Mask Profanity](https://arxiv.org/abs/2409.18708)                      |                                       **ASCII Art**&**LLM Attacks**&**Toxicity Detection**                                        |
| 24.09 |                                                                                    LMU Munich                                                                                     |             arxiv             |                                            [Multimodal Pragmatic Jailbreak on Text-to-image Models](https://arxiv.org/abs/2409.19149)                                            |                          **Multimodal Pragmatic Jailbreak**&**Text-to-image Models**&**Safety Filters**                           |
| 24.10 |                                                                              Stony Brook University                                                                               |             arxiv             |                                 [BUCKLE UP: ROBUSTIFYING LLMS AT EVERY CUSTOMIZATION STAGE VIA DATA CURATION](https://arxiv.org/abs/2410.02220)                                  |                                     **Jailbreaking**&**LLM Customization**&**Data Curation**                                      |
| 24.10 |                                                                         National University of Singapore                                                                          |             arxiv             |                                                   [FLIPATTACK: Jailbreak LLMs via Flipping](https://arxiv.org/abs/2410.02832)                                                    |                                               **Jailbreak**&**Adversarial Attacks**                                               |
| 24.10 |                                                                      University of Wisconsin‚ÄìMadison, NVIDIA                                                                      |             arxiv             |                               [AUTODAN-TURBO: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs](https://arxiv.org/abs/2410.05295)                                |                                            **Jailbreak**&**Strategy Self-Exploration**                                            |
| 24.10 |                                                                  University College London, Stanford University                                                                   |             arxiv             |                                   [Prompt Infection: LLM-to-LLM Prompt Injection within Multi-Agent Systems](https://arxiv.org/abs/2410.07283)                                   |                                           **Prompt Infection**&**Multi-Agent Systems**                                            |
| 24.10 |                                                                          University of Wisconsin‚ÄìMadison                                                                          |             arxiv             |                           [RePD: Defending Jailbreak Attack through a Retrieval-based Prompt Decomposition Process](https://arxiv.org/abs/2410.08660)                            |                                  **Jailbreak attack**&**Prompt decomposition**&**LLMs defense**                                   |
| 24.10 |                                                UC Santa Cruz, Johns Hopkins University, University of Edinburgh, Peking University                                                |             arxiv             |                                 [AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation](https://arxiv.org/abs/2410.09040)                                  |                              **Jailbreaking**&**LLMs vulnerability**&**Optimization-based attacks**                               |
| 24.10 |                                                                              Independent Researcher                                                                               |             arxiv             |                           [Recent advancements in LLM Red-Teaming: Techniques, Defenses, and Ethical Considerations](https://arxiv.org/abs/2410.09097)                           |                               **LLM red-teaming**&**Jailbreaking defenses**&**Prompt engineering**                                |
| 24.10 |                                                            Beihang University, Tsinghua University, Peking University                                                             |             arxiv             |              [BLACKDAN: A Black-Box Multi-Objective Approach for Effective and Contextual Jailbreaking of Large Language Models](https://arxiv.org/abs/2410.09804)               |                                **Jailbreak**&**Multi-objective optimization**&**Black-box attack**                                |
| 24.10 |                                          Shanghai Jiao Tong University, Shanghai Artificial Intelligence Laboratory, Beihang University                                           |             arxiv             |                                [Derail Yourself: Multi-Turn LLM Jailbreak Attack Through Self-Discovered Clues](https://arxiv.org/abs/2410.10700)                                |                                  **Multi-turn attacks**&**Jailbreak**&**Self-discovered clues**                                   |
| 24.10 |                                                              Tsinghua University, Sea AI Lab, Peng Cheng Laboratory                                                               |             arxiv             |                                         [Denial-of-Service Poisoning Attacks on Large Language Models](https://arxiv.org/abs/2410.10760)                                         |                                            **Denial-of-Service**&**Poisoning attack**                                             |
| 24.10 |                                                                   University of New Haven, Robust Intelligence                                                                    |             arxiv             |                                         [COGNITIVE OVERLOAD ATTACK: PROMPT INJECTION FOR LONG CONTEXT](https://arxiv.org/abs/2410.11272)                                         |                                     **Cognitive overload**&**Prompt injection**&**Jailbreak**                                     |
| 24.10 |                                              Harbin Institute of Technology, Tencent, University of Glasgow, Independent Researcher                                               |             arxiv             |                            [DECIPHERING THE CHAOS: ENHANCING JAILBREAK ATTACKS VIA ADVERSARIAL PROMPT TRANSLATION](https://arxiv.org/abs/2410.11317)                             |                           **Jailbreak attacks**&**Adversarial prompt**&**Gradient-based optimization**                            |
| 24.10 |                                                                                 Monash University                                                                                 |             arxiv             |                                [JIGSAW PUZZLES: Splitting Harmful Questions to Jailbreak Large Language Models](https://arxiv.org/abs/2410.11459)                                |                                      **Jailbreak**&**Multi-turn attack**&**Query splitting**                                      |
| 24.10 |                                                                                 Wuhan University                                                                                  |             arxiv             |                                            [Multi-Round Jailbreak Attack on Large Language Models](https://arxiv.org/abs/2410.11533)                                             |                                               **Jailbreak**&**Multi-round attack**                                                |
| 24.10 |                                       The Hong Kong University of Science and Technology (Guangzhou), University of Birmingham, Baidu Inc.                                        |             arxiv             |               [JAILJUDGE: A Comprehensive Jailbreak Judge Benchmark with Multi-Agent Enhanced Explanation Evaluation Framework](https://arxiv.org/abs/2410.12855)                |                                           **Jailbreak judge**&**Multi-agent framework**                                           |
| 24.10 |                                                                                    Theori Inc.                                                                                    |           ICLR 2025           |                   [DO LLMS HAVE POLITICAL CORRECTNESS? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems](https://arxiv.org/abs/2410.13334)                   |                                **Political correctness**&**Jailbreak**&**Ethical vulnerabilities**                                |
| 24.10 |                                                                              University of Manitoba                                                                               |             arxiv             |                                                 [SoK: Prompt Hacking of Large Language Models](https://arxiv.org/abs/2410.13901)                                                 |                                             **Prompt Hacking**&**Jailbreak Attacks**                                              |
| 24.10 |                                                                                    Thales DIS                                                                                     |             arxiv             |                [Backdoored Retrievers for Prompt Injection Attacks on Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2410.14479)                 |                           **Retrieval-Augmented Generation**&**Prompt Injection**&**Backdoor Attacks**                            |
| 24.10 |                                                                                  Duke University                                                                                  |             arxiv             |                                      [Making LLMs Vulnerable to Prompt Injection via Poisoning Alignment](https://arxiv.org/abs/2410.14827)                                      |                               **Prompt Injection**&**Poisoning Alignment**&**LLM Vulnerabilities**                                |
| 24.10 |                                                                                Tsinghua University                                                                                |             arxiv             |                     [Faster-GCG: Efficient Discrete Optimization Jailbreak Attacks against Aligned Large Language Models](https://arxiv.org/abs/2410.15362)                      |                              **Jailbreak Attacks**&**Discrete Optimization**&**Adversarial Attacks**                              |
| 24.10 |                                                                       International Digital Economy Academy                                                                       |             arxiv             |                              [SMILES-Prompting: A Novel Approach to LLM Jailbreak Attacks in Chemical Synthesis](https://arxiv.org/abs/2410.15641)                               |                                  **SMILES-Prompting**&**Jailbreak Attacks**&**Chemical Safety**                                   |
| 24.10 |                                                                Beijing University of Posts and Telecommunications                                                                 |             arxiv             |                              [FEINT AND ATTACK: Attention-Based Strategies for Jailbreaking and Protecting LLMs](https://arxiv.org/abs/2410.16327)                               |                               **Attention Mechanisms**&**Jailbreak Attacks**&**Defense Strategies**                               |
| 24.10 |                                                                                  IBM Research AI                                                                                  |             arxiv             |                                        [Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In](https://arxiv.org/abs/2410.16950)                                        |                                 **ReAct Agents**&**Prompt Injection**&**Foot-in-the-Door Attack**                                 |
| 24.10 |                                                                                  Google DeepMind                                                                                  |             arxiv             |                                         [Remote Timing Attacks on Efficient Language Model Inference](https://arxiv.org/abs/2410.17175)                                          |                                      **Timing Attacks**&**Efficient Inference**&**Privacy**                                       |
| 24.10 |                                                                                       Meta                                                                                        |             arxiv             |                             [Towards Understanding the Fragility of Multilingual LLMs against Fine-Tuning Attacks](https://arxiv.org/abs/2410.18210)                             |                                **Fine-Tuning Attacks**&**Multilingual LLMs**&**Safety Alignment**                                 |
| 24.10 |                                                                        University of California San Diego                                                                         |             arxiv             |                                      [Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities](https://arxiv.org/abs/2410.18469)                                       |                                 **Jailbreaking**&**LLM Vulnerabilities**&**Adversarial Attacks**                                  |
| 24.10 |                                                                             Florida State University                                                                              |             arxiv             |                                  [Adversarial Attacks on Large Language Models Using Regularized Relaxation](https://arxiv.org/abs/2410.19160)                                   |                                        **Adversarial Attacks**&**Continuous Optimization**                                        |
| 24.10 |                                                                         The Pennsylvania State University                                                                         |             arxiv             |                                        [Humanizing the Machine: Proxy Attacks to Mislead LLM Detectors](https://arxiv.org/abs/2410.19230)                                        |                                           **Adversarial Attacks**&**Detection Evasion**                                           |
| 24.10 |                                                                         Nanyang Technological University                                                                          |             arxiv             |                                  [Mask-based Membership Inference Attacks for Retrieval-Augmented Generation](https://arxiv.org/abs/2410.20142)                                  |                          **Retrieval-Augmented Generation**&**Membership Inference Attacks**&**Privacy**                          |
| 24.10 |                                                                              George Mason University                                                                              |             arxiv             |                          [Hacking Back the AI-Hacker: Prompt Injection as a Defense Against LLM-driven Cyberattacks](https://arxiv.org/abs/2410.20911)                           |                             **Prompt Injection Defense**&**LLM Cybersecurity**&**Adversarial Inputs**                             |
| 24.10 |                                                                                 Fudan University                                                                                  |             arxiv             |                           [BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against Jailbreak Attacks](https://arxiv.org/abs/2410.20971)                           |                            **Jailbreak Defense**&**Vision-Language Models**&**Reinforcement Learning**                            |
| 24.10 |                                                                          Harbin Institute of Technology                                                                           |             arxiv             |                                [Stealthy Jailbreak Attacks on Large Language Models via Benign Data Mirroring](https://arxiv.org/abs/2410.21083)                                 |                                           **Jailbreak Attacks**&**Adversarial Prompts**                                           |
| 24.10 |                                                                      SRM Institute of Science and Technology                                                                      |             arxiv             |                                               [Palisade - Prompt Injection Detection Framework](https://arxiv.org/abs/2410.21146)                                                |                                        **Prompt Injection**&**Heuristic-based Detection**                                         |
| 24.10 |                                                                             The Ohio State University                                                                             |             arxiv             |        [AmpleGCG-Plus: A Strong Generative Model of Adversarial Suffixes to Jailbreak LLMs with Higher Success Rates in Fewer Attempts](https://arxiv.org/abs/2410.22143)        |                               **Jailbreak Attacks**&**Adversarial Suffixes**&**Generative Models**                                |
| 24.10 |                                                                                Zhejiang University                                                                                |             arxiv             |                                [HIJACKRAG: Hijacking Attacks against Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2410.22832)                                |                    **Retrieval-Augmented Generation**&**Prompt Injection Attacks**&**Security Vulnerability**                     |
| 24.10 |                                                                                The Baldwin School                                                                                 |             arxiv             |                            [Systematically Analyzing Prompt Injection Vulnerabilities in Diverse LLM Architectures](https://arxiv.org/abs/2410.23308)                            |                                   **Prompt Injection Vulnerabilities**&**Model Susceptibility**                                   |
| 24.10 |                                                                     Competition for LLM and Agent Safety 2024                                                                     |             arxiv             |                                  [Transferable Ensemble Black-box Jailbreak Attacks on Large Language Models](https://arxiv.org/abs/2410.23558)                                  |                                       **Black-box Jailbreak Attacks**&**Ensemble Methods**                                        |
| 24.10 |                                                             University of Electronic Science and Technology of China                                                              |             arxiv             |                                             [Pseudo-Conversation Injection for LLM Goal Hijacking](https://arxiv.org/abs/2410.23678)                                             |                                              **Goal Hijacking**&**Prompt Injection**                                              |
| 24.10 |                                                                                 Monash University                                                                                 |             arxiv             |                                    [Audio Is the Achilles‚Äô Heel: Red Teaming Audio Large Multimodal Models](https://arxiv.org/abs/2410.23861)                                    |                           **Audio Multimodal Models**&**Safety Vulnerabilities**&**Jailbreak Attacks**                            |
| 24.11 | Fudan University | arxiv | [IDEATOR: Jailbreaking VLMs Using VLMs](https://arxiv.org/abs/2411.00827) | **Vision-Language Models**&**Jailbreak Attack**&**Multimodal Safety** |
| 24.11 | International Computer Science Institute | arxiv | [Emoji Attack: A Method for Misleading Judge LLMs in Safety Risk Detection](https://arxiv.org/abs/2411.01077) | **Emoji Attack**&**Jailbreaking**&**Judge LLMs Bias** |
| 24.11 | Peking University | arxiv | [B4: A Black-Box ScruBBing Attack on LLM Watermarks](https://arxiv.org/abs/2411.01222) | **Black-Box Attack**&**Watermark Removal**&**Adversarial Text Generation** |
| 24.11 | University of Science and Technology of China | arxiv | [SQL Injection Jailbreak: a structural disaster of large language models](https://arxiv.org/abs/2411.01565) | **SQL Injection**&**Jailbreak Attack**&**LLM Vulnerability** |
| 24.11 | University of Illinois Urbana-Champaign | arxiv | [Stochastic Monkeys at Play: Random Augmentations Cheaply Break LLM Safety Alignment](https://arxiv.org/abs/2411.02785) | **Random Augmentations**&**Safety Alignment**&**LLM Jailbreak** |
| 24.11 | Cambridge ERA: AI Fellowship | arxiv | [What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms Behind Attacks](https://arxiv.org/abs/2411.03343) | **Jailbreak Prompts**&**Nonlinear Probes**&**Adversarial Attacks** |
| 24.11 | Alibaba Group | arxiv | [MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue](https://arxiv.org/abs/2411.03814) | **Multi-Round Dialogue**&**Jailbreak Agent**&**LLM Vulnerability** |
| 24.11 | Columbia University | arxiv | [Diversity Helps Jailbreak Large Language Models](https://arxiv.org/abs/2411.04223) | **Jailbreak Techniques**&**LLM Safety**&**Prompt Diversity** |
| 24.11 | Bangladesh University of Engineering and Technology | arXiv | [SequentialBreak: Large Language Models Can Be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains](https://arxiv.org/abs/2411.06426) | **Jailbreak Attacks**&**Prompt Engineering**&**LLM Vulnerabilities** |
| 24.11 | Xi'an Jiaotong-Liverpool University | arXiv | [Target-driven Attack for Large Language Models](https://arxiv.org/abs/2411.07268) | **Black-box Attacks**&**Optimization Methods** |
| 24.11 | Georgia Institute of Technology | arXiv | [LLM STINGER: Jailbreaking LLMs using RL Fine-tuned LLMs](https://arxiv.org/abs/2411.08862) | **Jailbreaking Attacks**&**Reinforcement Learning**&**Adversarial Suffixes** |
| 24.11 | Beijing University of Posts and Telecommunications | arXiv | [Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey](https://arxiv.org/abs/2411.09259) | **Jailbreak Attacks**&**Multimodal Generative Models**&**Security Challenges** |
| 24.11 | Arizona State University | NeurIPS 2024 SafeGenAI Workshop | [Zer0-Jack: A Memory-efficient Gradient-based Jailbreaking Method for Black-box Multi-modal Large Language Models](https://arxiv.org/abs/2411.07559) | **Black-box Jailbreaking**&**Multi-modal Models**&**Zeroth-order Optimization** |
| 24.11 | Zhejiang University | arxiv | [JailbreakLens: Interpreting Jailbreak Mechanism in the Lens of Representation and Circuit](https://arxiv.org/abs/2411.11114) | **Jailbreak Attacks**&**Large Language Models**&**Mechanism Interpretability** |
| 24.11 | University of Electronic Science and Technology of China | arxiv | [Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to Jailbreak Large Vision-Language Models](https://arxiv.org/abs/2411.11496) | **Jailbreaking**&**Large Vision-Language Models**&**Safety Snowball Effect** |
| 24.11 | Tsinghua University | arxiv | [Playing Language Game with LLMs Leads to Jailbreaking](https://arxiv.org/abs/2411.12762) | **Jailbreaking**&**Language Games**&**LLM Safety** |
| 24.11 | University of Texas at Dallas | arxiv | [AttentionBreaker: Adaptive Evolutionary Optimization for Unmasking Vulnerabilities in LLMs through Bit-Flip Attacks](https://arxiv.org/abs/2411.13757) | **Bit-Flip Attacks**&**Model Vulnerability Optimization** |
| 24.11 | BITS Pilani | arxiv | [GASP: Efficient Black-Box Generation of Adversarial Suffixes for Jailbreaking LLMs](https://arxiv.org/abs/2411.14133) | **Jailbreaking**&**Latent Bayesian Optimization**&**Adversarial Prompts** |



## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |        Type        |                                       Title                                       |                                  URL                                  |
|:-----:|:------------------:|:---------------------------------------------------------------------------------:|:---------------------------------------------------------------------:|
| 23.01 |     Community      |                              Reddit/ChatGPTJailbrek                               |           [link](https://www.reddit.com/r/ChatGPTJailbreak)           |
| 23.02 | Resource&Tutorials |                                  Jailbreak Chat                                   |                [link](https://www.jailbreakchat.com/)                 |
| 23.10 |     Tutorials      |                                Awesome-LLM-Safety                                 |         [link](https://github.com/ydyjya/Awesome-LLM-Safety)          |
| 23.10 |      Article       |                 Adversarial Attacks on LLMs(Author: Lilian Weng)                  | [link](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/) |
| 23.11 |       Video        | [1hr Talk] Intro to Large Language Models<br/>From 45:45(Author: Andrej Karpathy) |          [link](https://www.youtube.com/watch?v=zjkBMFhNj_g)          |

## üì∞News & Articles

| Date  |  Type   |            Title            |   Author    |                                  URL                                  |
|:-----:|:-------:|:---------------------------:|:-----------:|:---------------------------------------------------------------------:|
| 23.10 | Article | Adversarial Attacks on LLMs | Lilian Weng | [link](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/) |


## üßë‚Äçüè´Scholars