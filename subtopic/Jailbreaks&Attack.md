# Jailbreaks&Attack

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"

## üìëPapers

| Date  |                                                                                     Institute                                                                                     |          Publication          |                                                                                  Paper                                                                                   |                                                             Keywords                                                          |
|:-----:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------------------------------------:|
| 20.12 |                                                                                      Google                                                                                       |     USENIX Security 2021      |                [Extracting Training Data from Large Language Models](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting)                 |                                          **Verbatim Text Sequences**&**Rank Likelihood**                                      |
| 22.11 |                                                                                     AE Studio                                                                                     | NIPS2022(ML Safety Workshop)  |                                    [Ignore Previous Prompt: Attack Techniques For Language Models](https://arxiv.org/abs/2211.09527)                                     |                                                **Prompt Injection**&**Misaligned**                                            |
| 23.02 |                                                                                Saarland University                                                                                |             arxiv             |          [Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/abs/2302.12173)           |                      **Adversarial Prompting**&**Indirect Prompt Injection**&**LLM-Integrated Applications**                  |
| 23.04 |                                                                  Hong Kong University of Science and Technology                                                                   |      EMNLP2023(findings)      |                                          [Multi-step Jailbreaking Privacy Attacks on ChatGPT](https://arxiv.org/abs/2304.05197)                                          |                                                    **Privacy**&**Jailbreaks**                                                 |
| 23.05 |                              Jinan University, Hong Kong University of Science and Technology, Nanyang Technological University, Zhejiang University                              |          EMNLP 2023           |                        [Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models](https://arxiv.org/abs/2305.01219)                        |                                                       **Backdoor Attacks**                                                    |
| 23.05 |                                                  Nanyang Technological University, University of New South Wales, Virginia Tech                                                   |             arXiv             |                                   [Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study](https://arxiv.org/abs/2305.13860)                                    |                                            Large **Jailbreak**&**Prompt Engineering**                                         |
| 23.06 |                                                                               Princeton University                                                                                |      ICML2023(Workshop)       |                                 [Visual Adversarial Examples Jailbreak Aligned Large Language Models](https://arxiv.org/abs/2306.15447)                                  |                                **Visual Language Models**&**Adversarial Attacks**&**AI Alignment**                            |
| 23.06 | Nanyang Technological University, University of New South Wales, Huazhong University of Science and Technology, Southern University of Science and Technology, Tianjin University |             arxiv             |                                     [Prompt Injection attack against LLM-integrated Applications](https://arxiv.org/abs/2306.05499)                                      |                         **&LLM-integrated Applications**&**Security Risks**&**Prompt Injection Attacks**                      |
| 23.06 |                                                                                      Google                                                                                       |             arxiv             |                                          [Are aligned neural networks adversarially aligned?](https://arxiv.org/abs/2306.15447)                                          |                                                   **Multimodal**&**Jailbreak**                                                |
| 23.07 |                                                                                        CMU                                                                                        |             arxiv             |                              [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)                               |                                   **Jailbreak**&**Transferable Attack**&**Adversarial Attack**                                |
| 23.07 |                                                            Language Technologies Institute Carnegie Mellon University                                                             |             arXiv             |                   [Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success](https://arxiv.org/abs/2307.06865)                   |                           **Prompt Extraction**&**Attack Success Measurement**&**Defensive Strategies**                       |
| 23.07 |                                                                         Nanyang Technological University                                                                          |           NDSS2023            |                             [MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots](https://arxiv.org/abs/2307.08715)                             |                                  **Jailbreak**&**Reverse-Engineering**&**Automatic Generation**                               |
| 23.07 |                                                                                   Cornell Tech                                                                                    |             arxiv             |                           [Abusing Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs](https://arxiv.org/abs/2307.10490)                           |                       **Multi-Modal LLMs**&**Indirect Instruction Injection**&**Adversarial Perturbations**                   |
| 23.07 |                                                                   UNC Chapel Hill, Google DeepMind, ETH Zurich                                                                    | AdvML Frontiers Workshop 2023 |                                    [Backdoor Attacks for In-Context Learning with Language Models](https://arxiv.org/abs/2307.14692)                                     |                                           **Backdoor Attacks**&**In-Context Learning**                                        |
| 23.07 |                                                                                  Google DeepMind                                                                                  |             arXiv             |                           [Large language models (LLMs) are now highly capable at a diverse range of tasks](https://arxiv.org/abs/2307.15008)                            |                              **Adversarial Machine Learning**&**AI-Guardian**&**Defense Robustness**                          |
| 23.08 |                                                              CISPA Helmholtz Center for Information Security; NetApp                                                              |             arxiv             |               [‚ÄúDo Anything Now‚Äù: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models](https://arxiv.org/abs/2308.03825)                |                               **Jailbreak Prompts**&**Adversarial Prompts**&**Proactive Detection**                           |
| 23.09 |                                                                          Ben-Gurion University, DeepKeep                                                                          |             arxiv             |                                [OPEN SESAME! UNIVERSAL BLACK BOX JAILBREAKING OF LARGE LANGUAGE MODELS](https://arxiv.org/abs/2309.01446)                                |                               **Genetic Algorithm**&**Adversarial Prompt**&**Black Box Jailbreak**                            |
| 23.10 |                                                      Princeton University, Virginia Tech, IBM Research, Stanford University                                                       |             arxiv             |                       [FINE-TUNING ALIGNED LANGUAGE MODELS COMPROMISES SAFETY EVEN WHEN USERS DO NOT INTEND TO!](https://arxiv.org/abs/2310.03693)                       |                                     **Fine-tuning****Safety Risks**&**Adversarial Training**                                  |
| 23.10 |                                                 University of California Santa Barbara, Fudan University, Shanghai AI Laboratory                                                  |             arxiv             |                               [SHADOW ALIGNMENT: THE EASE OF SUBVERTING SAFELY-ALIGNED LANGUAGE MODELS](https://arxiv.org/abs/2310.02949)                                |                                          **AI Safety**&**Malicious Use**&**Fine-tuning**                                      |
| 23.10 |                                                                                 Peking University                                                                                 |             arxiv             |                         [Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations](https://arxiv.org/abs/2310.06387)                          |                           **In-Context Learning**&**Adversarial Attacks**&**In-Context Demonstrations**                       |
| 23.10 |                                                                University of Maryland College Park, Adobe Research                                                                |             arxiv             |                          [AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2310.15140)                          |                                   **Adversarial Attacks**&**Interpretabilty**&**Jailbreaking**                                |
| 23.11 |                                                                                      MBZUAI                                                                                       |             arxiv             |                         [Robustness Tests for Automatic Machine Translation Metrics with Adversarial Attacks](https://arxiv.org/abs/2311.00508)                          |                             **Adversarially-synthesized Texts**&**Word-level Attacks**&**Evaluation**                         |
| 23.11 |                                                                                 Palisade Research                                                                                 |             arxiv             |                                 [BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B](https://arxiv.org/abs/2311.00117)                                  |                                                   **Remove Safety Fine-tuning**                                               |
| 23.11 |                                                                               University of Twente                                                                                |          ICNLSP 2023          |                                   [Efficient Black-Box Adversarial Attacks on Neural Text Detectors](https://arxiv.org/abs/2311.01873)                                   |                                           **Misclassification**&**Adversarial attacks**                                       |
| 23.11 |                                                                  PRISM AI&Harmony Intelligenc&Leap Laboratories                                                                   |             arxiv             |                      [Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation](https://arxiv.org/abs/2311.03348 )                      |                                **Persona-modulation Attacks**&**Jailbreaks**&**Automated Prompt**                             |
| 23.11 |                                                                                Tsinghua University                                                                                |             arxiv             |                               [Jailbreaking Large Vision-Language Models via Typographic Visual Prompts](https://arxiv.org/abs/2311.05608)                               |                                   **Typographic Attack**&**Multi-modal**&**Safety Evaluation**                                |
| 23.11 |                                                        Huazhong University of Science and Technology, Tsinghua University                                                         |             arxiv             |             [Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration](https://arxiv.org/abs/2311.06062)              |                                     **Membership Inference Attacks**&**Privacy and Security**                                 |
| 23.11 |                                                                          Nanjing University, Meituan Inc                                                                          |             arxiv             |                [A Wolf in Sheep‚Äôs Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily](https://arxiv.org/abs/2311.08268)                |                              **Jailbreak Prompts**&**Safety Alignment**&**Safeguard Effectiveness**                           |
| 23.11 |                                                                                  Google DeepMind                                                                                  |             arxiv             |           [Frontier Language Models Are Not Robust to Adversarial Arithmetic or "What Do I Need To Say So You Agree 2+2=5?"](https://arxiv.org/abs/2311.07587)           |                              **Adversarial Arithmetic**&**Model Robustness**&**Adversarial Attacks**                          |
| 23.11 |                                                               University of Illinois Chicago, Texas A&M University                                                                |             arxiv             |                     [DALA: A Distribution-Aware LoRA-Based Adversarial Attack against Pre-trained Language Models](https://arxiv.org/abs/2311.08598)                     |                                **Adversarial Attack**&**Distribution-Aware**&**LoRA-Based Attack**                            |
| 23.11 |                                                                         Illinois Institute of Technology                                                                          |             arxiv             |               [Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment](https://arxiv.org/abs/2311.09433)                |              Backdoor Activation Attack&Large Language Models&AI Safety&Activation Steering&Trojan Steering Vectors           |
| 23.11 |                                                                              Wayne State University                                                                               |             arXiv             |                                 [Hijacking Large Language Models via Adversarial In-Context Learning](https://arxiv.org/abs/2311.09948)                                  |                         **Adversarial Attacks**&**Gradient-Based Prompt Search**&**Adversarial Suffixes**                     |
| 23.11 |                                   Hong Kong Baptist University, Shanghai Jiao Tong University, Shanghai AI Laboratory, The University of Sydney                                   |             arXiv             |                                   [DeepInception: Hypnotize Large Language Model to Be Jailbreaker](https://arxiv.org/abs/2311.03191)                                    |                                                  **Jailbreak**&**DeepInception**                                              |
| 23.11 |                                                                        Xi‚Äôan Jiaotong-Liverpool University                                                                        |             arxiv             |                             [Generating Valid and Natural Adversarial Examples with Large Language Models](https://arxiv.org/abs/2311.11861)                             |                                         **Adversarial examples**&**Text classification**                                      |
| 23.11 |                                                                             Michigan State University                                                                             |             arxiv             |                           [Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems](https://arxiv.org/abs/2311.11796)                            |                                  **Transferable Attacks**&**AI Systems**&**Adversarial Attacks**                              |
| 23.11 |                                                                     Tsinghua University & Kuaishou Technology                                                                     |             arxiv             |                                     [Evil Geniuses: Delving into the Safety of LLM-based Agents](https://arxiv.org/abs/2311.11855v1)                                     |                                       **LLM-based Agents**&**Safety**&**Malicious Attacks**                                   |
| 23.11 |                                                                                Cornell University                                                                                 |             arxiv             |                                                       [Language Model Inversion](https://arxiv.org/abs/2311.13647)                                                       |                                     **Model Inversion**&**Prompt Reconstruction**&**Privacy**                                 |
| 23.11 |                                                                                    ETH Zurich                                                                                     |             arxiv             |                                      [Universal Jailbreak Backdoors from Poisoned Human Feedback](https://arxiv.org/abs/2311.14455)                                      |                                                   **RLHF**&**Backdoor Attacks**                                               |
| 23.11 |                                                                          UC Santa Cruz, UNC-Chapel Hill                                                                           |             arxiv             |                              [How Many Are in This Image? A Safety Evaluation Benchmark for Vision LLMs](https://arxiv.org/abs/2311.16101)                               |                         **Vision Large Language Models**&**Safety Evaluation**&A**dversarial Robustness**                     |
| 23.11 |                                                                               Texas Tech University                                                                               |             arxiv             |                    [Exploiting Large Language Models (LLMs) through Deception Techniques and Persuasion Principles](https://arxiv.org/abs/2311.14876)                    |                                    **Social Engineering**&**Security**&**Prompt Engineering**                                 |
| 23.11 |                                                                             Johns Hopkins University                                                                              |             arxiv             |                                    [Instruct2Attack: Language-Guided Semantic Adversarial Attacks](https://arxiv.org/abs/2311.15551)                                     |                          **Language-guided Attacks**&**Latent Diffusion Models**&**Adversarial Attack**                       |
| 23.11 |                                                 Google DeepMind, University of Washington, Cornell, CMU, UC Berkeley, ETH Zurich                                                  |             arxiv             |                                [Scalable Extraction of Training Data from (Production) Language Models](https://arxiv.org/abs/2311.17035)                                |                              **Extractable Memorization**&**Data Extraction**&**Adversary Attacks**                           |
| 23.11 |                                    University of Maryland, Mila, Towards AI, Stanford, Technical University of Sofia, University of Milan, NYU                                    |             arxiv             |    [Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition](https://arxiv.org/abs/2311.16119)    |                                              **Prompt Hacking**&**Security Threats**                                          |
| 23.11 |                                               University of Washington, UIUC, Pennsylvania State University, University of Chicago                                                |             arxiv             |                              [IDENTIFYING AND MITIGATING VULNERABILITIES IN LLM-INTEGRATED APPLICATIONS](https://arxiv.org/abs/2311.16153)                               |                                        **LLM-Integrated Applications**&**Attack Surfaces**                                    |
| 23.11 |                                      Jinan University, Guangzhou Xuanyuan Research Institute Co. Ltd., The Hong Kong Polytechnic University                                       |             arxiv             |                        [TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4](https://arxiv.org/abs/2311.17429)                        |                                           **Prompt-based Learning**&**Backdoor Attack**                                       |
| 23.12 |                                                                         The Pennsylvania State University                                                                         |             arxiv             |                         [Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections](https://arxiv.org/abs/2312.00027)                         |                                            **Backdoor Injection**&**Safety Alignment**                                        |
| 23.12 |                                                                                 Drexel University                                                                                 |             arXiv             |                      [A Survey on Large Language Model (LLM) Security and Privacy: The Good the Bad and the Ugly](https://arxiv.org/abs/2312.02003)                      |                                               **Security**&**Privacy**&**Attacks**                                            |
| 23.12 |                                                                       Yale University, Robust Intelligence                                                                        |             arXiv             |                                      [Tree of Attacks: Jailbreaking Black-Box LLMs Automatically](https://arxiv.org/abs/2312.02119)                                      |                           **Tree of Attacks with Pruning (TAP)**&**Jailbreaking**&**Prompt Generation**                       |
| 23.12 |                                                                       Independent (Now at Google DeepMind)                                                                        |             arXiv             |                                  [Scaling Laws for Adversarial Attacks on Language Model Activations](https://arxiv.org/abs/2312.02780)                                  |                              **Adversarial Attacks**&**Language Model Activations**&**Scaling Laws**                          |
| 23.12 |                                                                          Harbin Institute of Technology                                                                           |             arxiv             |                      [Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak](https://arxiv.org/abs/2312.04127)                      |                           **Jailbreak Attack**&**Inherent Response Tendency**&**Affirmation Tendency**                        |
| 23.12 |                                                                          University of Wisconsin-Madison                                                                          |             arxiv             |                  [DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial Natural Language Instructions](https://arxiv.org/abs/2312.04730)                   |                                   **Code Generation**&**Adversarial Attacks**&**Cybersecurity**                               |
| 23.12 |                                                                      Carnegie Melon University, IBM Research                                                                      |             arxiv             |                          [Forcing Generative Models to Degenerate Ones: The Power of Data Poisoning Attacks](https://arxiv.org/abs/2312.04748)                           |                           **Data Poisoning Attacks**&**Natural Language Generation**&**Cybersecurity**                        |
| 23.12 |                                                                                 Purdue University                                                                                 |      NIPS2023ÔºàWorkshopÔºâ       |                           [Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs](https://arxiv.org/abs/2312.04782)                            |                              **Knowledge Extraction**&**Interrogation Techniques**&**Cybersecurity**                          |
| 23.12 |                                                                 Sungkyunkwan University, University of Tennessee                                                                  |             arXiv             | [Poisoned ChatGPT Finds Work for Idle Hands: Exploring Developers‚Äô Coding Practices with Insecure Suggestions from Poisoned AI Models](https://arxiv.org/abs/2312.06227) |                                          **Poisoning Attacks**&**Software Development**                                       |
| 23.12 |                                                     North Carolina State University, New York University, Stanford University                                                     |             arXiv             |        [BEYOND GRADIENT AND PRIORS IN PRIVACY ATTACKS: LEVERAGING POOLER LAYER INPUTS OF LANGUAGE MODELS IN FEDERATED LEARNING](https://arxiv.org/abs/2312.05720)        |                                            **Federated Learning**&P**rivacy Attacks**                                         |
| 23.12 |                                                            Korea Advanced Institute of Science, Graduate School of AI                                                             |             arxiv             |                                            [Hijacking Context in Large Multi-modal Models](https://arxiv.org/abs/2312.07553)                                             |                                        **Large Multi-modal Models**&**Context Hijacking**                                     |
| 23.12 |                                           Xi‚Äôan Jiaotong University, Nanyang Technological University, Singapore Management University                                            |             arXiv             |                                [A Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection](https://arxiv.org/abs/2312.10766)                                 |                                            **Jailbreaking Detection**&**Multi-Modal**                                         |
| 23.12 |                                                               Logistic and Supply Chain MultiTech R&D Centre (LSCM)                                                               |          UbiSec-2023          |            [A Comprehensive Survey of Attack Techniques Implementation and Mitigation Strategies in Large Language Models](https://arxiv.org/abs/2312.10982)             |                                         **Cybersecurity Attacks**&**Defense Strategies**                                      |
| 23.12 |                                                             University of Illinois Urbana-Champaign, VMware Research                                                              |             arXiv             |                                [BYPASSING THE SAFETY TRAINING OF OPEN-SOURCE LLMS WITH PRIMING ATTACKS](https://arxiv.org/abs/2312.12321)                                |                                              **Safety Training**&**Priming Attacks**                                          |
| 23.12 |                                                                          Delft University of Technology                                                                           |           ICSE 2024           |                                       [Traces of Memorisation in Large Language Models for Code](https://arxiv.org/abs/2312.11658)                                       |                                         **Code Memorisation**&**Data Extraction Attacks**                                     |
| 23.12 |                                     University of Science and Technology of China, Hong Kong University of Science and Technology, Microsoft                                      |             arxiv             |                    [Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models](https://arxiv.org/abs/2312.14197)                     |                               **Indirect Prompt Injection Attacks**&**BIPIA Benchmark**&**Defense**                           |
| 23.12 |                                                                Nanjing University of Aeronautics and Astronautics                                                                 |           NLPCC2023           |                                  [Punctuation Matters! Stealthy Backdoor Attack for Language Models](https://arxiv.org/abs/2312.15867)                                   |                                        **Backdoor Attack**&**PuncAttack**&**Stealthiness**                                    |
| 23.12 |                                                             FAR AI, McGill University, MILA, Jagiellonian University                                                              |             arXiv             |                                                     [Exploiting Novel GPT-4 APIs](https://arxiv.org/abs/2312.14302)                                                      |                               **Fine-Tuning**&**Knowledge Retrieval**&**Security Vulnerabilities**                            |
| 23.12 |                                                                                       EPFL                                                                                        |                               |                                    [Adversarial Attacks on GPT-4 via Simple Random Search](https://www.andriushchenko.me/gpt4adv.pdf)                                    |                                      **Adversarial Attacks**&**Random Search**&**Jailbreak**                                  |
| 24.01 |                                                               Logistic and Supply Chain MultiTech R&D Centre (LSCM)                                                               |           CSDE2023            |           [A Novel Evaluation Framework for Assessing Resilience Against Prompt Injection Attacks in Large Language Models](https://arxiv.org/abs/2401.00991)            |                                      **Evaluation**&**Prompt Injection**&**Cyber Security**                                   |
| 24.01 |                                                                         University of Southern California                                                                         |             arxiv             |          [The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance](https://arxiv.org/abs/2401.03729)          |                                   **Prompt Engineering**&**Text Classification**&**Jailbreaks**                               |
| 24.01 |                                                     Virginia Tech, Renmin University of China, UC Davis, Stanford University                                                      |             arxiv             |              [How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs](https://arxiv.org/abs/ÈìæÊé•ÂæÖÂÆö)               |                                  **AI Safety**&**Persuasion Adversarial Prompts**&**Jailbreak**                               |
| 24.01 |                                                    Anthropic, Redwood Research, Mila Quebec AI Institute, University of Oxford                                                    |             arxiv             |                             [SLEEPER AGENTS: TRAINING DECEPTIVE LLMS THAT PERSIST THROUGH SAFETY TRAINING](https://arxiv.org/abs/2401.05566)                             |                    **Deceptive Behavior**&**Safety Training**&**Backdoored Behavior**&**Adversarial Training**                |
| 24.01 |                                          Jinan University,Nanyang Technological University, Beijing Institute of Technology, Pazhou Lab                                           |             arxiv             |                       [UNIVERSAL VULNERABILITIES IN LARGE LANGUAGE MODELS: IN-CONTEXT LEARNING BACKDOOR ATTACKS](https://arxiv.org/abs/2401.05949)                       |                                     **In-context Learning**&**Security**&**Backdoor Attacks**                                 |
| 24.01 |                                                                            Carnegie Mellon University                                                                             |             arxiv             |                                        [Combating Adversarial Attacks with Multi-Agent Debate](https://arxiv.org/abs/2401.05998)                                         |                                    **Adversarial Attacks**&**Multi-Agent Debate**&**Red Team**                                |
| 24.01 |                                                                                 Fudan University                                                                                  |             arxiv             |                         [Open the Pandora‚Äôs Box of LLMs: Jailbreaking LLMs through Representation Engineering](https://arxiv.org/abs/2401.06824)                         |                                          **LLM Security**&**Representation Engineering**                                      |
| 24.01 |                                             Northwestern University, New York University, University of Liverpool, Rutgers University                                             |             arxiv             |                    [AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models](https://arxiv.org/abs/2401.09002)                     |                              **Jailbreak Attack**&**Evaluation Frameworks**&**Ground Truth Dataset**                          |
| 24.01 |                                                                          Kyushu Institute of Technology                                                                           |             arxiv             |                               [ALL IN HOW YOU ASK FOR IT: SIMPLE BLACK-BOX METHOD FOR JAILBREAK ATTACKS](https://arxiv.org/abs/2401.09798)                               |                                            **Jailbreak Attacks**&**Black-box Method**                                         |
| 24.01 |                                                                                        MIT                                                                                        |             arXiv             |                     [Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning](https://arxiv.org/abs/2401.10862)                      |                                                 **Jailbreaking**&**Model Safety**                                             |
| 24.01 |                                                                                Aalborg University                                                                                 |             arxiv             |                                   [Text Embedding Inversion Attacks on Multilingual Language Models](https://arxiv.org/abs/2401.12192)                                   |                             **Text Embedding**&**Inversion Attacks**&**Multilingual Language Models**                         |
| 24.01 |                                         University of Illinois Urbana-Champaign, University of Washington, Western Washington University                                          |             arxiv             |                               [BADCHAIN: BACKDOOR CHAIN-OF-THOUGHT PROMPTING FOR LARGE LANGUAGE MODELS](https://arxiv.org/abs/2401.12242)                                |                                        **Chain-of-Thought Prompting**&**Backdoor Attacks**                                    |
| 24.01 |                                                                 The University of Hong Kong, Zhejiang University                                                                  |             arxiv             |                                                  [Red Teaming Visual Language Models](https://arxiv.org/abs/2401.12915)                                                  |                                            **Vision-Language Models**&**Red Teaming**                                         |
| 24.01 |                                              University of California Santa Barbara,Sea AI Lab Singapore, Carnegie Mellon University                                              |             arxiv             |                                         [Weak-to-Strong Jailbreaking on Large Language Models](https://arxiv.org/abs/2401.17256)                                         |                                      **Jailbreaking**&**Adversarial Prompts**&**AI Safety**                                   |
| 24.02 |                                                                                 Boston University                                                                                 |             arxiv             |                               [Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks](https://arxiv.org/abs/2402.00626)                                |                        **Large Vision-Language Models**&**Typographic Attacks**&**Self-Generated Attacks**                    |
| 24.02 |                                                                   Copenhagen Business School, Temple University                                                                   |             arxiv             |                             [An Early Categorization of Prompt Injection Attacks on Large Language Models](https://arxiv.org/abs/2402.00898)                             |                                              **Prompt Injection**&**Categorization**                                          |
| 24.02 |                                                   Michigan State University, Okinawa Institute of Science and Technology (OIST)                                                   |             arxiv             |                                                [Data Poisoning for In-context Learning](https://arxiv.org/abs/2402.02160)                                                |                                      **In-context learning**&**Data poisoning**&**Security**                                  |
| 24.02 |                                                                  CISPA Helmholtz Center for Information Security                                                                  |             arxiv             |                                        [Conversation Reconstruction Attack Against GPT Models](https://arxiv.org/abs/2402.02987)                                         |                               **Conversation Reconstruction Attack**&**Privacy risks**&**Security**                           |
| 24.02 |                                 University of Illinois Urbana-Champaign, Center for AI Safety, Carnegie Mellon University, UC Berkeley, Microsoft                                 |             arxiv             |                     [HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal](https://arxiv.org/abs/2402.04249)                      |                                           **Automated Red Teaming**&**Robust Refusal**                                        |
| 24.02 |                                           University of Washington, University of Virginia, Allen Institute for Artificial Intelligence                                           |             arxiv             |                                    [Do Membership Inference Attacks Work on Large Language Models?](https://arxiv.org/abs/2402.07841)                                    |                                     **Membership Inference Attacks**&**Privacy**&**Security**                                 |
| 24.02 |                                                 Pennsylvania State University, Wuhan University, Illinois Institute of Technology                                                 |             arxiv             |                 [PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2402.07867)                  |                                **Knowledge Poisoning Attacks**&**Retrieval-Augmented Generation**                             |
| 24.02 |                                                             Purdue University, University of Massachusetts at Amherst                                                             |             arxiv             |                        [RAPID OPTIMIZATION FOR JAILBREAKING LLMS VIA SUBCONSCIOUS EXPLOITATION AND ECHOPRAXIA](https://arxiv.org/abs/2402.05467)                         |                                               **Jailbreaking LLM**&**Optimization**                                           |
| 24.02 |                                                                  CISPA Helmholtz Center for Information Security                                                                  |             arxiv             |                                      [Comprehensive Assessment of Jailbreak Attacks Against LLMs](https://arxiv.org/abs/2402.05668)                                      |                                   **Jailbreak Attacks**&**Attack Methods**&**Policy Alignment**                               |
| 24.02 |                                                                                    UC Berkeley                                                                                    |             arxiv             |                                  [StruQ: Defending Against Prompt Injection with Structured Queries](https://arxiv.org/abs/2402.06363)                                   |                            **Prompt Injection Attacks**&**Structured Queries**&**Defense Mechanisms**                         |
| 24.02 |                                  Nanyang Technological University, Huazhong University of Science and Technology, University of New South Wales                                   |             arxiv             |                                 [PANDORA: Jailbreak GPTs by Retrieval Augmented Generation Poisoning](https://arxiv.org/abs/2402.08416)                                  |                                  **Jailbreak Attacks**&**Retrieval Augmented Generation (RAG)**                               |
| 24.02 |                                                                          Sea AI Lab, Southern University                                                                          |             arxiv             |                                    [Test-Time Backdoor Attacks on Multimodal Large Language Models](https://arxiv.org/abs/2402.08577)                                    |                   **Backdoor Attacks**&**Multimodal Large Language Models (MLLMs)**&**Adversarial Test Images**               |
| 24.02 |                                      University of Illinois at Urbana‚ÄìChampaign, University of California, San Diego, Allen Institute for AI                                      |             arxiv             |                                 [COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability](https://arxiv.org/abs/2402.08679)                                 |                                         **Jailbreaks**&**Controllable Attack Generation**                                     |
| 24.02 |                                                                                    ISCAS, NTU                                                                                     |             arxiv             |                              [Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues](https://arxiv.org/abs/2402.09091)                              |                                       **Jailbreak Attacks**&**Indirect Attack**&**Puzzler**                                   |
| 24.02 |                                                     √âcole Polytechnique F√©d√©rale de Lausanne, University of Wisconsin-Madison                                                     |             arxiv             |                           [Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks](https://arxiv.org/abs/2402.09177)                           |                         **Jailbreaking Attacks**&**Contextual Interaction**&**Multi-Round Interactions**                      |
| 24.02 |                                 University of Electronic Science and Technology of China, CISPA Helmholtz Center for Information Security, NetApp                                 |             arxiv             |                         [Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization](https://arxiv.org/abs/2402.09179)                          |                                    **Customization**&**Instruction Backdoor Attacks**&**GPTs**                                |
| 24.02 |                                                                    Shanghai Artificial Intelligence Laboratory                                                                    |             arxiv             |                               [Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey](https://arxiv.org/abs/2402.09283)                                |                                       **LLM Conversation Safety**&**Attacks**&**Defenses**                                    |
| 24.02 |                                                                         UC Berkeley, New York University                                                                          |             arxiv             |                                     [PAL: Proxy-Guided Black-Box Attack on Large Language Models](https://arxiv.org/abs/2402.09674)                                      |                                       **Black-Box Attack**&**Proxy-Guided Attack**&**PAL**                                    |
| 24.02 |                                                                    Center for Human-Compatible AI, UC Berkeley                                                                    |             arxiv             |                                                 [A STRONGREJECT for Empty Jailbreaks](https://arxiv.org/abs/2402.10260)                                                  |                                         **Jailbreaks**&**Benchmarking**&**StrongREJECT**                                      |
| 24.02 |                                                                             Arizona State University                                                                              |             arxiv             |                            [Jailbreaking Proprietary Large Language Models using Word Substitution Cipher](https://arxiv.org/abs/2402.10601)                             |                                **Jailbreak**&**Word Substitution Cipher**&**Attack Success Rate**                             |
| 24.02 |                                                         Renmin University of China, Beijing, Peking University, WeChat AI                                                         |             arxiv             |                            [Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents](https://arxiv.org/abs/2402.11208)                             |                                        **Backdoor Attacks**&**Agent Safety**&**Framework**                                    |
| 24.02 |                                               University of Washington, UIUC, Western Washington University, University of Chicago                                                |             arxiv             |                                  [ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs](https://arxiv.org/abs/2402.11753)                                   |                                     **ASCII Art**&**Jailbreak Attacks**&**Safety Alignment**                                  |
| 24.02 |      Jinan University, Nanyang Technological University, Zhejiang University, Hong Kong University of Science and Technology, Beijing Institute of Technology, Sony Research      |             arxiv             |                       [Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.12168)                        | **Weight-Poisoning Backdoor Attacks**&**Parameter-Efficient Fine-Tuning (PEFT)**&**Poisoned Sample Identification Module (PSIM)** |
| 24.02 |                                                                  CISPA Helmholtz Center for Information Security                                                                  |             arxiv             |                                        [Prompt Stealing Attacks Against Large Language Models](https://arxiv.org/abs/2402.12959)                                         |                                                **Prompt Engineering**&**Security**                                            |
| 24.02 |                        University of New South Wales Australia, Delft University of Technology The Netherlands&Nanyang Technological University Singapore                         |             arxiv             |                                [LLM Jailbreak Attack versus Defense Techniques - A Comprehensive Study](https://arxiv.org/abs/2402.13457)                                |                                           **Jailbreak Attacks**&**Defense Techniques**                                        |
| 24.02 |                                                               Wayne State University, University of Michigan-Flint                                                                |             arxiv             |                                  [Learning to Poison Large Language Models During Instruction Tuning](https://arxiv.org/abs/2402.13459)                                  |                                              **Data Poisoning**&**Backdoor Attacks**                                          |
| 24.02 |                                            Nanyang Technological University, Zhejiang University, The Chinese University of Hong Kong                                             |             arxiv             |                            [Backdoor Attacks on Dense Passage Retrievers for Disseminating Misinformation](https://arxiv.org/abs/2402.13532)                             |                                **Dense Passage Retrieval**&**Backdoor Attacks**&**Misinformation**                            |
| 24.02 |                                                                              University of Michigan                                                                               |             arxiv             |                         [PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails](https://arxiv.org/abs/2402.15911)                          |                                        **Universal Adversarial Prefixes**&**Guard Models**                                    |
| 24.02 |                                                                                       Meta                                                                                        |             arxiv             |                                [Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts](https://arxiv.org/abs/2402.16822)                                 |                                     **Adversarial Prompts**&**Quality-Diversity**&**Safety**                                  |
| 24.02 |                                                                                 Fudan University                                                                                  |             arxiv             |                       [CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models](https://arxiv.org/abs/2402.16717)                        |                                         **Personalized Encryption**&**Safety Mechanisms**                                     |
| 24.02 |                                                                            Carnegie Mellon University                                                                             |             arxiv             |                                        [Attacking LLM Watermarks by Exploiting Their Strengths](https://arxiv.org/abs/2402.16187)                                        |                                            **LLM Watermarks**&**Adversarial Attacks**                                         |
| 24.02 |                                                                                Beihang University                                                                                 |             arxiv             |     [From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings](https://arxiv.org/abs/2402.16006)      |                                       **Adversarial Suffix**&**Text Embedding Translation**                                   |
| 24.02 |                                                                        University of Maryland College Park                                                                        |             arxiv             |                                    [Fast Adversarial Attacks on Language Models In One GPU Minute](https://arxiv.org/abs/2402.15570)                                     |                                  **Adversarial Attacks**&**BEAST**&**Computational Efficiency**                               |
| 24.02 |                                                                    Shanghai Artificial Intelligence Laboratory                                                                    |             arxiv             |                                [Attacks Defenses and Evaluations for LLM Conversation Safety: A Survey](https://arxiv.org/abs/2402.09283)                                |                                                **Conversation Safety**&**Survey**                                             |
| 24.02 |                                                    Beijing University of Posts and Telecommunications, University of Michigan                                                     |             arxiv             |                       [Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue](https://arxiv.org/abs/2402.17262)                        |                                         **Multi-turn Dialogue**&**Safety Vulnerability**                                      |
| 24.02 |                                        University of California, The Hongkong University of Science and Technology, University of Maryland                                        |             arxiv             |                          [DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers](https://arxiv.org/abs/2402.16914)                           |                                         **Jailbreaking Attacks**&**Prompt Decomposition**                                     |
| 24.02 |                                                           Massachusetts Institute of Technology, MIT-IBM Watson AI Lab                                                            |             arxiv             |                                        [CURIOSITY-DRIVEN RED-TEAMING FOR LARGE LANGUAGE MODELS](https://arxiv.org/abs/2402.19464)                                        |                                         **Curiosity-Driven Exploration**&**Red Teaming**                                      |
| 24.02 |       SKLOIS Institute of Information Engineering Chinese Academy of Science, School of Cyber Security University of Chinese Academy of Sciences,Tsinghua University,RealAI       |             arxiv             |            [Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction](https://arxiv.org/abs/2402.18104)             |                                **Jailbreaking**&**Large Language Models**&**Adversarial Attacks**                             |
| 24.03 |                                                                   Rice University, Samsung Electronics America                                                                    |             arxiv             |                               [LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario](https://arxiv.org/abs/2403.00108)                               |                              **Low-Rank Adaptation (LoRA)**&**Backdoor Attacks**&**Model Security**                           |
| 24.03 |                                                                            The University of Hong Kong                                                                            |             arxiv             |                                    [ImgTrojan: Jailbreaking Vision-Language Models with ONE Image](https://arxiv.org/abs/2403.02910)                                     |                               **Vision-Language Models**&**Data Poisoning**&**Jailbreaking Attack**                           |
| 24.03 |                                                                                  SPRING Lab EPFL                                                                                  |             arxiv             |                      [Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks](https://arxiv.org/abs/2403.03792)                       |                             **Prompt Injection Attacks**&**Optimization-Based Approach**&**Security**                         |
| 24.03 |                                            Shanghai University of Finance and Economics, Southern University of Science and Technology                                            |             arxiv             |                                [Tastle: Distract Large Language Models for Automatic Jailbreak Attack](https://arxiv.org/abs/2403.08424)                                 |                                           **Jailbreak Attack**&**Black-box Framework**                                        |
| 24.03 |                                                 Google DeepMind, ETH Zurich, University of Washington, OpenAI, McGill University                                                  |             arxiv             |                                             [Stealing Part of a Production Language Model](https://arxiv.org/abs/2403.06634)                                             |                                        **Model Stealing**&**Language Models**&**Security**                                    |
| 24.03 |                                                                              University of Edinburgh                                                                              |             arxiv             |                  [Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks](https://arxiv.org/abs/2403.09832)                   |                             **Prompt Injection Attacks**&**Machine Translation**&**Inverse Scaling**                          |
| 24.03 |                                                                         Nanyang Technological University                                                                          |             arxiv             |                                     [BADEDIT: BACKDOORING LARGE LANGUAGE MODELS BY MODEL EDITING](https://arxiv.org/abs/2403.13355)                                      |                                        **Backdoor Attacks**&**Model Editing**&**Security**                                    |
| 24.03 |                                                                     Fudan University, Shanghai AI Laboratory                                                                      |             arxiv             |                              [EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models](https://arxiv.org/abs/2403.12171)                               |                                         **Jailbreak Attacks**&**Security**&**Framework**                                      |
| 24.03 |                          Shanghai Key Lab of Intelligent Information Processing, Fudan University, Shanghai Engineering Research Center of AI & Robotics                          |             arxiv             |      [Improving Adversarial Transferability of Visual-Language Pre-training Models through Collaborative Multimodal Interaction](https://arxiv.org/abs/2403.10883)       |                    **Vision-Language Pre-trained Model**&**Adversarial Transferability**&**Black-Box Attack**                 |
| 24.03 |                                                                                     Microsoft                                                                                     |             arxiv             |                         [Securing Large Language Models: Threats, Vulnerabilities, and Responsible Practices](https://arxiv.org/abs/2403.12503)                          |                                              **Security Risks**&**Vulnerabilities**                                           |
| 24.03 |                                                                            Carnegie Mellon University                                                                             |             arxiv             |                                              [Jailbreaking is Best Solved by Definition](https://arxiv.org/abs/2403.14725)                                               |                                            **Jailbreak Attacks**&**Adaptive Attacks**                                         |
| 24.03 |                                                                              ShanghaiTech University                                                                              |             arxiv             |                        [LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models](https://arxiv.org/abs/2403.16432)                         |                     **Universal Adversarial Triggers**&**Prompt-based Learning**&**Natural Language Attack**                  |
| 24.03 |                                   Huazhong University of Science and Technology, Lehigh University, University of Notre Dame & Duke University                                    |             arxiv             |                                     [Optimization-based Prompt Injection Attack to LLM-as-a-Judge](https://arxiv.org/abs/2403.17710)                                     |                                  **Prompt Injection Attack**&**LLM-as-a-Judge**&**Optimization**                              |
| 24.03 | Washington University in St. Louis, University of Wisconsin - Madison, John Burroughs School |     USENIX Security 2024      |                                                                   [Don‚Äôt Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models](https://arxiv.org/abs/2403.19260)                                                                   |                                                **Jailbreak Prompts**&**Security**                                             |
| 24.04 | University of Pennsylvania, ETH Zurich, EPFL, Sony AI | arxiv | [JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models](https://arxiv.org/abs/2404.01318) | **Jailbreaking Attacks**&**Robustness Benchmark** |
| 24.04 | Microsoft Azure, Microsoft, Microsoft Research | arxiv | [The Crescendo Multi-Turn LLM Jailbreak Attack](https://arxiv.org/abs/2404.01833) | **Jailbreak Attacks**&**Multi-Turn Interaction** |
| 24.04 | EPFL | arxiv | [Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks](https://arxiv.org/abs/2404.02151) | **Adaptive Attacks**&**Jailbreaking** |
| 24.04 | The Ohio State University, University of Wisconsin-Madison | arxiv | [JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks](https://arxiv.org/abs/2404.03027) | **Multimodal Large Language Models**&**Jailbreak Attacks**&**Benchmark** |


## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |        Type        |                                       Title                                       |                                  URL                                  |
|:-----:|:------------------:|:---------------------------------------------------------------------------------:|:---------------------------------------------------------------------:|
| 23.01 |     Community      |                              Reddit/ChatGPTJailbrek                               |           [link](https://www.reddit.com/r/ChatGPTJailbreak)           |
| 23.02 | Resource&Tutorials |                                  Jailbreak Chat                                   |                [link](https://www.jailbreakchat.com/)                 |
| 23.10 |     Tutorials      |                                Awesome-LLM-Safety                                 |         [link](https://github.com/ydyjya/Awesome-LLM-Safety)          |
| 23.10 |      Article       |                 Adversarial Attacks on LLMs(Author: Lilian Weng)                  | [link](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/) |
| 23.11 |       Video        | [1hr Talk] Intro to Large Language Models<br/>From 45:45(Author: Andrej Karpathy) |          [link](https://www.youtube.com/watch?v=zjkBMFhNj_g)          |

## üì∞News & Articles

| Date  |  Type   |            Title            |   Author    |                                  URL                                  |
|:-----:|:-------:|:---------------------------:|:-----------:|:---------------------------------------------------------------------:|
| 23.10 | Article | Adversarial Attacks on LLMs | Lilian Weng | [link](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/) |


## üßë‚Äçüè´Scholars