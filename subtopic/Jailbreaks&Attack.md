# Jailbreaks&Attack

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"

## üìëPapers

| Date  |      Institute      |         Publication          |                                                                         Paper                                                                         |                                        Keywords                                         |
|:-----:|:-------------------:|:----------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------:|
| 22.11 |      AE Studio      | NIPS2022(ML Safety Workshop) |                           [Ignore Previous Prompt: Attack Techniques For Language Models](https://arxiv.org/abs/2211.09527)                           |                           **Prompt Injection**&**Misaligned**                           |
| 23.02 | Saarland University |            arxiv             | [Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/abs/2302.12173) | **Adversarial Prompting**&**Indirect Prompt Injection**&**LLM-Integrated Applications** |
| 23.06 |       Google        |            arxiv             |                                [Are aligned neural networks adversarially aligned?](https://arxiv.org/abs/2306.15447)                                 |                              **Multimodal**&**Jailbreak**                               |
| 23.07 |         CMU         |            arxiv             |                     [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)                     |              **Jailbreak**&**Transferable Attack**&**Adversarial Attack**               |


## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

## üßë‚Äçüè´Scholars