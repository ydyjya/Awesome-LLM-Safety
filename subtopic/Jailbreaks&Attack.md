# Jailbreaks&Attack

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"

## üìëPapers

| Date  |                                                                                          Institute                                                                                           |                                Publication                                |                                                                                                    Paper                                                                                                     |                                                                 Keywords                                                                  |
|:-----:|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------:|
| 20.12 |                                                                                            Google                                                                                            |                           USENIX Security 2021                            |                                  [Extracting Training Data from Large Language Models](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting)                                   |                                              **Verbatim Text Sequences**&**Rank Likelihood**                                              |
| 22.11 |                                                                                          AE Studio                                                                                           |                       NIPS2022(ML Safety Workshop)                        |                                                      [Ignore Previous Prompt: Attack Techniques For Language Models](https://arxiv.org/abs/2211.09527)                                                       |                                                    **Prompt Injection**&**Misaligned**                                                    |
| 23.02 |                                                                                     Saarland University                                                                                      |                                   arxiv                                   |                            [Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/abs/2302.12173)                             |                          **Adversarial Prompting**&**Indirect Prompt Injection**&**LLM-Integrated Applications**                          |
| 23.04 |                                                                        Hong Kong University of Science and Technology                                                                        |                            EMNLP2023(findings)                            |                                                            [Multi-step Jailbreaking Privacy Attacks on ChatGPT](https://arxiv.org/abs/2304.05197)                                                            |                                                        **Privacy**&**Jailbreaks**                                                         |
| 23.04 |                                                                    University of Michigan&Arizona State University&NVIDIA                                                                    |                                 NAACL2024                                 |                                    [ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger](https://arxiv.org/abs/2304.14475)                                     |                              **Textual Backdoor Attack**&**Blackbox Generative Model**&**Trigger Detection**                              |
| 23.05 |                                   Jinan University, Hong Kong University of Science and Technology, Nanyang Technological University, Zhejiang University                                    |                                EMNLP 2023                                 |                                          [Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models](https://arxiv.org/abs/2305.01219)                                          |                                                           **Backdoor Attacks**                                                            |
| 23.05 |                                                        Nanyang Technological University, University of New South Wales, Virginia Tech                                                        |                                   arXiv                                   |                                                     [Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study](https://arxiv.org/abs/2305.13860)                                                      |                                                Large **Jailbreak**&**Prompt Engineering**                                                 |
| 23.06 |                                                                                     Princeton University                                                                                     |                                 AAAI 2024                                 |                                                   [Visual Adversarial Examples Jailbreak Aligned Large Language Models](https://arxiv.org/abs/2306.13213)                                                    |                                    **Visual Language Models**&**Adversarial Attacks**&**AI Alignment**                                    |
| 23.06 |      Nanyang Technological University, University of New South Wales, Huazhong University of Science and Technology, Southern University of Science and Technology, Tianjin University       |                                   arxiv                                   |                                                       [Prompt Injection attack against LLM-integrated Applications](https://arxiv.org/abs/2306.05499)                                                        |                             **&LLM-integrated Applications**&**Security Risks**&**Prompt Injection Attacks**                              |
| 23.06 |                                                                                            Google                                                                                            |                                   arxiv                                   |                                                            [Are aligned neural networks adversarially aligned?](https://arxiv.org/abs/2306.15447)                                                            |                                                       **Multimodal**&**Jailbreak**                                                        |
| 23.07 |                                                                                             CMU                                                                                              |                                   arxiv                                   |                                                [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)                                                 |                                       **Jailbreak**&**Transferable Attack**&**Adversarial Attack**                                        |
| 23.07 |                                                                  Language Technologies Institute Carnegie Mellon University                                                                  |                                   arXiv                                   |                                     [Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success](https://arxiv.org/abs/2307.06865)                                     |                               **Prompt Extraction**&**Attack Success Measurement**&**Defensive Strategies**                               |
| 23.07 |                                                                               Nanyang Technological University                                                                               |                                 NDSS2023                                  |                                               [MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots](https://arxiv.org/abs/2307.08715)                                               |                                      **Jailbreak**&**Reverse-Engineering**&**Automatic Generation**                                       |
| 23.07 |                                                                                         Cornell Tech                                                                                         |                                   arxiv                                   |                                             [Abusing Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs](https://arxiv.org/abs/2307.10490)                                             |                           **Multi-Modal LLMs**&**Indirect Instruction Injection**&**Adversarial Perturbations**                           |
| 23.07 |                                                                         UNC Chapel Hill, Google DeepMind, ETH Zurich                                                                         |                       AdvML Frontiers Workshop 2023                       |                                                      [Backdoor Attacks for In-Context Learning with Language Models](https://arxiv.org/abs/2307.14692)                                                       |                                               **Backdoor Attacks**&**In-Context Learning**                                                |
| 23.07 |                                                                                       Google DeepMind                                                                                        |                                   arXiv                                   |                                             [Large language models (LLMs) are now highly capable at a diverse range of tasks](https://arxiv.org/abs/2307.15008)                                              |                                  **Adversarial Machine Learning**&**AI-Guardian**&**Defense Robustness**                                  |
| 23.08 |                                                                   CISPA Helmholtz Center for Information Security; NetApp                                                                    |                                   arxiv                                   |                                 [‚ÄúDo Anything Now‚Äù: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models](https://arxiv.org/abs/2308.03825)                                  |                                   **Jailbreak Prompts**&**Adversarial Prompts**&**Proactive Detection**                                   |
| 23.09 |                                                                               Ben-Gurion University, DeepKeep                                                                                |                                   arxiv                                   |                                                  [OPEN SESAME! UNIVERSAL BLACK BOX JAILBREAKING OF LARGE LANGUAGE MODELS](https://arxiv.org/abs/2309.01446)                                                  |                                   **Genetic Algorithm**&**Adversarial Prompt**&**Black Box Jailbreak**                                    |
| 23.10 |                                                            Princeton University, Virginia Tech, IBM Research, Stanford University                                                            |                                   arxiv                                   |                                         [FINE-TUNING ALIGNED LANGUAGE MODELS COMPROMISES SAFETY EVEN WHEN USERS DO NOT INTEND TO!](https://arxiv.org/abs/2310.03693)                                         |                                         **Fine-tuning****Safety Risks**&**Adversarial Training**                                          |
| 23.10 |                                                       University of California Santa Barbara, Fudan University, Shanghai AI Laboratory                                                       |                                   arxiv                                   |                                                 [SHADOW ALIGNMENT: THE EASE OF SUBVERTING SAFELY-ALIGNED LANGUAGE MODELS](https://arxiv.org/abs/2310.02949)                                                  |                                              **AI Safety**&**Malicious Use**&**Fine-tuning**                                              |
| 23.10 |                                                                                      Peking University                                                                                       |                                   arxiv                                   |                                           [Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations](https://arxiv.org/abs/2310.06387)                                            |                               **In-Context Learning**&**Adversarial Attacks**&**In-Context Demonstrations**                               |
| 23.10 |                                                                                  University of Pennsylvania                                                                                  |                                   arxiv                                   |                                                      [Jailbreaking Black Box Large Language Models in Twenty Queries](https://arxiv.org/abs/2310.08419)                                                      |                                          **Prompt Automatic Iterative Refinement**&**Jailbreak**                                          |
| 23.10 |                                                                     University of Maryland College Park, Adobe Research                                                                      |                                   arxiv                                   |                                            [AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2310.15140)                                            |                                       **Adversarial Attacks**&**Interpretabilty**&**Jailbreaking**                                        |
| 23.11 |                                                                                            MBZUAI                                                                                            |                                   arxiv                                   |                                           [Robustness Tests for Automatic Machine Translation Metrics with Adversarial Attacks](https://arxiv.org/abs/2311.00508)                                            |                                 **Adversarially-synthesized Texts**&**Word-level Attacks**&**Evaluation**                                 |
| 23.11 |                                                                                      Palisade Research                                                                                       |                                   arxiv                                   |                                                   [BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B](https://arxiv.org/abs/2311.00117)                                                    |                                                       **Remove Safety Fine-tuning**                                                       |
| 23.11 |                                                                                     University of Twente                                                                                     |                                ICNLSP 2023                                |                                                     [Efficient Black-Box Adversarial Attacks on Neural Text Detectors](https://arxiv.org/abs/2311.01873)                                                     |                                               **Misclassification**&**Adversarial attacks**                                               |
| 23.11 |                                                                        PRISM AI&Harmony Intelligenc&Leap Laboratories                                                                        |                                   arxiv                                   |                                        [Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation](https://arxiv.org/abs/2311.03348 )                                        |                                    **Persona-modulation Attacks**&**Jailbreaks**&**Automated Prompt**                                     |
| 23.11 |                                                                                     Tsinghua University                                                                                      |                                   arxiv                                   |                                                 [Jailbreaking Large Vision-Language Models via Typographic Visual Prompts](https://arxiv.org/abs/2311.05608)                                                 |                                       **Typographic Attack**&**Multi-modal**&**Safety Evaluation**                                        |
| 23.11 |                                                              Huazhong University of Science and Technology, Tsinghua University                                                              |                                   arxiv                                   |                               [Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration](https://arxiv.org/abs/2311.06062)                                |                                         **Membership Inference Attacks**&**Privacy and Security**                                         |
| 23.11 |                                                                               Nanjing University, Meituan Inc                                                                                |                                   arxiv                                   |                                  [A Wolf in Sheep‚Äôs Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily](https://arxiv.org/abs/2311.08268)                                  |                                  **Jailbreak Prompts**&**Safety Alignment**&**Safeguard Effectiveness**                                   |
| 23.11 |                                                                                       Google DeepMind                                                                                        |                                   arxiv                                   |                             [Frontier Language Models Are Not Robust to Adversarial Arithmetic or "What Do I Need To Say So You Agree 2+2=5?"](https://arxiv.org/abs/2311.07587)                             |                                  **Adversarial Arithmetic**&**Model Robustness**&**Adversarial Attacks**                                  |
| 23.11 |                                                                     University of Illinois Chicago, Texas A&M University                                                                     |                                   arxiv                                   |                                       [DALA: A Distribution-Aware LoRA-Based Adversarial Attack against Pre-trained Language Models](https://arxiv.org/abs/2311.08598)                                       |                                    **Adversarial Attack**&**Distribution-Aware**&**LoRA-Based Attack**                                    |
| 23.11 |                                                                               Illinois Institute of Technology                                                                               |                                   arxiv                                   |                                 [Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment](https://arxiv.org/abs/2311.09433)                                  |                  Backdoor Activation Attack&Large Language Models&AI Safety&Activation Steering&Trojan Steering Vectors                   |
| 23.11 |                                                                                    Wayne State University                                                                                    |                                   arXiv                                   |                                                   [Hijacking Large Language Models via Adversarial In-Context Learning](https://arxiv.org/abs/2311.09948)                                                    |                             **Adversarial Attacks**&**Gradient-Based Prompt Search**&**Adversarial Suffixes**                             |
| 23.11 |                                        Hong Kong Baptist University, Shanghai Jiao Tong University, Shanghai AI Laboratory, The University of Sydney                                         |                                   arXiv                                   |                                                     [DeepInception: Hypnotize Large Language Model to Be Jailbreaker](https://arxiv.org/abs/2311.03191)                                                      |                                                      **Jailbreak**&**DeepInception**                                                      |
| 23.11 |                                                                             Xi‚Äôan Jiaotong-Liverpool University                                                                              |                                   arxiv                                   |                                               [Generating Valid and Natural Adversarial Examples with Large Language Models](https://arxiv.org/abs/2311.11861)                                               |                                             **Adversarial examples**&**Text classification**                                              |
| 23.11 |                                                                                  Michigan State University                                                                                   |                                   arxiv                                   |                                             [Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems](https://arxiv.org/abs/2311.11796)                                              |                                      **Transferable Attacks**&**AI Systems**&**Adversarial Attacks**                                      |
| 23.11 |                                                                          Tsinghua University & Kuaishou Technology                                                                           |                                   arxiv                                   |                                                       [Evil Geniuses: Delving into the Safety of LLM-based Agents](https://arxiv.org/abs/2311.11855v1)                                                       |                                           **LLM-based Agents**&**Safety**&**Malicious Attacks**                                           |
| 23.11 |                                                                                      Cornell University                                                                                      |                                   arxiv                                   |                                                                         [Language Model Inversion](https://arxiv.org/abs/2311.13647)                                                                         |                                         **Model Inversion**&**Prompt Reconstruction**&**Privacy**                                         |
| 23.11 |                                                                                          ETH Zurich                                                                                          |                                   arxiv                                   |                                                        [Universal Jailbreak Backdoors from Poisoned Human Feedback](https://arxiv.org/abs/2311.14455)                                                        |                                                       **RLHF**&**Backdoor Attacks**                                                       |
| 23.11 |                                                                                UC Santa Cruz, UNC-Chapel Hill                                                                                |                                   arxiv                                   |                                                [How Many Are in This Image? A Safety Evaluation Benchmark for Vision LLMs](https://arxiv.org/abs/2311.16101)                                                 |                             **Vision Large Language Models**&**Safety Evaluation**&A**dversarial Robustness**                             |
| 23.11 |                                                                                    Texas Tech University                                                                                     |                                   arxiv                                   |                                      [Exploiting Large Language Models (LLMs) through Deception Techniques and Persuasion Principles](https://arxiv.org/abs/2311.14876)                                      |                                        **Social Engineering**&**Security**&**Prompt Engineering**                                         |
| 23.11 |                                                                                   Johns Hopkins University                                                                                   |                                   arxiv                                   |                                                      [Instruct2Attack: Language-Guided Semantic Adversarial Attacks](https://arxiv.org/abs/2311.15551)                                                       |                              **Language-guided Attacks**&**Latent Diffusion Models**&**Adversarial Attack**                               |
| 23.11 |                                                       Google DeepMind, University of Washington, Cornell, CMU, UC Berkeley, ETH Zurich                                                       |                                   arxiv                                   |                                                  [Scalable Extraction of Training Data from (Production) Language Models](https://arxiv.org/abs/2311.17035)                                                  |                                  **Extractable Memorization**&**Data Extraction**&**Adversary Attacks**                                   |
| 23.11 |                                         University of Maryland, Mila, Towards AI, Stanford, Technical University of Sofia, University of Milan, NYU                                          |                                   arxiv                                   |                      [Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition](https://arxiv.org/abs/2311.16119)                      |                                                  **Prompt Hacking**&**Security Threats**                                                  |
| 23.11 |                                                     University of Washington, UIUC, Pennsylvania State University, University of Chicago                                                     |                                   arxiv                                   |                                                [IDENTIFYING AND MITIGATING VULNERABILITIES IN LLM-INTEGRATED APPLICATIONS](https://arxiv.org/abs/2311.16153)                                                 |                                            **LLM-Integrated Applications**&**Attack Surfaces**                                            |
| 23.11 |                                            Jinan University, Guangzhou Xuanyuan Research Institute Co. Ltd., The Hong Kong Polytechnic University                                            |                                   arxiv                                   |                                          [TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4](https://arxiv.org/abs/2311.17429)                                          |                                               **Prompt-based Learning**&**Backdoor Attack**                                               |
| 23.11 |                                                                               Nanjing University&Meituan Inc.                                                                                |                                 NAACL2024                                 |                                  [A Wolf in Sheep‚Äôs Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily](https://arxiv.org/abs/2311.08268)                                  |                                      **Jailbreak Prompts**&**LLM Security**&**Automated Framework**                                       |
| 23.11 |                                                                              University of Southern California                                                                               |                            NAACL2024(findings)                            |                                         [Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking](https://arxiv.org/abs/2311.09827)                                          |                                     **Jailbreaking**&**Large Language Models**&**Cognitive Overload**                                     |
| 23.12 |                                                                              The Pennsylvania State University                                                                               |                                   arxiv                                   |                                           [Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections](https://arxiv.org/abs/2312.00027)                                           |                                                **Backdoor Injection**&**Safety Alignment**                                                |
| 23.12 |                                                                                      Drexel University                                                                                       |                                   arXiv                                   |                                        [A Survey on Large Language Model (LLM) Security and Privacy: The Good the Bad and the Ugly](https://arxiv.org/abs/2312.02003)                                        |                                                   **Security**&**Privacy**&**Attacks**                                                    |
| 23.12 |                                                                             Yale University, Robust Intelligence                                                                             |                                   arXiv                                   |                                                        [Tree of Attacks: Jailbreaking Black-Box LLMs Automatically](https://arxiv.org/abs/2312.02119)                                                        |                               **Tree of Attacks with Pruning (TAP)**&**Jailbreaking**&**Prompt Generation**                               |
| 23.12 |                                                                             Independent (Now at Google DeepMind)                                                                             |                                   arXiv                                   |                                                    [Scaling Laws for Adversarial Attacks on Language Model Activations](https://arxiv.org/abs/2312.02780)                                                    |                                  **Adversarial Attacks**&**Language Model Activations**&**Scaling Laws**                                  |
| 23.12 |                                                                                Harbin Institute of Technology                                                                                |                                   arxiv                                   |                                        [Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak](https://arxiv.org/abs/2312.04127)                                        |                               **Jailbreak Attack**&**Inherent Response Tendency**&**Affirmation Tendency**                                |
| 23.12 |                                                                               University of Wisconsin-Madison                                                                                |                                   arxiv                                   |                                    [DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial Natural Language Instructions](https://arxiv.org/abs/2312.04730)                                     |                                       **Code Generation**&**Adversarial Attacks**&**Cybersecurity**                                       |
| 23.12 |                                                                           Carnegie Melon University, IBM Research                                                                            |                                   arxiv                                   |                                            [Forcing Generative Models to Degenerate Ones: The Power of Data Poisoning Attacks](https://arxiv.org/abs/2312.04748)                                             |                               **Data Poisoning Attacks**&**Natural Language Generation**&**Cybersecurity**                                |
| 23.12 |                                                                                      Purdue University                                                                                       |                            NIPS2023ÔºàWorkshopÔºâ                             |                                             [Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs](https://arxiv.org/abs/2312.04782)                                              |                                  **Knowledge Extraction**&**Interrogation Techniques**&**Cybersecurity**                                  |
| 23.12 |                                                                       Sungkyunkwan University, University of Tennessee                                                                       |                                   arXiv                                   |                   [Poisoned ChatGPT Finds Work for Idle Hands: Exploring Developers‚Äô Coding Practices with Insecure Suggestions from Poisoned AI Models](https://arxiv.org/abs/2312.06227)                   |                                              **Poisoning Attacks**&**Software Development**                                               |
| 23.12 |                                                          North Carolina State University, New York University, Stanford University                                                           |                                   arXiv                                   |                          [BEYOND GRADIENT AND PRIORS IN PRIVACY ATTACKS: LEVERAGING POOLER LAYER INPUTS OF LANGUAGE MODELS IN FEDERATED LEARNING](https://arxiv.org/abs/2312.05720)                          |                                                **Federated Learning**&P**rivacy Attacks**                                                 |
| 23.12 |                                                                  Korea Advanced Institute of Science, Graduate School of AI                                                                  |                                   arxiv                                   |                                                              [Hijacking Context in Large Multi-modal Models](https://arxiv.org/abs/2312.07553)                                                               |                                            **Large Multi-modal Models**&**Context Hijacking**                                             |
| 23.12 |                                                 Xi‚Äôan Jiaotong University, Nanyang Technological University, Singapore Management University                                                 |                                   arXiv                                   |                                                  [A Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection](https://arxiv.org/abs/2312.10766)                                                   |                                                **Jailbreaking Detection**&**Multi-Modal**                                                 |
| 23.12 |                                                                    Logistic and Supply Chain MultiTech R&D Centre (LSCM)                                                                     |                                UbiSec-2023                                |                              [A Comprehensive Survey of Attack Techniques Implementation and Mitigation Strategies in Large Language Models](https://arxiv.org/abs/2312.10982)                               |                                             **Cybersecurity Attacks**&**Defense Strategies**                                              |
| 23.12 |                                                                   University of Illinois Urbana-Champaign, VMware Research                                                                   |                                   arXiv                                   |                                                  [BYPASSING THE SAFETY TRAINING OF OPEN-SOURCE LLMS WITH PRIMING ATTACKS](https://arxiv.org/abs/2312.12321)                                                  |                                                  **Safety Training**&**Priming Attacks**                                                  |
| 23.12 |                                                                                Delft University of Technology                                                                                |                                 ICSE 2024                                 |                                                         [Traces of Memorisation in Large Language Models for Code](https://arxiv.org/abs/2312.11658)                                                         |                                             **Code Memorisation**&**Data Extraction Attacks**                                             |
| 23.12 |                                           University of Science and Technology of China, Hong Kong University of Science and Technology, Microsoft                                           |                                   arxiv                                   |                                      [Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models](https://arxiv.org/abs/2312.14197)                                       |                                   **Indirect Prompt Injection Attacks**&**BIPIA Benchmark**&**Defense**                                   |
| 23.12 |                                                                      Nanjing University of Aeronautics and Astronautics                                                                      |                                 NLPCC2023                                 |                                                    [Punctuation Matters! Stealthy Backdoor Attack for Language Models](https://arxiv.org/abs/2312.15867)                                                     |                                            **Backdoor Attack**&**PuncAttack**&**Stealthiness**                                            |
| 23.12 |                                                                   FAR AI, McGill University, MILA, Jagiellonian University                                                                   |                                   arXiv                                   |                                                                       [Exploiting Novel GPT-4 APIs](https://arxiv.org/abs/2312.14302)                                                                        |                                   **Fine-Tuning**&**Knowledge Retrieval**&**Security Vulnerabilities**                                    |
| 23.12 |                                                                                             EPFL                                                                                             |                                                                           |                                                      [Adversarial Attacks on GPT-4 via Simple Random Search](https://www.andriushchenko.me/gpt4adv.pdf)                                                      |                                          **Adversarial Attacks**&**Random Search**&**Jailbreak**                                          |
| 24.01 |                                                                    Logistic and Supply Chain MultiTech R&D Centre (LSCM)                                                                     |                                 CSDE2023                                  |                             [A Novel Evaluation Framework for Assessing Resilience Against Prompt Injection Attacks in Large Language Models](https://arxiv.org/abs/2401.00991)                              |                                          **Evaluation**&**Prompt Injection**&**Cyber Security**                                           |
| 24.01 |                                                                              University of Southern California                                                                               |                                   arxiv                                   |                            [The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance](https://arxiv.org/abs/2401.03729)                            |                                       **Prompt Engineering**&**Text Classification**&**Jailbreaks**                                       |
| 24.01 |                                                           Virginia Tech, Renmin University of China, UC Davis, Stanford University                                                           |                                   arxiv                                   |                                [How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs](https://arxiv.org/abs/ÈìæÊé•ÂæÖÂÆö)                                 |                                      **AI Safety**&**Persuasion Adversarial Prompts**&**Jailbreak**                                       |
| 24.01 |                                                         Anthropic, Redwood Research, Mila Quebec AI Institute, University of Oxford                                                          |                                   arxiv                                   |                                               [SLEEPER AGENTS: TRAINING DECEPTIVE LLMS THAT PERSIST THROUGH SAFETY TRAINING](https://arxiv.org/abs/2401.05566)                                               |                        **Deceptive Behavior**&**Safety Training**&**Backdoored Behavior**&**Adversarial Training**                        |
| 24.01 |                                                Jinan University,Nanyang Technological University, Beijing Institute of Technology, Pazhou Lab                                                |                                   arxiv                                   |                                         [UNIVERSAL VULNERABILITIES IN LARGE LANGUAGE MODELS: IN-CONTEXT LEARNING BACKDOOR ATTACKS](https://arxiv.org/abs/2401.05949)                                         |                                         **In-context Learning**&**Security**&**Backdoor Attacks**                                         |
| 24.01 |                                                                                  Carnegie Mellon University                                                                                  |                                   arxiv                                   |                                                          [Combating Adversarial Attacks with Multi-Agent Debate](https://arxiv.org/abs/2401.05998)                                                           |                                        **Adversarial Attacks**&**Multi-Agent Debate**&**Red Team**                                        |
| 24.01 |                                                                                       Fudan University                                                                                       |                                   arxiv                                   |                                           [Open the Pandora‚Äôs Box of LLMs: Jailbreaking LLMs through Representation Engineering](https://arxiv.org/abs/2401.06824)                                           |                                              **LLM Security**&**Representation Engineering**                                              |
| 24.01 |                                                  Northwestern University, New York University, University of Liverpool, Rutgers University                                                   |                                   arxiv                                   |                                      [AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models](https://arxiv.org/abs/2401.09002)                                       |                                  **Jailbreak Attack**&**Evaluation Frameworks**&**Ground Truth Dataset**                                  |
| 24.01 |                                                                                Kyushu Institute of Technology                                                                                |                                   arxiv                                   |                                                 [ALL IN HOW YOU ASK FOR IT: SIMPLE BLACK-BOX METHOD FOR JAILBREAK ATTACKS](https://arxiv.org/abs/2401.09798)                                                 |                                                **Jailbreak Attacks**&**Black-box Method**                                                 |
| 24.01 |                                                                                             MIT                                                                                              |                                   arXiv                                   |                                       [Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning](https://arxiv.org/abs/2401.10862)                                        |                                                     **Jailbreaking**&**Model Safety**                                                     |
| 24.01 |                                                                                      Aalborg University                                                                                      |                                   arxiv                                   |                                                     [Text Embedding Inversion Attacks on Multilingual Language Models](https://arxiv.org/abs/2401.12192)                                                     |                                 **Text Embedding**&**Inversion Attacks**&**Multilingual Language Models**                                 |
| 24.01 |                                               University of Illinois Urbana-Champaign, University of Washington, Western Washington University                                               |                                   arxiv                                   |                                                 [BADCHAIN: BACKDOOR CHAIN-OF-THOUGHT PROMPTING FOR LARGE LANGUAGE MODELS](https://arxiv.org/abs/2401.12242)                                                  |                                            **Chain-of-Thought Prompting**&**Backdoor Attacks**                                            |
| 24.01 |                                                                       The University of Hong Kong, Zhejiang University                                                                       |                                   arxiv                                   |                                                                    [Red Teaming Visual Language Models](https://arxiv.org/abs/2401.12915)                                                                    |                                                **Vision-Language Models**&**Red Teaming**                                                 |
| 24.01 |                                                   University of California Santa Barbara,Sea AI Lab Singapore, Carnegie Mellon University                                                    |                                   arxiv                                   |                                                           [Weak-to-Strong Jailbreaking on Large Language Models](https://arxiv.org/abs/2401.17256)                                                           |                                          **Jailbreaking**&**Adversarial Prompts**&**AI Safety**                                           |
| 24.02 |                                                                                      Boston University                                                                                       |                                   arxiv                                   |                                                 [Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks](https://arxiv.org/abs/2402.00626)                                                  |                            **Large Vision-Language Models**&**Typographic Attacks**&**Self-Generated Attacks**                            |
| 24.02 |                                                                        Copenhagen Business School, Temple University                                                                         |                                   arxiv                                   |                                               [An Early Categorization of Prompt Injection Attacks on Large Language Models](https://arxiv.org/abs/2402.00898)                                               |                                                  **Prompt Injection**&**Categorization**                                                  |
| 24.02 |                                                        Michigan State University, Okinawa Institute of Science and Technology (OIST)                                                         |                                   arxiv                                   |                                                                  [Data Poisoning for In-context Learning](https://arxiv.org/abs/2402.02160)                                                                  |                                          **In-context learning**&**Data poisoning**&**Security**                                          |
| 24.02 |                                                                       CISPA Helmholtz Center for Information Security                                                                        |                                   arxiv                                   |                                                          [Conversation Reconstruction Attack Against GPT Models](https://arxiv.org/abs/2402.02987)                                                           |                                   **Conversation Reconstruction Attack**&**Privacy risks**&**Security**                                   |
| 24.02 |                                      University of Illinois Urbana-Champaign, Center for AI Safety, Carnegie Mellon University, UC Berkeley, Microsoft                                       |                                   arxiv                                   |                                       [HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal](https://arxiv.org/abs/2402.04249)                                        |                                               **Automated Red Teaming**&**Robust Refusal**                                                |
| 24.02 |                                                University of Washington, University of Virginia, Allen Institute for Artificial Intelligence                                                 |                                   arxiv                                   |                                                      [Do Membership Inference Attacks Work on Large Language Models?](https://arxiv.org/abs/2402.07841)                                                      |                                         **Membership Inference Attacks**&**Privacy**&**Security**                                         |
| 24.02 |                                                      Pennsylvania State University, Wuhan University, Illinois Institute of Technology                                                       |                                   arxiv                                   |                                   [PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2402.07867)                                    |                                    **Knowledge Poisoning Attacks**&**Retrieval-Augmented Generation**                                     |
| 24.02 |                                                                  Purdue University, University of Massachusetts at Amherst                                                                   |                                   arxiv                                   |                                          [RAPID OPTIMIZATION FOR JAILBREAKING LLMS VIA SUBCONSCIOUS EXPLOITATION AND ECHOPRAXIA](https://arxiv.org/abs/2402.05467)                                           |                                                   **Jailbreaking LLM**&**Optimization**                                                   |
| 24.02 |                                                                       CISPA Helmholtz Center for Information Security                                                                        |                                   arxiv                                   |                                                        [Comprehensive Assessment of Jailbreak Attacks Against LLMs](https://arxiv.org/abs/2402.05668)                                                        |                                       **Jailbreak Attacks**&**Attack Methods**&**Policy Alignment**                                       |
| 24.02 |                                                                                         UC Berkeley                                                                                          |                                   arxiv                                   |                                                    [StruQ: Defending Against Prompt Injection with Structured Queries](https://arxiv.org/abs/2402.06363)                                                     |                                **Prompt Injection Attacks**&**Structured Queries**&**Defense Mechanisms**                                 |
| 24.02 |                                        Nanyang Technological University, Huazhong University of Science and Technology, University of New South Wales                                        |                                   arxiv                                   |                                                   [PANDORA: Jailbreak GPTs by Retrieval Augmented Generation Poisoning](https://arxiv.org/abs/2402.08416)                                                    |                                      **Jailbreak Attacks**&**Retrieval Augmented Generation (RAG)**                                       |
| 24.02 |                                                                               Sea AI Lab, Southern University                                                                                |                                   arxiv                                   |                                                      [Test-Time Backdoor Attacks on Multimodal Large Language Models](https://arxiv.org/abs/2402.08577)                                                      |                       **Backdoor Attacks**&**Multimodal Large Language Models (MLLMs)**&**Adversarial Test Images**                       |
| 24.02 |                                           University of Illinois at Urbana‚ÄìChampaign, University of California, San Diego, Allen Institute for AI                                            |                                   arxiv                                   |                                                   [COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability](https://arxiv.org/abs/2402.08679)                                                   |                                             **Jailbreaks**&**Controllable Attack Generation**                                             |
| 24.02 |                                                                                          ISCAS, NTU                                                                                          |                                   arxiv                                   |                                                [Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues](https://arxiv.org/abs/2402.09091)                                                |                                           **Jailbreak Attacks**&**Indirect Attack**&**Puzzler**                                           |
| 24.02 |                                                          √âcole Polytechnique F√©d√©rale de Lausanne, University of Wisconsin-Madison                                                           |                                   arxiv                                   |                                             [Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks](https://arxiv.org/abs/2402.09177)                                             |                             **Jailbreaking Attacks**&**Contextual Interaction**&**Multi-Round Interactions**                              |
| 24.02 |                                      University of Electronic Science and Technology of China, CISPA Helmholtz Center for Information Security, NetApp                                       |                                   arxiv                                   |                                           [Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization](https://arxiv.org/abs/2402.09179)                                            |                                        **Customization**&**Instruction Backdoor Attacks**&**GPTs**                                        |
| 24.02 |                                                                         Shanghai Artificial Intelligence Laboratory                                                                          |                                   arxiv                                   |                                                 [Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey](https://arxiv.org/abs/2402.09283)                                                  |                                           **LLM Conversation Safety**&**Attacks**&**Defenses**                                            |
| 24.02 |                                                                               UC Berkeley, New York University                                                                               |                                   arxiv                                   |                                                       [PAL: Proxy-Guided Black-Box Attack on Large Language Models](https://arxiv.org/abs/2402.09674)                                                        |                                           **Black-Box Attack**&**Proxy-Guided Attack**&**PAL**                                            |
| 24.02 |                                                                         Center for Human-Compatible AI, UC Berkeley                                                                          |                                   arxiv                                   |                                                                   [A STRONGREJECT for Empty Jailbreaks](https://arxiv.org/abs/2402.10260)                                                                    |                                             **Jailbreaks**&**Benchmarking**&**StrongREJECT**                                              |
| 24.02 |                                                                                   Arizona State University                                                                                   |                                   arxiv                                   |                                              [Jailbreaking Proprietary Large Language Models using Word Substitution Cipher](https://arxiv.org/abs/2402.10601)                                               |                                    **Jailbreak**&**Word Substitution Cipher**&**Attack Success Rate**                                     |
| 24.02 |                                                              Renmin University of China, Beijing, Peking University, WeChat AI                                                               |                                   arxiv                                   |                                              [Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents](https://arxiv.org/abs/2402.11208)                                               |                                            **Backdoor Attacks**&**Agent Safety**&**Framework**                                            |
| 24.02 |                                                     University of Washington, UIUC, Western Washington University, University of Chicago                                                     |                                   arxiv                                   |                                                    [ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs](https://arxiv.org/abs/2402.11753)                                                     |                                         **ASCII Art**&**Jailbreak Attacks**&**Safety Alignment**                                          |
| 24.02 |           Jinan University, Nanyang Technological University, Zhejiang University, Hong Kong University of Science and Technology, Beijing Institute of Technology, Sony Research            |                                   arxiv                                   |                                         [Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.12168)                                          |     **Weight-Poisoning Backdoor Attacks**&**Parameter-Efficient Fine-Tuning (PEFT)**&**Poisoned Sample Identification Module (PSIM)**     |
| 24.02 |                                                                       CISPA Helmholtz Center for Information Security                                                                        |                                   arxiv                                   |                                                          [Prompt Stealing Attacks Against Large Language Models](https://arxiv.org/abs/2402.12959)                                                           |                                                    **Prompt Engineering**&**Security**                                                    |
| 24.02 |                              University of New South Wales Australia, Delft University of Technology The Netherlands&Nanyang Technological University Singapore                              |                                   arxiv                                   |                                                  [LLM Jailbreak Attack versus Defense Techniques - A Comprehensive Study](https://arxiv.org/abs/2402.13457)                                                  |                                               **Jailbreak Attacks**&**Defense Techniques**                                                |
| 24.02 |                                                                     Wayne State University, University of Michigan-Flint                                                                     |                                   arxiv                                   |                                                    [Learning to Poison Large Language Models During Instruction Tuning](https://arxiv.org/abs/2402.13459)                                                    |                                                  **Data Poisoning**&**Backdoor Attacks**                                                  |
| 24.02 |                                                  Nanyang Technological University, Zhejiang University, The Chinese University of Hong Kong                                                  |                                   arxiv                                   |                                              [Backdoor Attacks on Dense Passage Retrievers for Disseminating Misinformation](https://arxiv.org/abs/2402.13532)                                               |                                    **Dense Passage Retrieval**&**Backdoor Attacks**&**Misinformation**                                    |
| 24.02 |                                                                                    University of Michigan                                                                                    |                                   arxiv                                   |                                           [PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails](https://arxiv.org/abs/2402.15911)                                            |                                            **Universal Adversarial Prefixes**&**Guard Models**                                            |
| 24.02 |                                                                                             Meta                                                                                             |                                   arxiv                                   |                                                  [Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts](https://arxiv.org/abs/2402.16822)                                                   |                                         **Adversarial Prompts**&**Quality-Diversity**&**Safety**                                          |
| 24.02 |                                                                                       Fudan University                                                                                       |                                   arxiv                                   |                                         [CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models](https://arxiv.org/abs/2402.16717)                                          |                                             **Personalized Encryption**&**Safety Mechanisms**                                             |
| 24.02 |                                                                                  Carnegie Mellon University                                                                                  |                                   arxiv                                   |                                                          [Attacking LLM Watermarks by Exploiting Their Strengths](https://arxiv.org/abs/2402.16187)                                                          |                                                **LLM Watermarks**&**Adversarial Attacks**                                                 |
| 24.02 |                                                                                      Beihang University                                                                                      |                                   arxiv                                   |                       [From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings](https://arxiv.org/abs/2402.16006)                        |                                           **Adversarial Suffix**&**Text Embedding Translation**                                           |
| 24.02 |                                                                             University of Maryland College Park                                                                              |                                   arxiv                                   |                                                      [Fast Adversarial Attacks on Language Models In One GPU Minute](https://arxiv.org/abs/2402.15570)                                                       |                                      **Adversarial Attacks**&**BEAST**&**Computational Efficiency**                                       |
| 24.02 |                                                                         Shanghai Artificial Intelligence Laboratory                                                                          |                                   arxiv                                   |                                                  [Attacks Defenses and Evaluations for LLM Conversation Safety: A Survey](https://arxiv.org/abs/2402.09283)                                                  |                                                    **Conversation Safety**&**Survey**                                                     |
| 24.02 |                                                          Beijing University of Posts and Telecommunications, University of Michigan                                                          |                                   arxiv                                   |                                         [Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue](https://arxiv.org/abs/2402.17262)                                          |                                             **Multi-turn Dialogue**&**Safety Vulnerability**                                              |
| 24.02 |                                             University of California, The Hongkong University of Science and Technology, University of Maryland                                              |                                   arxiv                                   |                                            [DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers](https://arxiv.org/abs/2402.16914)                                             |                                             **Jailbreaking Attacks**&**Prompt Decomposition**                                             |
| 24.02 |                                                                 Massachusetts Institute of Technology, MIT-IBM Watson AI Lab                                                                 |                                   arxiv                                   |                                                          [CURIOSITY-DRIVEN RED-TEAMING FOR LARGE LANGUAGE MODELS](https://arxiv.org/abs/2402.19464)                                                          |                                             **Curiosity-Driven Exploration**&**Red Teaming**                                              |
| 24.02 |            SKLOIS Institute of Information Engineering Chinese Academy of Science, School of Cyber Security University of Chinese Academy of Sciences,Tsinghua University,RealAI             |                                   arxiv                                   |                              [Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction](https://arxiv.org/abs/2402.18104)                               |                                    **Jailbreaking**&**Large Language Models**&**Adversarial Attacks**                                     |
| 24.03 |                                                                         Rice University, Samsung Electronics America                                                                         |                                   arxiv                                   |                                                 [LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario](https://arxiv.org/abs/2403.00108)                                                 |                                  **Low-Rank Adaptation (LoRA)**&**Backdoor Attacks**&**Model Security**                                   |
| 24.03 |                                                                                 The University of Hong Kong                                                                                  |                                   arxiv                                   |                                                      [ImgTrojan: Jailbreaking Vision-Language Models with ONE Image](https://arxiv.org/abs/2403.02910)                                                       |                                   **Vision-Language Models**&**Data Poisoning**&**Jailbreaking Attack**                                   |
| 24.03 |                                                                                       SPRING Lab EPFL                                                                                        |                                   arxiv                                   |                                        [Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks](https://arxiv.org/abs/2403.03792)                                         |                                 **Prompt Injection Attacks**&**Optimization-Based Approach**&**Security**                                 |
| 24.03 |                                                 Shanghai University of Finance and Economics, Southern University of Science and Technology                                                  |                                   arxiv                                   |                                                  [Tastle: Distract Large Language Models for Automatic Jailbreak Attack](https://arxiv.org/abs/2403.08424)                                                   |                                               **Jailbreak Attack**&**Black-box Framework**                                                |
| 24.03 |                                                       Google DeepMind, ETH Zurich, University of Washington, OpenAI, McGill University                                                       |                                   arxiv                                   |                                                               [Stealing Part of a Production Language Model](https://arxiv.org/abs/2403.06634)                                                               |                                            **Model Stealing**&**Language Models**&**Security**                                            |
| 24.03 |                                                                                   University of Edinburgh                                                                                    |                                   arxiv                                   |                                    [Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks](https://arxiv.org/abs/2403.09832)                                     |                                 **Prompt Injection Attacks**&**Machine Translation**&**Inverse Scaling**                                  |
| 24.03 |                                                                               Nanyang Technological University                                                                               |                                   arxiv                                   |                                                       [BADEDIT: BACKDOORING LARGE LANGUAGE MODELS BY MODEL EDITING](https://arxiv.org/abs/2403.13355)                                                        |                                            **Backdoor Attacks**&**Model Editing**&**Security**                                            |
| 24.03 |                                                                           Fudan University, Shanghai AI Laboratory                                                                           |                                   arxiv                                   |                                                [EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models](https://arxiv.org/abs/2403.12171)                                                 |                                             **Jailbreak Attacks**&**Security**&**Framework**                                              |
| 24.03 |                               Shanghai Key Lab of Intelligent Information Processing, Fudan University, Shanghai Engineering Research Center of AI & Robotics                                |                                   arxiv                                   |                        [Improving Adversarial Transferability of Visual-Language Pre-training Models through Collaborative Multimodal Interaction](https://arxiv.org/abs/2403.10883)                         |                        **Vision-Language Pre-trained Model**&**Adversarial Transferability**&**Black-Box Attack**                         |
| 24.03 |                                                                                          Microsoft                                                                                           |                                   arxiv                                   |                                           [Securing Large Language Models: Threats, Vulnerabilities, and Responsible Practices](https://arxiv.org/abs/2403.12503)                                            |                                                  **Security Risks**&**Vulnerabilities**                                                   |
| 24.03 |                                                                                  Carnegie Mellon University                                                                                  |                                   arxiv                                   |                                                                [Jailbreaking is Best Solved by Definition](https://arxiv.org/abs/2403.14725)                                                                 |                                                **Jailbreak Attacks**&**Adaptive Attacks**                                                 |
| 24.03 |                                                                                   ShanghaiTech University                                                                                    |                                   arxiv                                   |                                          [LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models](https://arxiv.org/abs/2403.16432)                                           |                         **Universal Adversarial Triggers**&**Prompt-based Learning**&**Natural Language Attack**                          |
| 24.03 |                                         Huazhong University of Science and Technology, Lehigh University, University of Notre Dame & Duke University                                         |                                   arxiv                                   |                                                       [Optimization-based Prompt Injection Attack to LLM-as-a-Judge](https://arxiv.org/abs/2403.17710)                                                       |                                      **Prompt Injection Attack**&**LLM-as-a-Judge**&**Optimization**                                      |
| 24.03 |                                                 Washington University in St. Louis, University of Wisconsin - Madison, John Burroughs School                                                 |                           USENIX Security 2024                            |                                        [Don‚Äôt Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models](https://arxiv.org/abs/2403.19260)                                        |                                                    **Jailbreak Prompts**&**Security**                                                     |
| 24.03 |                                                            School of Information Science and Technology, ShanghaiTech University                                                             |                                 NAACL2024                                 |                                          [LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models](https://arxiv.org/abs/2403.16432)                                           |                     **Prompt-based Language Models**&**Universal Adversarial Triggers**&**Natural Language Attacks**                      |
| 24.04 |                                                                    University of Pennsylvania, ETH Zurich, EPFL, Sony AI                                                                     |                                   arxiv                                   |                                           [JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models](https://arxiv.org/abs/2404.01318)                                            |                                             **Jailbreaking Attacks**&**Robustness Benchmark**                                             |
| 24.04 |                                                                        Microsoft Azure, Microsoft, Microsoft Research                                                                        |                                   arxiv                                   |                                                              [The Crescendo Multi-Turn LLM Jailbreak Attack](https://arxiv.org/abs/2404.01833)                                                               |                                             **Jailbreak Attacks**&**Multi-Turn Interaction**                                              |
| 24.04 |                                                                                             EPFL                                                                                             |                                   arxiv                                   |                                                  [Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks](https://arxiv.org/abs/2404.02151)                                                   |                                                   **Adaptive Attacks**&**Jailbreaking**                                                   |
| 24.04 |                                                                  The Ohio State University, University of Wisconsin-Madison                                                                  |                                   arxiv                                   |                          [JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks](https://arxiv.org/abs/2404.03027)                          |                                 **Multimodal Large Language Models**&**Jailbreak Attacks**&**Benchmark**                                  |
| 24.04 |                                                                                          Enkrypt AI                                                                                          |                                   arxiv                                   |                                                     [INCREASED LLM VULNERABILITIES FROM FINE-TUNING AND QUANTIZATION](https://arxiv.org/abs/2404.04392)                                                      |                                         **Fine-tuning**&**Quantization**&**LLM Vulnerabilities**                                          |
| 24.04 |                                                                The Pennsylvania State University, Carnegie Mellon University                                                                 |                                   arxiv                                   |                             [Hidden You Malicious Goal Into Benigh Narratives: Jailbreak Large Language Models through Logic Chain Injection](https://arxiv.org/abs/2404.04849)                              |                                                **LLM**&**Jailbreak**&**Prompt Injection**                                                 |
| 24.04 |                                                                      Technical University of Darmstadt, Google Research                                                                      |                                   arxiv                                   |                                                   [Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data](https://arxiv.org/abs/2404.05530)                                                    |                  **Reinforcement Learning from Human Feedback**&**Poisoned Preference Data**&**Language Model Security**                  |
| 24.04 |                                                                                      Purdue University                                                                                       |                                   arxiv                                   |                                                           [Rethinking How to Evaluate Language Model Jailbreak](https://arxiv.org/abs/2404.06407)                                                            |                                                   **Jailbreak**&**Evaluation Metrics**                                                    |
| 24.04 |                                                       Xi‚Äôan Jiaotong-Liverpool University, Rutgers University, University of Liverpool                                                       |                                   arxiv                                   |                                                 [Goal-guided Generative Prompt Injection Attack on Large Language Models](https://arxiv.org/abs/2404.07234)                                                  |                                       **Prompt Injection**&**Robustness**&**Mahalanobis Distance**                                        |
| 24.04 |                                                                                   University of New Haven                                                                                    |                                   arxiv                                   |                                                     [SANDWICH ATTACK: MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS](https://arxiv.org/abs/2404.07242)                                                      |                                      **Multi-language Mixture**&**Adaptive Attack**&**LLM Security**                                      |
| 24.04 |                                                                                  The Ohio State University                                                                                   |                                   arxiv                                   |                   [AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs](https://arxiv.org/abs/2404.07921)                    |                                                     **Adversarial Suffix Generation**                                                     |
| 24.04 |                                                                        Renmin University of China, Microsoft Research                                                                        |                                   arxiv                                   |                                              [Uncovering Safety Risks in Open-source LLMs through Concept Activation Vector](https://arxiv.org/abs/2404.12038)                                               |                                                       **Safety**&**Attack Methods**                                                       |
| 24.04 |                                                                                     Zhejiang University                                                                                      |                                   arxiv                                   |                                            [JailbreakLens: Visual Analysis of Jailbreak Attacks Against Large Language Models](https://arxiv.org/abs/2404.08793)                                             |                                                **Jailbreak Attacks**&**Visual Analytics**                                                 |
| 24.04 |                                                                                   Shanghaitech University                                                                                    |                                   arxiv                                   |                                                          [Don‚Äôt Say No: Jailbreaking LLM by Suppressing Refusal](https://arxiv.org/abs/2404.16369)                                                           |                                             **Jailbreaking Attacks**&**Adversarial Attacks**                                              |
| 24.04 |                                                 University of Electronic Science and Technology of China,  Chengdu University of Technology                                                  |                                   arxiv                                   |                                                     [TALK TOO MUCH: Poisoning Large Language Models under Token Limit](https://arxiv.org/abs/2404.14795)                                                     |                                                 **Token Limitation**&**Poisoning Attack**                                                 |
| 24.04 |                                                           ETH Zurich,  EPFL, University of Twente, Georgia Institute of Technology                                                           |                                   arxiv                                   |                                                [Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs](https://arxiv.org/abs/2404.14461)                                                 |                                 **Aligned LLMs**&**Universal Jailbreak Backdoors**&**Poisoning Attacks**                                  |
| 24.04 |                                                                                             N/A                                                                                              |                                   arxiv                                   |                                         [Trojan Detection in Large Language Models: Insights from The Trojan Detection Challenge](https://arxiv.org/abs/2404.13660)                                          |                                                 **Trojan Detection**&**Model Robustness**                                                 |
| 24.04 |                                                                                Shanghai Jiao Tong University                                                                                 |                                   arxiv                                   |                                            [Physical Backdoor Attack can Jeopardize Driving with Vision-Large-Language Models](https://arxiv.org/abs/2404.12916)                                             |                                   **Vision-Large-Language Models**&**Autonomous Driving**&**Security**                                    |
| 24.04 |                                                               Max-Planck-Institute for Intelligent Systems, AI at Meta (FAIR)                                                                |                                   arxiv                                   |                                                        [AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs](https://arxiv.org/abs/2404.16873)                                                         |                                                **Adversarial Prompting**&**Safety in AI**                                                 |
| 24.04 |                                                Singapore Management University, Shanghai Institute for Advanced Study of Zhejiang University                                                 |                                   arxiv                                   |                                               [Evaluating and Mitigating Linguistic Discrimination in Large Language Models](https://arxiv.org/abs/2404.18534)                                               |                                         =**Linguistic Discrimination**&**Jailbreak**&**Defense**                                          |
| 24.04 |                                   University of Louisiana at Lafayette, Beijing Electronic Science and Technology Institute, The Johns Hopkins University                                    |                                   arxiv                                   |                                                  [Assessing Cybersecurity Vulnerabilities in Code Large Language Models](https://arxiv.org/abs/2404.18567)                                                   |                                                               **Code LLMs**                                                               |
| 24.04 |                                            University College London, The University of Melbourne, Macquarie University, University of Edinburgh                                             |                                   arxiv                                   |                                 [Transferring Troubles: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction Tuning](https://arxiv.org/abs/2404.19597)                                 |                               **Cross-Lingual Transferability**&**Backdoor Attacks**&**Instruction Tuning**                               |
| 24.04 |                           University of Cambridge, Indian Institute of Technology Bombay, University of Melbourne, University College London, Macquarie University                           |                            ICLR 2024 Workshop                             |                                                           [Attacks on Third-Party APIs of Large Language Models](https://arxiv.org/abs/2404.16891)                                                           |                                                     **Third-Party API**&**Security**                                                      |
| 24.04 |                                                                                Purdue University, Fort Wayne                                                                                 |                                 NAACL2024                                 |                                                   [Vert Attack: Taking advantage of Text Classifiers‚Äô horizontal vision](https://arxiv.org/abs/2404.08538)                                                   |                                        **Text Classifiers**&**Adversarial Attacks**&**VertAttack**                                        |
| 24.04 |                                                          The University of Melbourne&Macquarie University&University College London                                                          |                                 NAACL2024                                 |                                                           [Backdoor Attacks on Multilingual Machine Translation](https://arxiv.org/abs/2404.02393)                                                           |                                  **Multilingual Machine Translation**&**Security**&**Backdoor Attacks**                                   |
| 24.05 |                                                              Institute of Information Engineering, Chinese Academy of Sciences                                                               |                                   arxiv                                   |                                  [Can LLMs Deeply Detect Complex Malicious Queries? A Framework for Jailbreaking via Obfuscating Intent](https://arxiv.org/abs/2405.03654)                                   |                                       **Prompt Jailbreak Attack**&**Red Team**&**Black-box Attack**                                       |
| 24.05 |                                                                                University of Texas at Austin                                                                                 |                                   arxiv                                   |                                                          [Mitigating Exaggerated Safety in Large Language Models](https://arxiv.org/abs/2405.05418)                                                          |                                            **Model Safety**&**Utility**&**Exaggerated Safety**                                            |
| 24.05 |                                                              Institute of Information Engineering, Chinese Academy of Sciences                                                               |                                   arxiv                                   |                                                [Chain of Attack: A Semantic-Driven Contextual Multi-Turn Attacker for LLM](https://arxiv.org/abs/2405.05610)                                                 |                           **Multi-Turn Dialogue Attack**&**LLM Security**&**Semantic-Driven Contextual Attack**                           |
| 24.05 |                                                                                      Peking University                                                                                       |                            ICLR 2024 Workshop                             |                                                                 [BOOSTING JAILBREAK ATTACK WITH MOMENTUM](https://arxiv.org/abs/2405.01229)                                                                  |                                                 **Jailbreak Attack**&**Momentum Method**                                                  |
| 24.05 |                                                                           √âcole Polytechnique F√©d√©rale de Lausanne                                                                           |                                 ICML 2024                                 |                                                              [Revisiting character-level adversarial attacks](https://arxiv.org/abs/2405.04346)                                                              |                                           **Character-level Adversarial Attack**&**Robustness**                                           |
| 24.05 |                                                                                   Johns Hopkins University                                                                                   |                                 CCS 2024                                  |                                                 [PLeak: Prompt Leaking Attacks against Large Language Model Applications](https://arxiv.org/abs/2405.06823)                                                  |                                            **Prompt Leaking Attacks**&**Adversarial Queries**                                             |
| 24.05 |                                                                                 IT University of Copenhagen                                                                                  |                                   arxiv                                   |                                                              [Hacc-Man: An Arcade Game for Jailbreaking LLMs](https://arxiv.org/abs/2405.15902)                                                              |                                               **creative problem solving**&**jailbreaking**                                               |
| 24.05 |                                                                             The Hong Kong Polytechnic University                                                                             |                                   arxiv                                   |                                                [No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning Attacks](https://arxiv.org/abs/2405.16229)                                                 |                               **Fine-tuning Attacks**&**LLM Safeguarding**&**Mechanistic Interpretability**                               |
| 24.05 |                                                                                            KAIST                                                                                             |                                   arxiv                                   |                                                    [Automatic Jailbreaking of the Text-to-Image Generative AI Systems](https://arxiv.org/abs/2405.16567)                                                     |                                           **Jailbreaking**&**Text-to-Image**&**Generative AI**                                            |
| 24.05 |                                                                                       Fudan University                                                                                       |                                   arxiv                                   |                                                   [White-box Multimodal Jailbreaks Against Large Vision-Language Models](https://arxiv.org/abs/2405.17894)                                                   |                                 **Fine-tuning Attacks**&**Multimodal Models**&**Adversarial Robustness**                                  |
| 24.05 |                                                                               Singapore Management University                                                                                |                                   arxiv                                   |                                           [Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing](https://arxiv.org/abs/2405.18166)                                           |                                   **Jailbreak Attacks**&**Layer-specific Editing**&**LLM Safeguarding**                                   |
| 24.05 |                                                                                  Mila ‚Äì Qu√©bec AI Institute                                                                                  |                                   arxiv                                   |                                        [Learning Diverse Attacks on Large Language Models for Robust Red-Teaming and Safety Tuning](https://arxiv.org/abs/2405.18540)                                        |                                        **Red-Teaming**&**Safety Tuning**&**GFlowNet Fine-tuning**                                         |
| 24.05 |                                                                       CISPA Helmholtz Center for Information Security                                                                        |                                   arxiv                                   |                                                                  [Voice Jailbreak Attacks Against GPT-4o](https://arxiv.org/abs/2405.19103)                                                                  |                                              **Jailbreak Attacks**&**Voice Mode**&**GPT-4o**                                              |
| 24.05 |                                                                               Nanyang Technological University                                                                               |                                   arxiv                                   |                                               [ART: Automatic Red-teaming for Text-to-Image Models to Protect Benign Users](https://arxiv.org/abs/2405.19360)                                                |                                     **Red-Teaming**&**Text-to-Image Models**&**Generative AI Safety**                                     |
| 24.05 |                                                                                      Xidian University                                                                                       |                                   arxiv                                   |                                                        [Efficient LLM-Jailbreaking by Introducing Visual Modality](https://arxiv.org/abs/2405.20015)                                                         |                                        **Jailbreaking**&**Multimodal Models**&**Visual Modality**                                         |
| 24.05 |                                                              Institute of Information Engineering, Chinese Academy of Sciences                                                               |                                   arxiv                                   |                                                            [Context Injection Attacks on Large Language Models](https://arxiv.org/abs/2405.20234)                                                            |                                           **Context Injection Attacks**&**Misleading Context**                                            |
| 24.05 |                                                                          University of Illinois at Urbana-Champaign                                                                          |                                   arxiv                                   |                                          [Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters](https://arxiv.org/abs/2405.20413)                                          |                                       **Jailbreak**&**Moderation Guardrails**&**Cipher Characters**                                       |
| 24.05 |                                                                                   Northeastern University                                                                                    |                                   arxiv                                   |                                               [Phantom: General Trigger Attacks on Retrieval Augmented Language Generation](https://arxiv.org/abs/2405.20485)                                                |                                   **Trigger Attacks**&**Retrieval Augmented Generation**&**Poisoning**                                    |
| 24.05 |                                                                                   Northwestern University                                                                                    |                                   arxiv                                   |                                              [Enhancing Jailbreak Attack Against Large Language Models through Silent Tokens](https://arxiv.org/abs/2405.20653)                                              |                                                  **Jailbreak Attack**&**Silent Tokens**                                                   |
| 24.05 |                                                                                      Peking University                                                                                       |                                   arxiv                                   |                             [Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character](https://arxiv.org/abs/2405.20773)                             |                                **Jailbreak Attack**&**MultiModal Large Language Models**&**Role-playing**                                 |
| 24.05 |                                                                                   Northwestern University                                                                                    |                                   arxiv                                   |                                              [Exploring Backdoor Attacks against Large Language Model-based Decision Making](https://arxiv.org/abs/2405.20774)                                               |                                                 **Backdoor Attacks**&**Decision Making**                                                  |
| 24.05 |                                                                                      Beihang University                                                                                      |                                   arxiv                                   |                                       [Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models](https://arxiv.org/abs/2405.20775)                                        |                                  **Jailbreak**&**Multimodal Large Language Models**&**Medical Contexts**                                  |
| 24.05 |                                                                                Harbin Institute of Technology                                                                                |                                   arxiv                                   |                                                 [Improved Generation of Adversarial Examples Against Safety-aligned LLMs](https://arxiv.org/abs/2405.20778)                                                  |                                **Adversarial Examples**&**Safety-aligned LLMs**&**Gradient-based Methods**                                |
| 24.05 |                                                                               Nanyang Technological University                                                                               |                                   arxiv                                   |                                             [Improved Techniques for Optimization-Based Jailbreaking on Large Language Models](https://arxiv.org/abs/2405.21018)                                             |                                               **Jailbreaking**&**Optimization Techniques**                                                |
| 24.06 |                                                                                University of Central Florida                                                                                 |                                   arxiv                                   |                                      [BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models](https://arxiv.org/abs/2406.00083)                                      |                                         **Retrieval-Augmented Generation**&**Poisoning Attacks**                                          |
| 24.06 |                                                                                        Zscaler, Inc.                                                                                         |                                   arxiv                                   |                                               [Exploring Vulnerabilities and Protections in Large Language Models: A Survey](https://arxiv.org/abs/2406.00240)                                               |                                           **Prompt Hacking**&**Adversarial Attacks**&**Suvery**                                           |
| 24.06 |                                                                               Singapore Management University                                                                                |                                   arxiv                                   |                                         [Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses](https://arxiv.org/abs/2406.01288)                                         |                               **Few-Shot Jailbreaking**&**Aligned Language Models**&**Adversarial Attacks**                               |
| 24.06 |                                                                                   Capgemini Invent, Paris                                                                                    |                                   arxiv                                   |                                                       [QROA: A Black-Box Query-Response Optimization Attack on LLMs](https://arxiv.org/abs/2406.02044)                                                       |                                           **Query-Response Optimization Attack**&**Black-Box**                                            |
| 24.06 |                                                                        Huazhong University of Science and Technology                                                                         |                                   arxiv                                   |                                            [AutoJailbreak: Exploring Jailbreak Attacks and Defenses through a Dependency Lens](https://arxiv.org/abs/2406.03805)                                             |                                               **Jailbreak Attacks**&**Dependency Analysis**                                               |
| 24.06 |                                                                                      Beihang University                                                                                      |                                   arxiv                                   |                                                     [Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt](https://arxiv.org/abs/2406.04031)                                                     |                             **Jailbreak Attacks**&**Vision Language Models**&**Bi-Modal Adversarial Prompt**                              |
| 24.06 |                                                                                     Zhengzhou University                                                                                     |                                 ACL 2024                                  |                                                    [BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents](https://arxiv.org/abs/2406.03007)                                                     |                                          **Backdoor Attacks**&**LLM Agents**&**Data Poisoning**                                           |
| 24.06 |                                                                                Ludwig-Maximilians-University                                                                                 |                                   arxiv                                   |                                        [Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models](https://arxiv.org/abs/2406.09289)                                        |                                              **Jailbreak Success**&**Latent Space Dynamics**                                              |
| 24.06 |                                                                                        Alibaba Group                                                                                         |                                   arxiv                                   |                                         [How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States](https://arxiv.org/abs/2406.05644)                                          |                                                **LLM Safety**&**Alignment**&**Jailbreak**                                                 |
| 24.06 |                                                                                      Beihang University                                                                                      |                                   arxiv                                   |                                                [Unveiling the Safety of GPT-4O: An Empirical Study Using Jailbreak Attacks](https://arxiv.org/abs/2406.06302)                                                |                                          **GPT-4O**&**Jailbreak Attacks**&**Safety Evaluation**                                           |
| 24.06 |                                                                               Nanyang Technological University                                                                               |                                   arxiv                                   |                                  [A Survey of Backdoor Attacks and Defenses on Large Language Models: Implications for Security Measures](https://arxiv.org/abs/2406.06852)                                  |                                               **Backdoor Attacks**&**Defenses**&**Survey**                                                |
| 24.06 |                                                                                        Anomalee Inc.                                                                                         |                                   arxiv                                   |                                                                  [On Trojans in Refined Language Models](https://arxiv.org/abs/2406.07778)                                                                   |                                        **Trojans**&**Refined Language Models**&**Data Poisoning**                                         |
| 24.06 |                                                                                      Purdue University                                                                                       |                                   arxiv                                   |                                               [When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-Guided Search](https://arxiv.org/abs/2406.08705)                                                |                                             **Jailbreaking**&**Deep Reinforcement Learning**                                              |
| 24.06 |                                                                                      Xidian University                                                                                       |                                   arxiv                                   |                            [StructuralSleight: Automated Jailbreak Attacks on Large Language Models Utilizing Uncommon Text-Encoded Structure](https://arxiv.org/abs/2406.08754)                             |                                                **Jailbreak Attacks**&**StructuralSleight**                                                |
| 24.06 |                                                                                     Tsinghua University                                                                                      |                                   arxiv                                   |                                   [JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models](https://arxiv.org/abs/2406.09321)                                   |                                               **Jailbreak Attempts**&**Evaluation Toolkit**                                               |
| 24.06 |                                                                The Hong Kong University of Science and Technology (Guangzhou)                                                                |                                   arxiv                                   |                                                         [Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs](https://arxiv.org/abs/2406.09324)                                                         |                                                  **Jailbreak Attacks**&**Benchmarking**                                                   |
| 24.06 |                                                                                Pennsylvania State University                                                                                 |                                NAACL 2024                                 |                                                    [PromptFix: Few-shot Backdoor Removal via Adversarial Prompt Tuning](https://arxiv.org/abs/2406.04478)                                                    |                                 **Backdoor Removal**&**Adversarial Prompt Tuning**&**Few-shot Learning**                                  |
| 24.06 |                                                           Shanghai Jiao Tong University, Peking University, Shanghai AI Laboratory                                                           |                                   arxiv                                   |                                       [Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models](https://arxiv.org/abs/2406.10630)                                        |                                      **Federated Instruction Tuning**&**Safety Attack**&**Defense**                                       |
| 24.06 |                                                                                  Michigan State University                                                                                   |                                   arxiv                                   |                                             [Towards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis](https://arxiv.org/abs/2406.10794)                                             |                                          **Jailbreak Attacks**&**Representation Space Analysis**                                          |
| 24.06 |                                                                                 Chinese Academy of Sciences                                                                                  |                                   arxiv                                   |                                 [‚ÄúNot Aligned‚Äù is Not ‚ÄúMalicious‚Äù: Being Careful about Hallucinations of Large Language Models‚Äô Jailbreak](https://arxiv.org/abs/2406.11668)                                 |                                                 **Jailbreak**&**Hallucinations**&**LLMs**                                                 |
| 24.06 |                                                                                     Tsinghua University                                                                                      |                                   arxiv                                   |                                                       [Knowledge-to-Jailbreak: One Knowledge Point Worth One Attack](https://arxiv.org/abs/2406.11682)                                                       |                                **Knowledge-to-Jailbreak**&**Jailbreak Attacks**&**Domain-Specific Safety**                                |
| 24.06 |                                                                                    University of Maryland                                                                                    |                                   arxiv                                   |                                                [Is Poisoning a Real Threat to LLM Alignment? Maybe More So Than You Think](https://arxiv.org/abs/2406.12091)                                                 |                    **Poisoning Attacks**&**Direct Policy Optimization**&**Reinforcement Learning with Human Feedback**                    |
| 24.06 |                                                                                  Carnegie Mellon University                                                                                  |                                   arxiv                                   |                                                              [Jailbreak Paradox: The Achilles‚Äô Heel of LLMs](https://arxiv.org/abs/2406.12702)                                                               |                                                    **Jailbreak Paradox**&**Security**                                                     |
| 24.06 |                                                                                  Carnegie Mellon University                                                                                  |                                   arxiv                                   |                                                                 [Adversarial Attacks on Multimodal Agents](https://arxiv.org/abs/2406.12814)                                                                 |                                 **Adversarial Attacks**&**Multimodal Agents**&**Vision-Language Models**                                  |
| 24.06 |                                                                       University of Washington, Allen Institute for AI                                                                       |                                   arxiv                                   |                                                [ChatBug: A Common Vulnerability of Aligned LLMs Induced by Chat Templates](https://arxiv.org/abs/2406.12935)                                                 |                                  **LLM Vulnerabilities**&**Jailbreak Attacks**&**Adversarial Training**                                   |
| 24.06 |                                       University of Notre Dame, Huazhong University of Science and Technology, Tsinghua University, Lehigh University                                        |                                   arxiv                                   |                                                   [ObscurePrompt: Jailbreaking Large Language Models via Obscure Input](https://arxiv.org/abs/2406.13662)                                                    |                                   **Jailbreaking**&**Adversarial Attacks**&**Out-of-Distribution Data**                                   |
| 24.06 |                                                                      The University of Hong Kong, Huawei Noah‚Äôs Ark Lab                                                                      |                                   arxiv                                   |                                                            [Jailbreaking as a Reward Misspecification Problem](https://arxiv.org/abs/2406.14393)                                                             |                                   **Jailbreaking**&**Reward Misspecification**&**Adversarial Attacks**                                    |
| 24.06 |                                                                                         UC Berkeley                                                                                          |                                   arxiv                                   |                                                            [Adversaries Can Misuse Combinations of Safe Models](https://arxiv.org/abs/2406.14595)                                                            |                                           **Model Misuse**&**AI Safety**&**Task Decomposition**                                           |
| 24.06 |                                                                                       UC Santa Barbara                                                                                       |                                   arxiv                                   |                                [MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations](https://arxiv.org/abs/2406.14711)                                 |                                           **MultiAgent Collaboration**&**Adversarial Attacks**                                            |
| 24.06 |                                                                              University of Southern California                                                                               |                                   arxiv                                   |                                                  [From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking](https://arxiv.org/abs/2406.14859)                                                  |                                            **Multimodal Jailbreaking**&**MLLMs**&**Security**                                             |
| 24.06 |                                                                                            KAIST                                                                                             |                                   arxiv                                   |                                              [CSRT: Evaluation and Analysis of LLMs using Code-Switching Red-Teaming Dataset](https://arxiv.org/abs/2406.15481)                                              |                                          **Code-Switching**&**Red-Teaming**&**Multilingualism**                                           |
| 24.06 |                                                                               China University of Geosciences                                                                                |                                   arxiv                                   |                                              [Large Language Models for Link Stealing Attacks Against Graph Neural Networks](https://arxiv.org/abs/2406.16963)                                               |                                  **Link Stealing Attacks**&**Graph Neural Networks**&**Privacy Attacks**                                  |
| 24.06 |                                                                             The Hong Kong Polytechnic University                                                                             |                                   arxiv                                   |                                            [CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference](https://arxiv.org/abs/2406.17626)                                             |                                       **Safety Evaluation**&**Dialogue Coreference**&**LLM Safety**                                       |
| 24.06 |                                                                                   Imperial College London                                                                                    |                                   arxiv                                   |                                              [Inherent Challenges of Post-Hoc Membership Inference for Large Language Models](https://arxiv.org/abs/2406.17975)                                              |                              **Membership Inference Attacks**&**Post-Hoc Evaluation**&**Distribution Shift**                              |
| 24.06 |                                                                                       Hubei University                                                                                       |                                   arxiv                                   |                                                             [Poisoned LangChain: Jailbreak LLMs by LangChain](https://arxiv.org/abs/2406.18122)                                                              |                                      **Jailbreak**&**Retrieval-Augmented Generation**&**LangChain**                                       |
| 24.06 |                                                                                University of Central Florida                                                                                 |                                   arxiv                                   |                                                        [Jailbreaking LLMs with Arabic Transliteration and Arabizi](https://arxiv.org/abs/2406.18725)                                                         |                                          **Jailbreaking**&**Arabic Transliteration**&**Arabizi**                                          |
| 24.06 |                                                                                       Hubei University                                                                                       |                            TRAC 2024 Workshop                             |                                    [SEEING IS BELIEVING: BLACK-BOX MEMBERSHIP INFERENCE ATTACKS AGAINST RETRIEVAL AUGMENTED GENERATION](https://arxiv.org/abs/2406.19234)                                    |                                    **Membership Inference Attacks**&**Retrieval-Augmented Generation**                                    |
| 24.06 |                                                                        Huazhong University of Science and Technology                                                                         |                                   arxiv                                   |                                                [Virtual Context: Enhancing Jailbreak Attacks with Special Token Injection](https://arxiv.org/abs/2406.19845)                                                 |                                                 **Jailbreak Attacks**&**Special Tokens**                                                  |
| 24.06 |                                                                                         UC Berkeley                                                                                          |                                   arxiv                                   |                                                  [Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation](https://arxiv.org/abs/2406.20053)                                                  |                                                        **AI Safety**&**Backdoors**                                                        |
| 24.07 |                                                                                University of Illinois Chicago                                                                                |                                   arxiv                                   |                                  [Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks](https://arxiv.org/abs/2407.00869)                                   |                                              **Jailbreak Attacks**&**Fallacious Reasoning**                                               |
| 24.07 |                                                                                      Palisade Research                                                                                       |                                   arxiv                                   |                                                      [Badllama 3: Removing Safety Finetuning from Llama 3 in Minutes](https://arxiv.org/abs/2407.01376)                                                      |                                                **Safety Finetuning**&**Jailbreak Attacks**                                                |
| 24.07 |                                                                           University of Illinois Urbana-Champaign                                                                            |                                   arxiv                                   |                                 [JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large Language and Vision-Language Models](https://arxiv.org/abs/2407.01599)                                 |                                                **Jailbreaking**&**Vision-Language Models**                                                |
| 24.07 |                                                                         Shanghai University of Finance and Economics                                                                         |                                   arxiv                                   |                                               [SoP: Unlock the Power of Social Facilitation for Automatic Jailbreak Attack](https://arxiv.org/abs/2407.01902)                                                |                                  **Jailbreak Attacks**&**Large Language Models**&**Social Facilitation**                                  |
| 24.07 |                                                                                     University of Exeter                                                                                     |                                   arxiv                                   |                                                 [Image-to-Text Logic Jailbreak: Your Imagination can Help You Do Anything](https://arxiv.org/abs/2407.02534)                                                 |                                            **Machine Learning**&**ICML**&**Jailbreak Attacks**                                            |
| 24.07 |                                                                        Hong Kong University of Science and Technology                                                                        |                                   arxiv                                   |                     [JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational Datasets](https://arxiv.org/abs/2407.03045)                      |                                                **Visual Analytics**&**Jailbreak Prompts**                                                 |
| 24.07 |                                                                       CISPA Helmholtz Center for Information Security                                                                        |                                   arxiv                                   |                                                    [SOS! Soft Prompt Attack Against Open-Source Large Language Models](https://arxiv.org/abs/2407.03160)                                                     |                                               **Soft Prompt Attack**&**Open-Source Models**                                               |
| 24.07 |                                                                               National University of Singapore                                                                               |                                   arxiv                                   |                                                            [Single Character Perturbations Break LLM Alignment](https://arxiv.org/abs/2407.03232)                                                            |                                                 **Jailbreak Attacks**&**Model Alignment**                                                 |
| 24.07 |                                                                    Deutsches Forschungszentrum f√ºr K√ºnstliche Intelligenz                                                                    |                                   arxiv                                   |                          [Soft Begging: Modular and Efficient Shielding of LLMs against Prompt Injection and Jailbreaking based on Prompt Tuning](https://arxiv.org/abs/2407.03391)                          |                                          **Prompt Injection**&**Jailbreaking**&**Soft Prompts**                                           |
| 24.07 |                                                                                           UC Davis                                                                                           |                                   arxiv                                   |                                         [Securing Multi-turn Conversational Language Models Against Distributed Backdoor Triggers](https://arxiv.org/abs/2407.04151)                                         |                                    **Multi-turn Conversation**&**Backdoor Triggers**&**LLM Security**                                     |
| 24.07 |                                                                                     Tsinghua University                                                                                      |                                   arxiv                                   |                                                  [Jailbreak Attacks and Defenses Against Large Language Models: A Survey](https://arxiv.org/abs/2407.04295)                                                  |                                                    **Jailbreak Attacks**&**Defenses**                                                     |
| 24.07 |                                                                                     Zhejiang University                                                                                      |                                   arxiv                                   |                                             [TAPI: Towards Target-Specific and Adversarial Prompt Injection against Code LLMs](https://arxiv.org/abs/2407.09164)                                             |                        **Target-Specific Attacks**&**Adversarial Prompt Injection**&**Malicious Code Generation**                         |
| 24.07 |                                                                                   Northwestern University                                                                                    |                                   arxiv                                   |                                      [CEIPA: Counterfactual Explainable Incremental Prompt Attack Analysis on Large Language Models](https://arxiv.org/abs/2407.09292)                                       |                        **Counterfactual Explanation**&**Prompt Attack Analysis**&**Incremental Prompt Injection**                         |
| 24.07 |                                                                                             EPFL                                                                                             |                                   arxiv                                   |                                                       [Does Refusal Training in LLMs Generalize to the Past Tense?](https://arxiv.org/abs/2407.11969)                                                        |                                 **Refusal Training**&**Past Tense Reformulation**&**Adversarial Attacks**                                 |
| 24.07 |                                                                                    University of Chicago                                                                                     |                                   arxiv                                   |                                               [AGENTPOISON: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases](https://arxiv.org/abs/2407.12784)                                                |                                               **Red-teaming**&**LLM Agents**&**Poisoning**                                                |
| 24.07 |                                                                                       Wuhan University                                                                                       |                                   arxiv                                   |                                    [Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2407.13757)                                     |                                          **Black-box Attacks**&**RAG**&**Opinion Manipulation**                                           |
| 24.07 |                                                                                University of New South Wales                                                                                 |                                   arxiv                                   |                                          [Continuous Embedding Attacks via Clipped Inputs in Jailbreaking Large Language Models](https://arxiv.org/abs/2407.13796)                                           |                                                 **Continuous Embedding**&**Jailbreaking**                                                 |
| 24.07 |                                                                                          Bloomberg                                                                                           |                                   arxiv                                   |                                               [Operationalizing a Threat Model for Red-Teaming Large Language Models (LLMs)](https://arxiv.org/abs/2407.14937)                                               |                                                     **Threat Model**&**Red-Teaming**                                                      |
| 24.07 |                                                                                     Stanford University                                                                                      |                                   arxiv                                   |                                               [When Do Universal Image Jailbreaks Transfer Between Vision-Language Models?](https://arxiv.org/abs/2407.15211)                                                |                               **Universal Image Jailbreaks**&**Vision-Language Models**&**Transferability**                               |
| 24.07 |                                                                                  Michigan State University                                                                                   |                                   arxiv                                   |                            [Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal Mechanisms and the Superficial Hypothesis](https://arxiv.org/abs/2407.15286)                            |                                            **Moral Self-Correction**&**Intrinsic Mechanisms**                                             |
| 24.07 |                                                                                        Meetyou AI Lab                                                                                        |                                   arxiv                                   |                                      [Imposter.AI: Adversarial Attacks with Hidden Intentions towards Aligned Large Language Models](https://arxiv.org/abs/2407.15399)                                       |                                               **Adversarial Attacks**&**Hidden Intentions**                                               |
| 24.07 |                                                                                Zhejiang Gongshang University                                                                                 |                                   arxiv                                   |                                                 [Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models](https://arxiv.org/abs/2407.16205)                                                 |                                            **Jailbreak Attack**&**Analyzing-based Jailbreak**                                             |
| 24.07 |                                                                                     Zhejiang University                                                                                      |                                   arxiv                                   |                                         [RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent](https://arxiv.org/abs/2407.16667)                                         |                                      **Red Teaming**&**Jailbreak Attacks**&**Context-aware Prompts**                                      |
| 24.07 |                                                                                         Confirm Labs                                                                                         |                                   arxiv                                   |                                                                    [Fluent Student-Teacher Redteaming](https://arxiv.org/abs/2407.17447)                                                                     |                                       **Fluent Student-Teacher Redteaming**&**Adversarial Attacks**                                       |
| 24.07 |                                                                                 City University of Hong Kong                                                                                 |                                ACM MM 2024                                |                                  [Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts](https://arxiv.org/abs/2407.15050)                                   |                                   **Large Vision Language Model**&**Red Teaming**&**Jailbreak Attack**                                    |
| 24.07 |                                                                        Huazhong University of Science and Technology                                                                         |                            NAACL 2024 Workshop                            |                                                        [Can Large Language Models Automatically Jailbreak GPT-4V?](https://arxiv.org/abs/2407.16686)                                                         |                                      **Jailbreak**&**Multimodal Information**&**Facial Recognition**                                      |
| 24.07 |                                                                               Illinois Institute of Technology                                                                               |                                   arxiv                                   |                                                                      [Can Editing LLMs Inject Harm?](https://arxiv.org/abs/2407.20224)                                                                       |                                   **Knowledge Editing**&**Misinformation Injection**&**Bias Injection**                                   |
| 24.07 |                                                                                            KAIST                                                                                             |                                   arxiv                                   |                                                 [Prompt-Driven Contrastive Learning for Transferable Adversarial Attacks](https://arxiv.org/abs/2407.20657)                                                  |                                 **Adversarial Attack**&**Vision-Language Model**&**Contrastive Learning**                                 |
| 24.07 |                                                                       CISPA Helmholtz Center for Information Security                                                                        |                                   arxiv                                   |                                          [Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification](https://arxiv.org/abs/2407.20859)                                           |                                     **LLM Agents**&**Security Vulnerability**&**Autonomous Systems**                                      |
| 24.08 |                                                                       CISPA Helmholtz Center for Information Security                                                                        |                                   arxiv                                   |                                                                 [Vera Verto: Multimodal Hijacking Attack](https://arxiv.org/abs/2408.00129)                                                                  |                                            **Multimodal Hijacking Attack**&**Model Hijacking**                                            |
| 24.08 |                                                                                     Shandong University                                                                                      |                                   arxiv                                   |                                                         [Jailbreaking Text-to-Image Models with LLM-Based Agents](https://arxiv.org/abs/2408.00523)                                                          |                             **Jailbreak Attacks**&**Vision-Language Models (VLMs)**&**Generative AI Safety**                              |
| 24.08 |                                                                               Technological University Dublin                                                                                |                                   arxiv                                   |                                            [Pathway to Secure and Trustworthy 6G for LLMs: Attacks, Defense, and Opportunities](https://arxiv.org/abs/2408.00722)                                            |                                       **6G Networks**&**Security**&**Membership Inference Attacks**                                       |
| 24.08 |                                                                                          Microsoft                                                                                           |                                   arxiv                                   |                                                 [WHITE PAPER: A Brief Exploration of Data Exfiltration using GCG Suffixes](https://arxiv.org/abs/2408.00925)                                                 |                          **Cross-Prompt Injection Attack**&**Greedy Coordinate Gradient**&**Data Exfiltration**                           |
| 24.08 |                                                                                     NYU & Meta AI, FAIR                                                                                      |                                   arxiv                                   |                                                    [Mission Impossible: A Statistical Perspective on Jailbreaking LLMs](https://arxiv.org/abs/2408.01420)                                                    |                                      **Jailbreaking**&**Reinforcement Learning with Human Feedback**                                      |
| 24.08 |                                                                                      Beihang University                                                                                      |                                   arxiv                                   |                                                      [Compromising Embodied Agents with Contextual Backdoor Attacks](https://arxiv.org/abs/2408.02882)                                                       |                         **Embodied Agents**&**Contextual Backdoor Attacks**&**Adversarial In-Context Generation**                         |
| 24.08 |                                                                                            FAR AI                                                                                            |                                   arxiv                                   |                                                                 [Scaling Laws for Data Poisoning in LLMs](https://arxiv.org/abs/2408.02946)                                                                  |                                                    **Data Poisoning**&**Scaling Laws**                                                    |
| 24.08 |                                                                             The University of Western Australia                                                                              |                                   arxiv                                   |                                             [A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic Systems](https://arxiv.org/abs/2408.03515)                                             |                                                   **Mobile Robot**&**Prompt Injection**                                                   |
| 24.08 |                                                                                       Fudan University                                                                                       |                                   arxiv                                   |                                                            [EnJa: Ensemble Jailbreak on Large Language Models](https://arxiv.org/abs/2408.03603)                                                             |                                                       **Jailbreaking**&**Security**                                                       |
| 24.08 |                                                                                      Bocconi University                                                                                      |                                   arxiv                                   |                                         [Compromesso! Italian Many-Shot Jailbreaks Undermine the Safety of Large Language Models](https://arxiv.org/abs/2408.04522)                                          |                                                 **Jailbreaking**&**Multilingual Safety**                                                  |
| 24.08 |                                                                                      Xidian University                                                                                       |                                   arxiv                                   |                                            [Multi-Turn Context Jailbreak Attack on Large Language Models From First Principles](https://arxiv.org/abs/2408.04686)                                            |                                       **Multi-Turn Jailbreak Attack**&**Contextual Fusion Attack**                                        |
| 24.08 |                                                                                         Cornell Tech                                                                                         |                                   arxiv                                   |                              [A Jailbroken GenAI Model Can Cause Substantial Harm: GenAI-powered Applications are Vulnerable to PromptWares](https://arxiv.org/abs/2408.05061)                               |                                **Jailbroken GenAI Models**&**PromptWares**&**GenAI-powered Applications**                                 |
| 24.08 |                                                                               University of California Irvine                                                                                |                                   arxiv                                   |                                               [Using Retriever Augmented Large Language Models for Attack Graph Generation](https://arxiv.org/abs/2408.05855)                                                |                                  **Retriever Augmented Generation**&**Attack Graphs**&**Cybersecurity**                                   |
| 24.08 |                                                                            University of California, Los Angeles                                                                             |                                 CCS 2024                                  |                                                            [BadMerging: Backdoor Attacks Against Model Merging](https://arxiv.org/abs/2408.07362)                                                            |                                           **Backdoor Attack**&**Model Merging**&**AI Security**                                           |
| 24.08 |                                                                                     Stanford University                                                                                      |                                   arxiv                                   |                                 [Kov: Transferable and Naturalistic Black-Box LLM Attacks using Markov Decision Processes and Tree Search](https://arxiv.org/abs/2408.08899)                                 |                              **Black-Box Attacks**&**Markov Decision Processes**&**Monte Carlo Tree Search**                              |
| 24.08 |                                                                                Shanghai Jiao Tong University                                                                                 |                                   arxiv                                   |                                              [Transferring Backdoors between Large Language Models by Knowledge Distillation](https://arxiv.org/abs/2408.09878)                                              |                                              **Backdoor Attacks**&**Knowledge Distillation**                                              |
| 24.08 |                                                                                     Tsinghua University                                                                                      |                                   arxiv                                   |                                    [Probing the Safety Response Boundary of Large Language Models via Unsafe Decoding Path Generation](https://arxiv.org/abs/2408.10668)                                     |                                           **Safety Response Boundary**&**Unsafe Decoding Path**                                           |
| 24.08 |                                                                        Singapore University of Technology and Design                                                                         |                                   arxiv                                   |                                          [FERRET: Faster and Effective Automated Red Teaming with Reward-Based Scoring Technique](https://arxiv.org/abs/2408.10701)                                          |                                **Automated Red Teaming**&**Adversarial Prompts**&**Reward-Based Scoring**                                 |
| 24.08 |                                                                                Shanghai Jiao Tong University                                                                                 |                                   arxiv                                   |                                                  [MEGen: Generative Backdoor in Large Language Models via Model Editing](https://arxiv.org/abs/2408.10722)                                                   |                                                  **Backdoor Attacks**&**Model Editing**                                                   |
| 24.08 |                                                                                 Chinese Academy of Sciences                                                                                  |                                   arxiv                                   |                       [DiffZOO: A Purely Query-Based Black-Box Attack for Red-Teaming Text-to-Image Generative Model via Zeroth Order Optimization](https://arxiv.org/abs/2408.11071)                        |                           **Black-Box Attack**&**Text-to-Image Generative Model**&**Zeroth Order Optimization**                           |
| 24.08 |                                                                              The Pennsylvania State University                                                                               |                                   arxiv                                   |                             [Hide Your Malicious Goal Into Benign Narratives: Jailbreak Large Language Models through Neural Carrier Articles](https://arxiv.org/abs/2408.11182)                             |                                                **Jailbreak Attacks**&**Prompt Injection**                                                 |
| 24.08 |                                                                                  Xi'an Jiaotong University                                                                                   |                                   arxiv                                   |                       [Unlocking Adversarial Suffix Optimization Without Affirmative Phrases: Efficient Black-box Jailbreaking via LLM as Optimizer](https://arxiv.org/abs/2408.11313)                       |                                              **Jailbreak Attacks**&**Adversarial Suffixes**                                               |
| 24.08 |                                                                   Nanjing University of Information Science and Technology                                                                   |                                   arxiv                                   |                                        [Large Language Models are Good Attackers: Efficient and Stealthy Textual Backdoor Attacks](https://arxiv.org/abs/2408.11587)                                         |                                             **Textual Backdoor Attacks**&**Sample Selection**                                             |
| 24.08 |                                                                                      Nankai University                                                                                       |                                   arxiv                                   |                                                      [RT-Attack: Jailbreaking Text-to-Image Models via Random Token](https://arxiv.org/abs/2408.13896)                                                       |                                          **Jailbreak**&**Text-to-Image**&**Adversarial Attacks**                                          |
| 24.08 |                                                                           Harbin Institute of Technology, Shenzhen                                                                           |                                   arxiv                                   |                                              [TF-Attack: Transferable and Fast Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2408.13985)                                               |                                         **Adversarial Attack**&**Transferability**&**Efficiency**                                         |
| 24.08 |                                                                           Shenzhen Research Institute of Big Data                                                                            |                                   arxiv                                   |                                             [Detecting AI Flaws: Target-Driven Attacks on Internal Faults in Language Models](https://arxiv.org/abs/2408.14853)                                              |                                 **Target-Driven Attacks**&**Internal Faults**&**Reinforcement Learning**                                  |
| 24.08 |                                                                               National University of Singapore                                                                               |                                   arxiv                                   |                                             [Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models](https://arxiv.org/abs/2408.14866)                                              |                                       **Adversarial Suffixes**&**Transfer Learning**&**Jailbreak**                                        |
| 24.08 |                                                                                           Scale AI                                                                                           |                                   arxiv                                   |                                                      [LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet](https://arxiv.org/abs/2408.15221)                                                      |                                      **Multi-Turn Jailbreaks**&**LLM Defense**&**Human Red Teaming**                                      |
| 24.09 |                                                                              University of California, Berkeley                                                                              |                                   arxiv                                   |                                                [Emerging Vulnerabilities in Frontier Models: Multi-Turn Jailbreak Attacks](https://arxiv.org/abs/2409.00137)                                                 |                                       **Multi-Turn Jailbreak**&**Frontier Models**&**LLM Security**                                       |
| 24.09 |                                                                              University of Southern California                                                                               |                                   arxiv                                   |                                                       [Rethinking Backdoor Detection Evaluation for Language Models](https://arxiv.org/abs/2409.00399)                                                       |                                   **Backdoor Attacks**&**Detection Robustness**&**Training Intensity**                                    |
| 24.09 |                                                                                  Michigan State University                                                                                   |                                   arxiv                                   |                                             [The Dark Side of Human Feedback: Poisoning Large Language Models via User Inputs](https://arxiv.org/abs/2409.00787)                                             |                                       **User-Guided Poisoning**&**RLHF**&**Toxicity Manipulation**                                        |
| 24.09 |                                                                                   University of Cambridge                                                                                    |                                   arxiv                                   |                                                  [Conversational Complexity for Assessing Risk in Large Language Models](https://arxiv.org/abs/2409.01247)                                                   |                                             **Conversational Complexity**&**Risk Assessment**                                             |
| 24.09 |                                                                                         Independent                                                                                          |                                   arxiv                                   |                                                  [Well, that escalated quickly: The Single-Turn Crescendo Attack (STCA)](https://arxiv.org/abs/2409.03131)                                                   |                                         **Single-Turn Crescendo Attack**&**Adversarial Attacks**                                          |
| 24.09 |                                                                       CISPA Helmholtz Center for Information Security                                                                        |                                 CCS 2024                                  |                                                         [Membership Inference Attacks Against In-Context Learning](https://arxiv.org/abs/2409.01380)                                                         |                                         **Membership Inference Attacks**&**In-Context Learning**                                          |
| 24.09 |                                                                         Radboud University, Ikerlan Research Centre                                                                          |                                   arxiv                                   |                                          [Context is the Key: Backdoor Attacks for In-Context Learning with Vision Transformers](https://arxiv.org/abs/2409.04142)                                           |                                   **Backdoor Attacks**&**In-Context Learning**&**Vision Transformers**                                    |
| 24.09 |                                                              Institute of Information Engineering, Chinese Academy of Sciences                                                               |                                   arxiv                                   |                                               [AdaPPA: Adaptive Position Pre-Fill Jailbreak Attack Approach Targeting LLMs](https://arxiv.org/abs/2409.07503)                                                |                                           **Jailbreak Attacks**&**Adaptive Position Pre-Fill**                                            |
| 24.09 |                                                               Technion - Israel Institute of Technology, Intuit, Cornell Tech                                                                |                                   arxiv                                   |               [Unleashing Worms and Extracting Data: Escalating the Outcome of Attacks against RAG-based Inference in Scale and Severity Using Jailbreaking](https://arxiv.org/abs/2409.08045)               |                                          **Jailbreaking**&**RAG Inference**&**Data Extraction**                                           |
| 24.09 |                                                                              University of Texas at San Antonio                                                                              |                                   arxiv                                   |                                                       [Jailbreaking Large Language Models with Symbolic Mathematics](https://arxiv.org/abs/2409.11445)                                                       |                                                 **Jailbreaking**&**Symbolic Mathematics**                                                 |
| 24.09 |                                                                                      Beihang University                                                                                      |                                   arxiv                                   |                                [PathSeeker: Exploring LLM Security Vulnerabilities with a Reinforcement Learning-Based Jailbreak Approach](https://arxiv.org/abs/2409.14177)                                 |                             **LLM Security Vulnerabilities**&**Jailbreak Attack**&**Reinforcement Learning**                              |
| 24.09 |                                                                                            AWS AI                                                                                            |                                   arxiv                                   |                                                         [Order of Magnitude Speedups for LLM Membership Inference](https://arxiv.org/abs/2409.14513)                                                         |                                             **Membership Inference**&**Quantile Regression**                                              |
| 24.09 |                                                                               Nanyang Technological University                                                                               |                                   arxiv                                   |                                               [Effective and Evasive Fuzz Testing-Driven Jailbreaking Attacks against LLMs](https://arxiv.org/abs/2409.14866)                                                |                                        **Jailbreaking Attacks**&**Fuzz Testing**&**LLM Security**                                         |
| 24.09 |                                                                                        Hippocratic AI                                                                                        |                                   arxiv                                   |                                         [RED QUEEN: Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking](https://arxiv.org/abs/2409.17458)                                          |                                       **Jailbreaking**&**Multi-Turn Attacks**&**Concealed Attacks**                                       |
| 24.09 |                                                                               Nanyang Technological University                                                                               |                                   arxiv                                   |                                             [Weak-to-Strong Backdoor Attacks for LLMs with Contrastive Knowledge Distillation](https://arxiv.org/abs/2409.17946)                                             |                      **Backdoor Attacks**&**Contrastive Knowledge Distillation**&**Parameter-Efficient Fine-Tuning**                      |
| 24.09 |                                                                               Georgia Institute of Technology                                                                                |                                   arxiv                                   |                                               [Harmful Fine-tuning Attacks and Defenses for Large Language Models: A Survey](https://arxiv.org/abs/2409.18169)                                               |                                         **Harmful Fine-tuning**&**LLM Attacks**&**LLM Defenses**                                          |
| 24.09 |                                                                               Institut Polytechnique de Paris                                                                                |                                   arxiv                                   |                                   [Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with ASCII Art to Mask Profanity](https://arxiv.org/abs/2409.18708)                                    |                                           **ASCII Art**&**LLM Attacks**&**Toxicity Detection**                                            |
| 24.09 |                                                                                          LMU Munich                                                                                          |                                   arxiv                                   |                                                          [Multimodal Pragmatic Jailbreak on Text-to-image Models](https://arxiv.org/abs/2409.19149)                                                          |                              **Multimodal Pragmatic Jailbreak**&**Text-to-image Models**&**Safety Filters**                               |
| 24.10 |                                                                                    Stony Brook University                                                                                    |                                   arxiv                                   |                                               [BUCKLE UP: ROBUSTIFYING LLMS AT EVERY CUSTOMIZATION STAGE VIA DATA CURATION](https://arxiv.org/abs/2410.02220)                                                |                                         **Jailbreaking**&**LLM Customization**&**Data Curation**                                          |
| 24.10 |                                                                               National University of Singapore                                                                               |                                   arxiv                                   |                                                                 [FLIPATTACK: Jailbreak LLMs via Flipping](https://arxiv.org/abs/2410.02832)                                                                  |                                                   **Jailbreak**&**Adversarial Attacks**                                                   |
| 24.10 |                                                                           University of Wisconsin‚ÄìMadison, NVIDIA                                                                            |                                   arxiv                                   |                                             [AUTODAN-TURBO: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs](https://arxiv.org/abs/2410.05295)                                              |                                                **Jailbreak**&**Strategy Self-Exploration**                                                |
| 24.10 |                                                                        University College London, Stanford University                                                                        |                                   arxiv                                   |                                                 [Prompt Infection: LLM-to-LLM Prompt Injection within Multi-Agent Systems](https://arxiv.org/abs/2410.07283)                                                 |                                               **Prompt Infection**&**Multi-Agent Systems**                                                |
| 24.10 |                                                                               University of Wisconsin‚ÄìMadison                                                                                |                                   arxiv                                   |                                         [RePD: Defending Jailbreak Attack through a Retrieval-based Prompt Decomposition Process](https://arxiv.org/abs/2410.08660)                                          |                                      **Jailbreak attack**&**Prompt decomposition**&**LLMs defense**                                       |
| 24.10 |                                                     UC Santa Cruz, Johns Hopkins University, University of Edinburgh, Peking University                                                      |                                   arxiv                                   |                                               [AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation](https://arxiv.org/abs/2410.09040)                                                |                                  **Jailbreaking**&**LLMs vulnerability**&**Optimization-based attacks**                                   |
| 24.10 |                                                                                    Independent Researcher                                                                                    |                                   arxiv                                   |                                         [Recent advancements in LLM Red-Teaming: Techniques, Defenses, and Ethical Considerations](https://arxiv.org/abs/2410.09097)                                         |                                   **LLM red-teaming**&**Jailbreaking defenses**&**Prompt engineering**                                    |
| 24.10 |                                                                  Beihang University, Tsinghua University, Peking University                                                                  |                                   arxiv                                   |                            [BLACKDAN: A Black-Box Multi-Objective Approach for Effective and Contextual Jailbreaking of Large Language Models](https://arxiv.org/abs/2410.09804)                             |                                    **Jailbreak**&**Multi-objective optimization**&**Black-box attack**                                    |
| 24.10 |                                                Shanghai Jiao Tong University, Shanghai Artificial Intelligence Laboratory, Beihang University                                                |                                   arxiv                                   |                                              [Derail Yourself: Multi-Turn LLM Jailbreak Attack Through Self-Discovered Clues](https://arxiv.org/abs/2410.10700)                                              |                                      **Multi-turn attacks**&**Jailbreak**&**Self-discovered clues**                                       |
| 24.10 |                                                                    Tsinghua University, Sea AI Lab, Peng Cheng Laboratory                                                                    |                                   arxiv                                   |                                                       [Denial-of-Service Poisoning Attacks on Large Language Models](https://arxiv.org/abs/2410.10760)                                                       |                                                **Denial-of-Service**&**Poisoning attack**                                                 |
| 24.10 |                                                                         University of New Haven, Robust Intelligence                                                                         |                                   arxiv                                   |                                                       [COGNITIVE OVERLOAD ATTACK: PROMPT INJECTION FOR LONG CONTEXT](https://arxiv.org/abs/2410.11272)                                                       |                                         **Cognitive overload**&**Prompt injection**&**Jailbreak**                                         |
| 24.10 |                                                    Harbin Institute of Technology, Tencent, University of Glasgow, Independent Researcher                                                    |                                   arxiv                                   |                                          [DECIPHERING THE CHAOS: ENHANCING JAILBREAK ATTACKS VIA ADVERSARIAL PROMPT TRANSLATION](https://arxiv.org/abs/2410.11317)                                           |                               **Jailbreak attacks**&**Adversarial prompt**&**Gradient-based optimization**                                |
| 24.10 |                                                                                      Monash University                                                                                       |                                   arxiv                                   |                                              [JIGSAW PUZZLES: Splitting Harmful Questions to Jailbreak Large Language Models](https://arxiv.org/abs/2410.11459)                                              |                                          **Jailbreak**&**Multi-turn attack**&**Query splitting**                                          |
| 24.10 |                                                                                       Wuhan University                                                                                       |                                   arxiv                                   |                                                          [Multi-Round Jailbreak Attack on Large Language Models](https://arxiv.org/abs/2410.11533)                                                           |                                                   **Jailbreak**&**Multi-round attack**                                                    |
| 24.10 |                                             The Hong Kong University of Science and Technology (Guangzhou), University of Birmingham, Baidu Inc.                                             |                                   arxiv                                   |                             [JAILJUDGE: A Comprehensive Jailbreak Judge Benchmark with Multi-Agent Enhanced Explanation Evaluation Framework](https://arxiv.org/abs/2410.12855)                              |                                               **Jailbreak judge**&**Multi-agent framework**                                               |
| 24.10 |                                                                                         Theori Inc.                                                                                          |                                 ICLR 2025                                 |                                 [DO LLMS HAVE POLITICAL CORRECTNESS? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems](https://arxiv.org/abs/2410.13334)                                 |                                    **Political correctness**&**Jailbreak**&**Ethical vulnerabilities**                                    |
| 24.10 |                                                                                    University of Manitoba                                                                                    |                                   arxiv                                   |                                                               [SoK: Prompt Hacking of Large Language Models](https://arxiv.org/abs/2410.13901)                                                               |                                                 **Prompt Hacking**&**Jailbreak Attacks**                                                  |
| 24.10 |                                                                                          Thales DIS                                                                                          |                                   arxiv                                   |                              [Backdoored Retrievers for Prompt Injection Attacks on Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2410.14479)                               |                               **Retrieval-Augmented Generation**&**Prompt Injection**&**Backdoor Attacks**                                |
| 24.10 |                                                                                       Duke University                                                                                        |                                   arxiv                                   |                                                    [Making LLMs Vulnerable to Prompt Injection via Poisoning Alignment](https://arxiv.org/abs/2410.14827)                                                    |                                   **Prompt Injection**&**Poisoning Alignment**&**LLM Vulnerabilities**                                    |
| 24.10 |                                                                                     Tsinghua University                                                                                      |                                   arxiv                                   |                                   [Faster-GCG: Efficient Discrete Optimization Jailbreak Attacks against Aligned Large Language Models](https://arxiv.org/abs/2410.15362)                                    |                                  **Jailbreak Attacks**&**Discrete Optimization**&**Adversarial Attacks**                                  |
| 24.10 |                                                                            International Digital Economy Academy                                                                             |                                   arxiv                                   |                                            [SMILES-Prompting: A Novel Approach to LLM Jailbreak Attacks in Chemical Synthesis](https://arxiv.org/abs/2410.15641)                                             |                                      **SMILES-Prompting**&**Jailbreak Attacks**&**Chemical Safety**                                       |
| 24.10 |                                                                                    University of T√ºbingen                                                                                    |                                 ICML 2025                                 |                                           [An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks](https://arxiv.org/abs/2410.16222)                                            |                                           **Jailbreak**&**Threat Model**&**N-gram Perplexity**                                            |
| 24.10 |                                                                      Beijing University of Posts and Telecommunications                                                                      |                                   arxiv                                   |                                            [FEINT AND ATTACK: Attention-Based Strategies for Jailbreaking and Protecting LLMs](https://arxiv.org/abs/2410.16327)                                             |                                   **Attention Mechanisms**&**Jailbreak Attacks**&**Defense Strategies**                                   |
| 24.10 |                                                                                       IBM Research AI                                                                                        |                                   arxiv                                   |                                                      [Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In](https://arxiv.org/abs/2410.16950)                                                      |                                     **ReAct Agents**&**Prompt Injection**&**Foot-in-the-Door Attack**                                     |
| 24.10 |                                                                                       Google DeepMind                                                                                        |                                   arxiv                                   |                                                       [Remote Timing Attacks on Efficient Language Model Inference](https://arxiv.org/abs/2410.17175)                                                        |                                          **Timing Attacks**&**Efficient Inference**&**Privacy**                                           |
| 24.10 |                                                                                             Meta                                                                                             |                                   arxiv                                   |                                           [Towards Understanding the Fragility of Multilingual LLMs against Fine-Tuning Attacks](https://arxiv.org/abs/2410.18210)                                           |                                    **Fine-Tuning Attacks**&**Multilingual LLMs**&**Safety Alignment**                                     |
| 24.10 |                                                                              University of California San Diego                                                                              |                                   arxiv                                   |                                                    [Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities](https://arxiv.org/abs/2410.18469)                                                     |                                     **Jailbreaking**&**LLM Vulnerabilities**&**Adversarial Attacks**                                      |
| 24.10 |                                                                                   Florida State University                                                                                   |                                   arxiv                                   |                                                [Adversarial Attacks on Large Language Models Using Regularized Relaxation](https://arxiv.org/abs/2410.19160)                                                 |                                            **Adversarial Attacks**&**Continuous Optimization**                                            |
| 24.10 |                                                                              The Pennsylvania State University                                                                               |                                   arxiv                                   |                                                      [Humanizing the Machine: Proxy Attacks to Mislead LLM Detectors](https://arxiv.org/abs/2410.19230)                                                      |                                               **Adversarial Attacks**&**Detection Evasion**                                               |
| 24.10 |                                                                               Nanyang Technological University                                                                               |                                   arxiv                                   |                                                [Mask-based Membership Inference Attacks for Retrieval-Augmented Generation](https://arxiv.org/abs/2410.20142)                                                |                              **Retrieval-Augmented Generation**&**Membership Inference Attacks**&**Privacy**                              |
| 24.10 |                                                                                   George Mason University                                                                                    |                                   arxiv                                   |                                        [Hacking Back the AI-Hacker: Prompt Injection as a Defense Against LLM-driven Cyberattacks](https://arxiv.org/abs/2410.20911)                                         |                                 **Prompt Injection Defense**&**LLM Cybersecurity**&**Adversarial Inputs**                                 |
| 24.10 |                                                                                       Fudan University                                                                                       |                                   arxiv                                   |                                         [BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against Jailbreak Attacks](https://arxiv.org/abs/2410.20971)                                         |                                **Jailbreak Defense**&**Vision-Language Models**&**Reinforcement Learning**                                |
| 24.10 |                                                                                Harbin Institute of Technology                                                                                |                                   arxiv                                   |                                              [Stealthy Jailbreak Attacks on Large Language Models via Benign Data Mirroring](https://arxiv.org/abs/2410.21083)                                               |                                               **Jailbreak Attacks**&**Adversarial Prompts**                                               |
| 24.10 |                                                                           SRM Institute of Science and Technology                                                                            |                                   arxiv                                   |                                                             [Palisade - Prompt Injection Detection Framework](https://arxiv.org/abs/2410.21146)                                                              |                                            **Prompt Injection**&**Heuristic-based Detection**                                             |
| 24.10 |                                                                                  The Ohio State University                                                                                   |                                   arxiv                                   |                      [AmpleGCG-Plus: A Strong Generative Model of Adversarial Suffixes to Jailbreak LLMs with Higher Success Rates in Fewer Attempts](https://arxiv.org/abs/2410.22143)                      |                                   **Jailbreak Attacks**&**Adversarial Suffixes**&**Generative Models**                                    |
| 24.10 |                                                                                     Zhejiang University                                                                                      |                                   arxiv                                   |                                              [HIJACKRAG: Hijacking Attacks against Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2410.22832)                                              |                        **Retrieval-Augmented Generation**&**Prompt Injection Attacks**&**Security Vulnerability**                         |
| 24.10 |                                                                                      The Baldwin School                                                                                      |                                   arxiv                                   |                                          [Systematically Analyzing Prompt Injection Vulnerabilities in Diverse LLM Architectures](https://arxiv.org/abs/2410.23308)                                          |                                       **Prompt Injection Vulnerabilities**&**Model Susceptibility**                                       |
| 24.10 |                                                                          Competition for LLM and Agent Safety 2024                                                                           |                                   arxiv                                   |                                                [Transferable Ensemble Black-box Jailbreak Attacks on Large Language Models](https://arxiv.org/abs/2410.23558)                                                |                                           **Black-box Jailbreak Attacks**&**Ensemble Methods**                                            |
| 24.10 |                                                                   University of Electronic Science and Technology of China                                                                   |                                   arxiv                                   |                                                           [Pseudo-Conversation Injection for LLM Goal Hijacking](https://arxiv.org/abs/2410.23678)                                                           |                                                  **Goal Hijacking**&**Prompt Injection**                                                  |
| 24.10 |                                                                                      Monash University                                                                                       |                                   arxiv                                   |                                                  [Audio Is the Achilles‚Äô Heel: Red Teaming Audio Large Multimodal Models](https://arxiv.org/abs/2410.23861)                                                  |                               **Audio Multimodal Models**&**Safety Vulnerabilities**&**Jailbreak Attacks**                                |
| 24.11 |                                                                                       Fudan University                                                                                       |                                   arxiv                                   |                                                                  [IDEATOR: Jailbreaking VLMs Using VLMs](https://arxiv.org/abs/2411.00827)                                                                   |                                   **Vision-Language Models**&**Jailbreak Attack**&**Multimodal Safety**                                   |
| 24.11 |                                                                           International Computer Science Institute                                                                           |                                   arxiv                                   |                                                [Emoji Attack: A Method for Misleading Judge LLMs in Safety Risk Detection](https://arxiv.org/abs/2411.01077)                                                 |                                           **Emoji Attack**&**Jailbreaking**&**Judge LLMs Bias**                                           |
| 24.11 |                                                                                      Peking University                                                                                       |                                   arxiv                                   |                                                            [B4: A Black-Box ScruBBing Attack on LLM Watermarks](https://arxiv.org/abs/2411.01222)                                                            |                                **Black-Box Attack**&**Watermark Removal**&**Adversarial Text Generation**                                 |
| 24.11 |                                                                        University of Science and Technology of China                                                                         |                                   arxiv                                   |                                                 [SQL Injection Jailbreak: a structural disaster of large language models](https://arxiv.org/abs/2411.01565)                                                  |                                       **SQL Injection**&**Jailbreak Attack**&**LLM Vulnerability**                                        |
| 24.11 |                                                                           University of Illinois Urbana-Champaign                                                                            |                                   arxiv                                   |                                           [Stochastic Monkeys at Play: Random Augmentations Cheaply Break LLM Safety Alignment](https://arxiv.org/abs/2411.02785)                                            |                                      **Random Augmentations**&**Safety Alignment**&**LLM Jailbreak**                                      |
| 24.11 |                                                                                 Cambridge ERA: AI Fellowship                                                                                 |                                   arxiv                                   |                                           [What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms Behind Attacks](https://arxiv.org/abs/2411.03343)                                           |                                    **Jailbreak Prompts**&**Nonlinear Probes**&**Adversarial Attacks**                                     |
| 24.11 |                                                                                        Alibaba Group                                                                                         |                                   arxiv                                   |                                                     [MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue](https://arxiv.org/abs/2411.03814)                                                     |                                    **Multi-Round Dialogue**&**Jailbreak Agent**&**LLM Vulnerability**                                     |
| 24.11 |                                                                                     Columbia University                                                                                      |                                   arxiv                                   |                                                             [Diversity Helps Jailbreak Large Language Models](https://arxiv.org/abs/2411.04223)                                                              |                                       **Jailbreak Techniques**&**LLM Safety**&**Prompt Diversity**                                        |
| 24.11 |                                                                     Bangladesh University of Engineering and Technology                                                                      |                                   arXiv                                   |                            [SequentialBreak: Large Language Models Can Be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains](https://arxiv.org/abs/2411.06426)                             |                                   **Jailbreak Attacks**&**Prompt Engineering**&**LLM Vulnerabilities**                                    |
| 24.11 |                                                                             Xi'an Jiaotong-Liverpool University                                                                              |                                   arXiv                                   |                                                              [Target-driven Attack for Large Language Models](https://arxiv.org/abs/2411.07268)                                                              |                                              **Black-box Attacks**&**Optimization Methods**                                               |
| 24.11 |                                                                               Georgia Institute of Technology                                                                                |                                   arXiv                                   |                                                         [LLM STINGER: Jailbreaking LLMs using RL Fine-tuned LLMs](https://arxiv.org/abs/2411.08862)                                                          |                               **Jailbreaking Attacks**&**Reinforcement Learning**&**Adversarial Suffixes**                                |
| 24.11 |                                                                      Beijing University of Posts and Telecommunications                                                                      |                                   arXiv                                   |                                              [Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey](https://arxiv.org/abs/2411.09259)                                               |                              **Jailbreak Attacks**&**Multimodal Generative Models**&**Security Challenges**                               |
| 24.11 |                                                                                   Arizona State University                                                                                   |                      NeurIPS 2024 SafeGenAI Workshop                      |                             [Zer0-Jack: A Memory-efficient Gradient-based Jailbreaking Method for Black-box Multi-modal Large Language Models](https://arxiv.org/abs/2411.07559)                             |                              **Black-box Jailbreaking**&**Multi-modal Models**&**Zeroth-order Optimization**                              |
| 24.11 |                                                                                     Zhejiang University                                                                                      |                                   arxiv                                   |                                        [JailbreakLens: Interpreting Jailbreak Mechanism in the Lens of Representation and Circuit](https://arxiv.org/abs/2411.11114)                                         |                              **Jailbreak Attacks**&**Large Language Models**&**Mechanism Interpretability**                               |
| 24.11 |                                                                   University of Electronic Science and Technology of China                                                                   |                                   arxiv                                   |                                [Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to Jailbreak Large Vision-Language Models](https://arxiv.org/abs/2411.11496)                                |                               **Jailbreaking**&**Large Vision-Language Models**&**Safety Snowball Effect**                                |
| 24.11 |                                                                                     Tsinghua University                                                                                      |                                   arxiv                                   |                                                          [Playing Language Game with LLMs Leads to Jailbreaking](https://arxiv.org/abs/2411.12762)                                                           |                                            **Jailbreaking**&**Language Games**&**LLM Safety**                                             |
| 24.11 |                                                                                University of Texas at Dallas                                                                                 |                                   arxiv                                   |                           [AttentionBreaker: Adaptive Evolutionary Optimization for Unmasking Vulnerabilities in LLMs through Bit-Flip Attacks](https://arxiv.org/abs/2411.13757)                            |                                         **Bit-Flip Attacks**&**Model Vulnerability Optimization**                                         |
| 24.11 |                                                                                         BITS Pilani                                                                                          |                                   arxiv                                   |                                            [GASP: Efficient Black-Box Generation of Adversarial Suffixes for Jailbreaking LLMs](https://arxiv.org/abs/2411.14133)                                            |                                 **Jailbreaking**&**Latent Bayesian Optimization**&**Adversarial Prompts**                                 |
| 24.11 |                                                                      Nanyang Technological University, Wuhan University                                                                      |                                   arxiv                                   |                                              [Neutralizing Backdoors through Information Conflicts for Large Language Models](https://arxiv.org/abs/2411.18280)                                              |                                     **Backdoor Defense**&**Information Conflicts**&**Model Security**                                     |
| 24.11 |                                                                          Duke University, University of Louisville                                                                           |                                   arxiv                                   |                                                            [LoBAM: LoRA-Based Backdoor Attack on Model Merging](https://arxiv.org/abs/2411.16746)                                                            |                                              **Model Merging**&**Backdoor Attack**&**LoRA**                                               |
| 24.11 |                                                                       Universit√© de Sherbrooke&University of Kinshasa                                                                        |                                   arxiv                                   |                                     [Preventing Jailbreak Prompts as Malicious Tools for Cybercriminals: A Cyber Defense Perspective](https://arxiv.org/abs/2411.16642)                                      |                                          **Jailbreak Prompts**&**Cyber Defense**&**AI Security**                                          |
| 24.12 |                                                                                       Google DeepMind                                                                                        |                      NeurIPS 2024 SafeGenAI Workshop                      |                                        [Does Safety Training of LLMs Generalize to Semantically Related Natural Prompts?](https://openreview.net/forum?id=0YdEK1Dnbn)                                        |                                       **Safety Training**&**LLM Generalization**&**Toxic Prompts**                                        |
| 24.12 |                                                                                      Speechmatic, MATS                                                                                       |                                   arxiv                                   |                                                                          [Best-of-N Jailbreaking](https://arxiv.org/abs/2412.03556)                                                                          |                                         **Jailbreaking**&**AI Systems**&**Defense Circumvention**                                         |
| 24.11 |                                                 Uppsala University, The University of Hong Kong, Tencent Inc., Research Institutes of Sweden                                                 |                                   arxiv                                   |                                     [PEFT-as-an-Attack! Jailbreaking Language Models during Federated Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2411.19335)                                     |                                   **PEFT Attack**&**Federated Learning**&**Privacy-Preserving Models**                                    |
| 24.12 |                                                  University of Central Florida, University of Maryland, College Park, Princeton University                                                   |                                   arxiv                                   |                                                   [LIAR: Leveraging Alignment (Best-of-N) to Jailbreak LLMs in Seconds](https://arxiv.org/abs/2412.05232)                                                    |                                   **Jailbreaking**&**Alignment Techniques**&**Reinforcement Learning**                                    |
| 24.12 |                                                                                      Palisade Research                                                                                       |                                   arxiv                                   |                                                         [BadGPT-4o: Stripping Safety Fine-Tuning from GPT Models](https://arxiv.org/abs/2412.05346)                                                          |                                         **Jailbreaking**&**Safety Fine-Tuning**&**LLM Security**                                          |
| 24.12 |                                         Beijing Electronic Science and Technology Institute, AVIC Nanjing Engineering Institute of Aircraft Systems                                          |                                   arxiv                                   |                                             [BAMBA: A Bimodal Adversarial Multi-Round Black-Box Jailbreak Attacker for LVLMs](https://arxiv.org/abs/2412.05892)                                              |                                    **Bimodal Adversarial Attack**&**LVLMs**&**Black-Box Jailbreaking**                                    |
| 24.12 |                                                 Sun Yat-Sen University, Nanyang Technological University, Alibaba Group, Zhejiang University                                                 |                                   arxiv                                   |                                   [Heuristic-Induced Multimodal Risk Distribution Jailbreak Attack for Multimodal Large Language Models](https://arxiv.org/abs/2412.05934)                                   |                              **Multimodal Large Language Models**&**Jailbreak Attack**&**Risk Distribution**                              |
| 24.12 |                        Oregon State University, University of British Columbia, Rutgers University, George Mason University, Pacific Northwest National Laboratories                         |                                   arxiv                                   |                                      [PrisonBreak: Jailbreaking Large Language Models with Fewer Than Twenty-Five Targeted Bit-Rips](https://arxiv.org/abs/2412.07192)                                       |                                     **Jailbreaking**&**Bitwise Corruptions**&**Adversarial Attacks**                                      |
| 24.12 |                                Institute of Information Engineering, Chinese Academy of Sciences, School of Computer Science, Georgia Institute of Technology                                |                                   arxiv                                   |                                                         [Antelope: Potent and Concealed Jailbreak Attack Strategy](https://arxiv.org/abs/2412.08156)                                                         |                                    **Jailbreak Attack**&**Generative Models**&**Adversarial Prompts**                                     |
| 24.12 |                                                       Huazhong University of Science and Technology, Nanyang Technological University                                                        |                                   arxiv                                   |                                                [Model-Editing-Based Jailbreak against Safety-aligned Large Language Models](https://arxiv.org/abs/2412.08201)                                                |                                      **Jailbreak Attack**&**Model Editing**&**Safety-aligned LLMs**                                       |
| 24.12 |                                                                          University of Illinois at Urbana-Champaign                                                                          |                                   arxiv                                   |                                            [ADVWAVE: Stealthy Adversarial Jailbreak Attack Against Large Audio-Language Models](https://arxiv.org/abs/2412.08608)                                            |                                **Adversarial Attack**&**Audio-Language Models**&**Gradient Optimization**                                 |
| 24.12 |                                                                                  University of West Florida                                                                                  |                                   arxiv                                   |                           [Proactive Adversarial Defense: Harnessing Prompt Tuning in Vision-Language Models to Detect Unseen Backdoored Images](https://arxiv.org/abs/2412.08755)                           |                                 **Adversarial Attacks**&**Backdoor Detection**&**Vision-Language Models**                                 |
| 24.12 |                                                                      Soochow University, University of Alberta, Tencent                                                                      |                                COLING 2025                                |                                       [Exploiting the Index Gradients for Optimization-Based Jailbreaking on Large Language Models](https://arxiv.org/abs/2412.09173)                                        |                                  **Jailbreaking**&**Optimization-Based Attack**&**Gradient Information**                                  |
| 24.12 |                                                                                          FAIR, Meta                                                                                          |                                   arxiv                                   |                                                            [AdvPrefix: An Objective for Nuanced LLM Jailbreaks](https://arxiv.org/abs/2412.10321)                                                            |                                       **Jailbreak Objective**&**Prefix-Forcing**&**LLM Alignment**                                        |
| 24.12 |                                                                                       Wuhan University                                                                                       |                                   arxiv                                   |                                                       [Towards Action Hijacking of Large Language Model-based Agent](https://arxiv.org/abs/2412.10807)                                                       |                                    **Action Hijacking**&**Memory Leakage**&**LLM Agent Vulnerability**                                    |
| 24.12 |                                                                                      Sichuan University                                                                                      |                                   arxiv                                   |                            [SpearBot: Leveraging Large Language Models in a Generative-Critique Framework for Spear-Phishing Email Generation](https://arxiv.org/abs/2412.11109)                             |                                **Spear-Phishing**&**Jailbreak Techniques**&**Critique-Based Optimization**                                |
| 24.12 |                                                                              Guangdong University of Technology                                                                              |                                   arxiv                                   |                                                                    [Jailbreaking? One Step Is Enough!](https://arxiv.org/abs/2412.12621)                                                                     |                       **Jailbreak Attack**&**Reverse Embedded Defense Attack (REDA)**&**In-Context Learning (ICL)**                       |
| 24.12 |                                                                      Beijing University of Posts and Telecommunications                                                                      |                                   arxiv                                   |                                        [Crabs: Consuming Resource via Auto-generation for LLM-DoS Attack under Black-box Settings](https://arxiv.org/abs/2412.13879)                                         |                                        **LLM-DoS Attack**&**AutoDoS Algorithm**&**Length Trojan**                                         |
| 24.12 |                                                                                         Cornell Tech                                                                                         |                                   arxiv                                   |                                                               [Adversarial Hubness in Multi-Modal Retrieval](https://arxiv.org/abs/2412.14113)                                                               |                               **Adversarial Hubness**&**Multi-Modal Retrieval**&**Embedding-Based Attacks**                               |
| 24.12 |                                                         National University of Singapore, Harbin Institute of Technology (Shenzhen)                                                          |                          ICML TiFA Workshop 2024                          |                                                 [Suffix Injection and Projected Gradient Descent Can Easily Fool An MLLM](https://arxiv.org/abs/2412.15614)                                                  |                  **Suffix Injection**&**Projected Gradient Descent (PGD)**&**Multi-Modal Large Language Models (MLLM)**                   |
| 24.12 |                                                                  Hong Kong Baptist University, NVIDIA AI Technology Center                                                                   |                                 AAAI 2025                                 |                                          [Meme Trojan: Backdoor Attacks Against Hateful Meme Detection via Cross-Modal Triggers](https://arxiv.org/abs/2412.15503)                                           |                                 **Backdoor Attacks**&**Hateful Meme Detection**&**Cross-Modal Triggers**                                  |
| 24.12 |                                                                                       Fudan University                                                                                       |                                 AAAI 2025                                 |                                      [JailPO: A Novel Black-box Jailbreak Framework via Preference Optimization Against Aligned LLMs](https://arxiv.org/abs/2412.15623)                                      |                                 **Jailbreak Attacks**&**Preference Optimization**&**LLM Vulnerabilities**                                 |
| 24.12 |                                                                                     Zhejiang University                                                                                      |                                   arxiv                                   |                                                          [POEX: Policy Executable Embodied AI Jailbreak Attacks](https://arxiv.org/abs/2412.16633)                                                           |                                    **Jailbreak Attacks**&**Embodied AI**&**Policy Executable Attacks**                                    |
| 24.12 |                                                                           University of Maryland, Baltimore County                                                                           |                                   arxiv                                   |                                 [Human-Readable Adversarial Prompts: An Investigation into LLM Vulnerabilities Using Situational Context](https://arxiv.org/abs/2412.16359)                                  |                            **Adversarial Prompts**&**Language Model Vulnerabilities**&**Situational Context**                             |
| 24.12 |                                                        Tsinghua University, Hefei University of Technology, Shanghai Qi Zhi Institute                                                        |                                   arxiv                                   |                                                   [SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage](https://arxiv.org/abs/2412.15289)                                                   |                                                 **Jailbreak Attacks**&**Assistive Task**                                                  |
| 24.12 |                                                                                            OpenAI                                                                                            |                                   arxiv                                   |                                   [Diverse and Effective Red Teaming with Auto-generated Rewards and Multi-step Reinforcement Learning](https://arxiv.org/abs/2412.18693)                                    |                                   **Red Teaming**&**Reinforcement Learning**&**Diversity Optimization**                                   |
| 24.12 |                                                                                     Safe AI for Humanity                                                                                     |                     NeurIPS 2024 Safe GenAI Workshop                      |                                                                Can Safety Fine-Tuning Be More Principled? Lessons Learned from Cybersecurity                                                                 |                                **LLM Safety Fine-Tuning**&**Cybersecurity Lessons**&**Jailbreak Defense**                                 |
| 25.01 |                                                                                     Squirrel Ai Learning                                                                                     |                                   arxiv                                   |                                                    [LLM-Virus: Evolutionary Jailbreak Attack on Large Language Models](https://arxiv.org/abs/2501.00055)                                                     |                                      **LLM Safety**&**Jailbreak Attack**&**Evolutionary Algorithm**                                       |
| 25.01 |                                                                                      Beihang University                                                                                      |                                   arxiv                                   |                                [A Method for Enhancing the Safety of Large Model Generation Based on Multi-dimensional Attack and Defense](https://arxiv.org/abs/2501.00517)                                 |                                **Safety Alignment**&**Attack-Defense Strategy**&**Large Language Models**                                 |
| 25.01 |                                                                                     Tsinghua University                                                                                      |                                   arxiv                                   |                                     [Image-based Multimodal Models as Intruders: Transferable Multimodal Attacks on Video-based MLLMs](https://arxiv.org/abs/2501.01042)                                     |                                 **Transferable Attacks**&**Multimodal Models**&**Adversarial Robustness**                                 |
| 25.01 |                                                                                Technical University of Munich                                                                                |                                   arxiv                                   |                                       [Self-Instruct Few-Shot Jailbreaking: Decompose the Attack into Pattern and Behavior Learning](https://arxiv.org/abs/2501.07959)                                       |                           **Few-Shot Jailbreaking**&**Adversarial Prompt Engineering**&**Model Vulnerability**                            |
| 25.01 |                                                                                      Beihang University                                                                                      |                                   arxiv                                   |                                                 [Jailbreaking Multimodal Large Language Models via Shuffle Inconsistency](https://arxiv.org/abs/2501.04931)                                                  |                           **Multimodal Large Language Models**&**Jailbreak Attacks**&**Shuffle Inconsistency**                            |
| 25.01 |                                                                      Institute of Software, Chinese Academy of Sciences                                                                      |                                   arxiv                                   |                                         [AUTO-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models](https://arxiv.org/abs/2501.01830)                                          |                                  **Red-Teaming**&**Reinforcement Learning**&**Vulnerability Discovery**                                   |
| 25.01 |                                                                                   The University of Oxford                                                                                   |                                   arxiv                                   |                                                        [Jailbreaking Large Language Models in Infinitely Many Ways](https://arxiv.org/abs/2501.10800)                                                        |                                      **Jailbreaking**&**Infinitely Many Meanings**&**LLM Security**                                       |
| 25.01 |                                                                               Nanyang Technological University                                                                               |                                   arxiv                                   |                                                         [Dagger Behind Smile: Fool LLMs with a Happy Ending Story](https://arxiv.org/abs/2501.13115)                                                         |                                  **Jailbreak Attacks**&**Positive Prompt Exploitation**&**LLM Security**                                  |
| 25.01 |                                                                                      Beihang University                                                                                      |                                   arxiv                                   |                                              [Black-Box Adversarial Attack on Vision-Language Models for Autonomous Driving](https://arxiv.org/abs/2501.13563)                                               |                                 **Vision-Language Models**&**Adversarial Attacks**&**Autonomous Driving**                                 |
| 25.01 |                                                                           University of Illinois Urbana-Champaign                                                                            |                                   arxiv                                   |                                                [LLMs are Vulnerable to Malicious Prompts Disguised as Scientific Language](https://arxiv.org/abs/2501.14073)                                                 |                                   **Jailbreaking LLMs**&**Bias in AI**&**Scientific Language Attacks**                                    |
| 25.01 |                                                                             The Hong Kong Polytechnic University                                                                             |                                   arxiv                                   |                                 [Siren: A Learning-Based Multi-Turn Attack Framework for Simulating Real-World Human Jailbreak Behaviors](https://arxiv.org/abs/2501.14250)                                  |                               **Jailbreaking LLMs**&**Multi-Turn Attacks**&**Adversarial Machine Learning**                               |
| 25.01 |                                                                                         UC Berkeley                                                                                          |                                   arxiv                                   |                                                     [PromptShield: Deployable Detection for Prompt Injection Attacks](https://arxiv.org/abs/2501.15145)                                                      |                                  **Prompt Injection Detection**&**LLM Security**&**Adversarial Attacks**                                  |
| 25.01 |                                                           Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences                                                            |                                   arXiv                                   |                                    [xJailbreak: Representation Space Guided Reinforcement Learning for Interpretable LLM Jailbreaking](https://arxiv.org/abs/2501.16727)                                     |                                **LLM Jailbreaking**&**Reinforcement Learning**&**Representation Learning**                                |
| 25.01 |                                                                               Georgia Institute of Technology                                                                                |                                   arxiv                                   |                                       [Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation](https://arxiv.org/abs/2501.17433v1)                                       |                                          **Harmful Fine-tuning Attack**&**Guardrail Moderation**                                          |
| 25.01 |                                                       Tsinghua University, Beijing Institute of Mathematical Sciences and Applications                                                       |                                   arxiv                                   |                                           [Jailbreaking LLMs‚Äô Safeguard with Universal Magic Words for Text Embedding Models](https://arxiv.org/abs/2501.18280v1)                                            |                                        **Jailbreaking**&**Text Embedding Models**&**Magic Words**                                         |
| 25.01 |                                                                                    Saint Louis University                                                                                    |                                   arxiv                                   |                                                      [DarkMind: Latent Chain-of-Thought Backdoor in Customized LLMs](https://arxiv.org/abs/2501.18617)                                                       |                                       **Chain-of-Thought**&**Backdoor Attack**&**Customized LLMs**                                        |
| 25.01 |                                                                               Institut Polytechnique de Paris                                                                                |                                   arxiv                                   |                                      [The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt Adversarial Attacks on LLMs](https://arxiv.org/abs/2501.18626)                                      |                                    **Task-in-Prompt Attack**&**Adversarial Attacks**&**LLM Security**                                     |
| 25.01 |                                                                                   University of Pittsburgh                                                                                   |                                   arxiv                                   |                                   [Towards Safe AI Clinicians: A Comprehensive Study on Large Language Model Jailbreaking in Healthcare](https://arxiv.org/abs/2501.18632)                                   |                                   **LLM Jailbreaking**&**Healthcare AI Safety**&**Adversarial Attacks**                                   |
| 25.01 |                                                                                    Amazon Bedrock Science                                                                                    |                                   arxiv                                   |                            [Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt Generation for Enhanced LLM Content Moderation](https://arxiv.org/abs/2501.18638)                            |                                **Jailbreak Prompt Generation**&**Content Moderation**&**Graph of Attacks**                                |
| 25.02 |                                                                                     University of Sydney                                                                                     |                                   arxiv                                   |                                               [From Compliance to Exploitation: Jailbreak Prompt Attacks on Multimodal LLMs](https://arxiv.org/abs/2502.00735)                                               |                                **Multimodal LLMs**&**Jailbreak Prompt Attacks**&**Adversarial Robustness**                                |
| 25.02 |                                                                                     Zhejiang University                                                                                      |                                   arxiv                                   |                           [Activation Approximations Can Incur Safety Vulnerabilities Even in Aligned LLMs: Comprehensive Analysis and Defense](https://arxiv.org/abs/2502.00840)                            |                                    **Activation Approximation**&**LLM Safety**&**Security Alignment**                                     |
| 25.02 |                                                                       CISPA Helmholtz Center for Information Security                                                                        |                                   arxiv                                   |                                               [Peering Behind the Shield: Guardrail Identification in Large Language Models](https://arxiv.org/abs/2502.01241)                                               |                                     **LLM Guardrails**&**Adversarial Prompts**&**Security Auditing**                                      |
| 25.02 |                                                                                  University of Pennsylvania                                                                                  |                                   arxiv                                   |                                                                [Adversarial Reasoning at Jailbreaking Time](https://arxiv.org/abs/2502.01633)                                                                |                                **LLM Jailbreaking**&**Adversarial Reasoning**&**Security Vulnerabilities**                                |
| 25.02 |                                                                                    University of Toronto                                                                                     |                                   arxiv                                   |                             [PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation, Negative Demonstration, and Adaptive Sampling](https://arxiv.org/abs/2502.01925)                             |                                     **Many-shot Jailbreaking**&**Adaptive Sampling**&**LLM Security**                                     |
| 25.02 |                                                                             University of Massachusetts Amherst                                                                              |                                   arxiv                                   |                                                              [OVERTHINK: Slowdown Attacks on Reasoning LLMs](https://arxiv.org/abs/2502.02542)                                                               |                                   **Reasoning LLMs**&**Adversarial Attacks**&**Computational Overhead**                                   |
| 25.02 |                                                                                        CSIRO‚Äôs Data61                                                                                        |                                   arxiv                                   |                                             [Large Language Model Adversarial Landscape Through the Lens of Attack Objectives](https://arxiv.org/abs/2502.02960)                                             |                                       **LLM Security**&**Adversarial Attacks**&**Threat Taxonomy**                                        |
| 25.02 |                                                                                       Brown University                                                                                       |                                   arxiv                                   |                                               [SPEAK EASY: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions](https://arxiv.org/abs/2502.04322)                                                |                                  **Jailbreak Attacks**&**LLM Vulnerabilities**&**Multilingual Exploits**                                  |
| 25.02 |                                                                                   The University of Sydney                                                                                   |                                 ICLR 2025                                 |                                                 [Understanding and Enhancing the Transferability of Jailbreaking Attacks](https://arxiv.org/abs/2502.03052)                                                  |                                 **Jailbreaking Attacks**&**LLM Security**&**Adversarial Transferability**                                 |
| 25.02 |                                                                                  National Taiwan University                                                                                  |                            NAACL 2025 Findings                            |                                                                [Jailbreaking with Universal Multi-Prompts](https://arxiv.org/abs/2502.01154)                                                                 |                              **Jailbreaking Attacks**&**Universal Multi-Prompts**&**Adversarial Robustness**                              |
| 25.02 |                                                                                 City University of Hong Kong                                                                                 |                                   arxiv                                   |                                     [A Survey on Backdoor Threats in Large Language Models (LLMs): Attacks, Defenses, and Evaluations](https://arxiv.org/abs/2502.05224)                                     |                                                     **Backdoor Attacks**&**Security**                                                     |
| 25.02 |                                                                                     Tsinghua University                                                                                      |                                   arxiv                                   |                                         [Effective Black-Box Multi-Faceted Attacks Breach Vision Large Language Model Guardrails](https://arxiv.org/abs/2502.05772)                                          |                                **Vision-Language Models (VLLMs)**&**Adversarial Attacks**&**AI Security**                                 |
| 25.02 |                                                                                             AIRI                                                                                             |                                   arxiv                                   |                                                         [Universal Adversarial Attack on Aligned Multimodal LLMs](https://arxiv.org/abs/2502.07987)                                                          |                              **Universal Adversarial Attack**&**Multimodal LLMs**&**Security Vulnerability**                              |
| 25.02 |                                                                         Columbia University, University of Maryland                                                                          |                                   arXiv                                   |                                               [Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks](https://arxiv.org/abs/2502.08586)                                               |                                     **LLM Agents Security**&**Adversarial Attacks**&**Jailbreaking**                                      |
| 25.02 |                                                                                           Scale AI                                                                                           |                                   arxiv                                   |                                                      [LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet](https://arxiv.org/abs/2502.09638)                                                      |                                              **Jailbreaking**&**AI Safety**&**Red Teaming**                                               |
| 25.02 |                                                                       Tsinghua Shenzhen International Graduate School                                                                        |                                   arxiv                                   |                                    [Making Them a Malicious Database: Exploiting Query Code to Jailbreak Aligned Large Language Models](https://arxiv.org/abs/2502.09723)                                    |                                         **Jailbreaking**&**Query Code Attacks**&**LLM Security**                                          |
| 25.02 |                                                                          Technion - Israel Institute of Technology                                                                           |                                   arxiv                                   |                                                 [Enhancing Jailbreak Attacks via Compliance-Refusal-Based Initialization](https://arxiv.org/abs/2502.09755)                                                  |                                      **Jailbreaking**&**Compliance-Refusal**&**Adversarial Attacks**                                      |
| 25.02 |                                                                                      Beihang University                                                                                      |                                   arxiv                                   |                                        [Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models](https://arxiv.org/abs/2502.11054)                                        |                                           **Multi-Turn Jailbreak**&**Reasoning-Driven Attack**                                            |
| 25.02 |                                                                           Guangdong University of Foreign Studies                                                                            |                                 ICLR 2025                                 |                                                       [Injecting Universal Jailbreak Backdoors into LLMs in Minutes](https://arxiv.org/abs/2502.10438)                                                       |                                                          **Jailbreak Backdoor**                                                           |
| 25.02 |                                                                                     Zhejiang University                                                                                      |                                   arxiv                                   |                                        [Rewrite to Jailbreak: Discover Learnable and Transferable Implicit Harmfulness Instruction](https://arxiv.org/abs/2502.11084)                                        |                                                   **Jailbreak**&**Adversarial Attacks**                                                   |
| 25.02 |                                                                                 East China Normal University                                                                                 |                                   arxiv                                   |                                                [CCJA: Context-Coherent Jailbreak Attack for Aligned Large Language Models](https://arxiv.org/abs/2502.11379)                                                 |                                             **Jailbreak Attack**&**Security Vulnerabilities**                                             |
| 25.02 |                                                                        The Chinese University of Hong Kong, Shenzhen                                                                         |                                   arxiv                                   |                                      [BoT: Breaking Long Thought Processes of o1-like Large Language Models through Backdoor Attack](https://arxiv.org/abs/2502.12202)                                       |                                    **Backdoor Attack**&**Chain-of-Thought Reasoning**&**LLM Security**                                    |
| 25.02 |                                                     Beijing University of Posts and Telecommunications, Nanyang Technological University                                                     |                                   arxiv                                   |                                         [DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent](https://arxiv.org/abs/2502.12575)                                          |                                          **LLM Security**&**Backdoor Attacks**&**Agent Safety**                                           |
| 25.02 |                                                                                     University of Zagreb                                                                                     |                                   arxiv                                   |                                            [Automating Prompt Leakage Attacks on Large Language Models Using Agentic Approach](https://arxiv.org/abs/2502.12630)                                             |                                         **Prompt Leakage**&**Agentic Approach**&**LLM Security**                                          |
| 25.02 |                                                                                       Duke University                                                                                        |                                   arxiv                                   | [H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to Jailbreak Large Reasoning Models, Including OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking](https://arxiv.org/abs/2502.12893) |                                **Jailbreaking**&**Chain-of-Thought Reasoning**&**Large Reasoning Models**                                 |
| 25.02 |                                                                                      Peking University                                                                                       |                                   arXiv                                   |                                         [Exploiting Prefix-Tree in Structured Output Interfaces for Enhancing Jailbreak Attacking](https://arxiv.org/abs/2502.13527)                                         |                                        **Jailbreak Attack**&**Structured Output**&**Prefix-Tree**                                         |
| 25.02 |                                                                                      A*STAR, Singapore                                                                                       |                                   arXiv                                   |                                    [CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models](https://arxiv.org/abs/2502.14529)                                    |                                       **Multi-Agent Systems**&**LLM Security**&**Blocking Attacks**                                       |
| 25.02 |                                                                                     University of Oxford                                                                                     |                                   arXiv                                   |                                                         [Fundamental Limitations in Defending LLM Finetuning APIs](https://arxiv.org/abs/2502.14828)                                                         |                                     **Fine-Tuning Attacks**&**LLM Security**&**Pointwise Detection**                                      |
| 25.02 |                                                                                  Michigan State University                                                                                   |                                   arXiv                                   |                                                      [Red-Teaming LLM Multi-Agent Systems via Communication Attacks](https://arxiv.org/abs/2502.14847)                                                       |                                    **Multi-Agent Systems**&**Communication Attacks**&**LLM Security**                                     |
| 25.02 |                                                                           University of Illinois Urbana-Champaign                                                                            |                                   arxiv                                   |                                             [MM-POISONRAG: Disrupting Multimodal RAG with Local and Global Poisoning Attacks](https://arxiv.org/abs/2502.17832)                                              |                                    **Multimodal RAG**&**Knowledge Poisoning**&**Adversarial Attacks**                                     |
| 25.02 |                                                                                     Tsinghua University                                                                                      |                                   arxiv                                   |                               [Beyond the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models](https://arxiv.org/abs/2502.19883)                               |                                    **Jailbreak Attacks**&**Small Language Models**&**Security Risks**                                     |
| 25.02 |                                                                                     Stanford University                                                                                      |                                   arxiv                                   |                                         [No, of course I can! Refusal Mechanisms Can Be Exploited Using Harmless Fine-Tuning Data](https://arxiv.org/abs/2502.19537)                                         |                                   **Refusal Mechanisms**&**Fine-Tuning Attacks**&**Jailbreaking LLMs**                                    |
| 25.02 |                                                                               Nanyang Technological University                                                                               |                                   arxiv                                   |                                 [Detecting Offensive Memes with Social Biases in Singapore Context Using Multimodal Large Language Models](https://arxiv.org/abs/2502.18101)                                 |                                        **Multimodal LLM**&**Content Moderation**&**Online Safety**                                        |
| 25.02 |                                                                          Indian Institute of Technology Gandhinagar                                                                          |                                   arxiv                                   |                                        [Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs](https://arxiv.org/abs/2502.16901)                                         |                           **Cross-lingual Backdoor Attacks**&**Multilingual LLMs**&**Security Vulnerabilities**                           |
| 25.02 |                                                                                      Peking University                                                                                       |                                   arxiv                                   |                                                  [Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming](https://arxiv.org/abs/2502.16109)                                                  |                                           **Red Teaming**&**Prompt Evolution**&**LLM Security**                                           |
| 25.02 |                                                                         Shanghai Artificial Intelligence Laboratory                                                                          |                                   arxiv                                   |                                         [A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos](https://arxiv.org/abs/2502.15806)                                          |                                    **Jailbreak Attacks**&**Large Reasoning Models**&**Security Risks**                                    |
| 25.02 |                                                                                Pennsylvania State University                                                                                 |                                   arxiv                                   |                                                            [Foot-In-The-Door: A Multi-turn Jailbreak for LLMs](https://arxiv.org/abs/2502.19820)                                                             |                                 **Jailbreak Attacks**&**Multi-turn Exploitation**&**LLM Vulnerabilities**                                 |
| 25.02 |                                                                                     Amazon Web Services                                                                                      |                                NAACL 2025                                 |                            [TURBOFUZZLLM: Turbocharging Mutation-based Fuzzing for Effectively Jailbreaking Large Language Models in Practice](https://arxiv.org/abs/2502.18504)                             |                                 **Jailbreaking LLMs**&**Mutation-based Fuzzing**&**Security Evaluation**                                  |
| 25.02 |                                                                                   J.P. Morgan AI Research                                                                                    | AAAI'25 Workshop on Preventing and Detecting LLM Generated Misinformation |                                                   [Toward Breaking Watermarks in Distortion-free Large Language Models](https://arxiv.org/abs/2502.18608)                                                    |                                       **Watermark Breaking**&**LLM Security**&**Spoofing Attacks**                                        |
| 25.02 |                                                                                     Zhejiang University                                                                                      |                           USENIX Security 2025                            |                                         [Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models](https://arxiv.org/abs/2502.18943)                                         |                              **Membership Inference Attack**&**Label-Only Attack**&**Privacy Risks in LLMs**                              |
| 25.02 |                                                                                 Peking University, Ant Group                                                                                 |                                   arXiv                                   |                          [Efficient Jailbreaking of Large Models by Freeze Training: Lower Layers Exhibit Greater Sensitivity to Harmful Content](https://arxiv.org/abs/2502.20952)                          |                                            **Jailbreaking**&**Fine-Tuning**&**Model Security**                                            |
| 25.02 |                                                                                   Sogang University, KAIST                                                                                   |                                   arXiv                                   |                          [The RAG Paradox: A Black-Box Attack Exploiting Unintentional Vulnerabilities in Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2502.20995)                          |                              **Retrieval-Augmented Generation**&**Black-Box Attack**&**Document Poisoning**                               |
| 25.02 |                                                 The Hong Kong University of Science and Technology (Guangzhou), The University of Hong Kong                                                  |                                   arXiv                                   |                                            [FC-Attack: Jailbreaking Large Vision-Language Models via Auto-Generated Flowcharts](https://arxiv.org/abs/2502.21059)                                            |                                    **Jailbreaking**&**Vision-Language Models**&**Adversarial Attacks**                                    |
| 25.03 |                                          Institute of Computing Technology, Chinese Academy of Sciences, University of Chinese Academy of Sciences                                           |                                   arXiv                                   |                                             [From Benign to Toxic: Jailbreaking the Language Model via Adversarial Metaphors](https://arxiv.org/abs/2503.00038)                                              |                                   **Jailbreaking**&**Adversarial Metaphors**&**Large Language Models**                                    |
| 25.03 |                                                                          Duke University, Johns Hopkins University                                                                           |                                   arXiv                                   |                                                 [Jailbreaking Safeguarded Text-to-Image Models via Large Language Models](https://arxiv.org/abs/2503.01839)                                                  |                                    **Jailbreaking**&**Text-to-Image Models**&**Adversarial Prompting**                                    |
| 25.03 |                                                                                     Tsinghua University                                                                                      |                                   arXiv                                   |                         [Guiding not Forcing: Enhancing the Transferability of Jailbreaking Attacks on LLMs via Removing Superfluous Constraints](https://arxiv.org/abs/2503.01865)                          |                                   **Jailbreaking**&**Transferability**&**Gradient-Based Optimization**                                    |
| 25.03 |                                                                                 University of Chicago, Meta                                                                                  |                                   arXiv                                   |                                  [UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically Hijacking Their Own Reasoning](https://arxiv.org/abs/2503.01908)                                  |                                          **Red Teaming**&**LLM Agents**&**Adversarial Attacks**                                           |
| 25.03 |                                                                             University of Massachusetts Amherst                                                                              |                                   arXiv                                   |                                                             [LLM Misalignment via Adversarial RLHF Platforms](https://arxiv.org/abs/2503.03039)                                                              |                                      **RLHF Security**&**LLM Misalignment**&**Adversarial Attacks**                                       |
| 25.03 |                                                              University of California, Davis, University of Southern California                                                              |                                 ICLR 2025                                 |                                                           [BADJUDGE: Backdoor Vulnerabilities of LLM-as-a-Judge](https://arxiv.org/abs/2503.00596)                                                           |                                      **LLM-as-a-Judge**&**Backdoor Attacks**&**Evaluation Security**                                      |
| 25.03 |                                                                                       AIM Intelligence                                                                                       |                                   arxiv                                   |                                     [One-Shot is Enough: Consolidating Multi-Turn Attacks into Efficient Single-Turn Prompts for LLMs](https://arxiv.org/abs/2503.04856)                                     |                                       **Jailbreak Attack**&**Prompt Engineering**&**LLM Security**                                        |
| 25.03 |                                                                                      McGill University                                                                                       |                                   arxiv                                   |                                                        [SAFEARENA: Evaluating the Safety of Autonomous Web Agents](https://arxiv.org/abs/2503.04957)                                                         |                                   **Autonomous Web Agents**&**Safety Evaluation**&**Jailbreak Attacks**                                   |
| 25.03 |                                                                                          Microsoft                                                                                           |                                   arxiv                                   |                                                             [Jailbreaking is (Mostly) Simpler Than You Think](https://arxiv.org/abs/2503.05264)                                                              |                                        **Jailbreak Attack**&**Context Injection**&**LLM Security**                                        |
| 25.03 |                                                                      Beijing University of Posts and Telecommunications                                                                      |                                   arxiv                                   |                            [CtrlRAG: Black-box Adversarial Attacks Based on Masked Language Models in Retrieval-Augmented Language Generation](https://arxiv.org/abs/2503.06950)                             |                            **Retrieval-Augmented Generation**&**Black-box Attack**&**Masked Language Models**                             |
| 25.03 |                                                                                     360 AI Security Lab                                                                                      |                                   arxiv                                   |                                                 [Utilizing Jailbreak Probability to Attack and Safeguard Multimodal LLMs](https://arxiv.org/abs/2503.06989)                                                  |                                 **Multimodal LLMs**&**Jailbreak Probability**&**Adversarial Robustness**                                  |
| 25.03 |                                                                                     Zhejiang University                                                                                      |                                   arxiv                                   |                                                [Dialogue Injection Attack: Jailbreaking LLMs through Context Manipulation](https://arxiv.org/abs/2503.08195)                                                 |                                       **Jailbreaking**&**Black-box Attack**&**Dialogue Injection**                                        |
| 25.03 |                                                                                         Independent                                                                                          |                                   arxiv                                   |                                                   [JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing](https://arxiv.org/abs/2503.08990)                                                    |                                               **Jailbreaking**&**Fuzzing**&**Red-Teaming**                                                |
| 25.03 |                                                                           Home Team Science and Technology Agency                                                                            |                                   arxiv                                   |                                     [Probing Latent Subspaces in LLM for AI Security: Identifying and Manipulating Adversarial States](https://arxiv.org/abs/2503.09066)                                     |                                   **LLM Security**&**Latent Representation**&**Jailbreak Intervention**                                   |
| 25.03 |                                                                                         Intology AI                                                                                          |                        TrustworhtyLLM @ ICLR 2025                         |                                           [SIEGE: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search](https://arxiv.org/abs/2503.10619)                                            |                                **Multi-Turn Jailbreaking**&**Tree Search**&**Partial Compliance Tracking**                                |
| 25.03 |                                                                             University of Maryland, College Park                                                                             |                                NAACL 2025                                 |                             [PoisonedParrot: Subtle Data Poisoning Attacks to Elicit Copyright-Infringing Content from Large Language Models](https://arxiv.org/abs/2503.07697)                              |                                         **Data Poisoning**&**Copyright Evasion**&**LLM Security**                                         |
| 25.03 |                                                                                   University of Liverpool                                                                                    |                                   arxiv                                   |                                            [TAIJI: Textual Anchoring for Immunizing Jailbreak Images in Vision Language Models](https://arxiv.org/abs/2503.10872)                                            |                                  **Vision Language Models**&**Jailbreak Defense**&**Textual Anchoring**                                   |
| 25.03 |                                                                                     Southeast University                                                                                     |                                   arxiv                                   |                                 [Tit-for-Tat: Safeguarding Large Vision-Language Models Against Jailbreak Attacks via Adversarial Defense](https://arxiv.org/abs/2503.11619)                                 |                                 **Vision-Language Models**&**Jailbreak Defense**&**Adversarial Training**                                 |
| 25.03 |                                                                                     Southeast University                                                                                     |                                   arxiv                                   |                               [Making Every Step Effective: Jailbreaking Large Vision-Language Models Through Hierarchical KV Equalization](https://arxiv.org/abs/2503.11750)                                |                               **Vision-Language Models**&**Jailbreak Attack**&**Adversarial Optimization**                                |
| 25.03 |                                                                                     University of Sydney                                                                                     |                                   arxiv                                   |                                                        [A Framework to Assess Multilingual Vulnerabilities of LLMs](https://arxiv.org/abs/2503.13081)                                                        |                                    **Multilingual Evaluation**&**Jailbreak Detection**&**LLM Safety**                                     |
| 25.03 |                                                                           University of Illinois Urbana-Champaign                                                                            |                                   arxiv                                   |                                                  [AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration](https://arxiv.org/abs/2503.15754)                                                  |                                 **Automated Red Teaming**&**LLM Security**&**Attack Strategy Discovery**                                  |
| 25.03 |                                                                        Huazhong University of Science and Technology                                                                         |                                 CVPR 2025                                 |                                               [BadToken: Token-level Backdoor Attacks to Multi-modal Large Language Models](https://arxiv.org/abs/2503.16023)                                                |                                      **Backdoor Attack**&**Multi-modal LLMs**&**Token Manipulation**                                      |
| 25.03 |                                                                                     Independent Research                                                                                     |                                 IEEE CAI                                  |                             [Temporal Context Awareness: A Defense Framework Against Multi-turn Manipulation Attacks on Large Language Models](https://arxiv.org/abs/2503.15560)                             |                                    **LLM Security**&**Multi-turn Attacks**&**Temporal Risk Modeling**                                     |
| 25.03 |                                                          The University of Sydney, CSIRO Data61, The University of New South Wales                                                           |                                   arxiv                                   |                                                 [Jailbreaking the Non-Transferable Barrier via Test-Time Data Disguising](https://arxiv.org/abs/2503.17198)                                                  |                                   **Non-Transferable Learning**&**Model Security**&**Test-Time Attack**                                   |
| 25.03 |                                                                           Tianjin University, Huawei Technologies                                                                            |                                   arxiv                                   |                                                       [Metaphor-based Jailbreaking Attacks on Text-to-Image Models](https://arxiv.org/abs/2503.17987)                                                        |                                 **Jailbreaking attacks**&**Text-to-Image Models**&**Adversarial Prompt**                                  |
| 25.03 |                          University of Waterloo, National University of Singapore, University of California Merced, University of Alberta, University of Queensland                          |                                   arxiv                                   |                                       [MIRAGE: Multimodal Immersive Reasoning and Guided Exploration for Red-Team Jailbreak Attacks](https://arxiv.org/abs/2503.19134)                                       |                                  **Multimodal Jailbreaking**&**Visual Storytelling**&**Role Immersion**                                   |
| 25.03 |                                                                                 National Central University                                                                                  |                                   arxiv                                   |                                             [Iterative Prompting with Persuasion Skills in Jailbreaking Large Language Models](https://arxiv.org/abs/2503.20320)                                             |                                       **Jailbreaking**&**Iterative Prompting**&**Persuasion Skill**                                       |
| 25.03 |                                                                              KTH Royal Institute of Technology                                                                               |                                   arxiv                                   |                        [Prompt, Divide, and Conquer: Bypassing Large Language Model Safety Filters via Segmented and Distributed Prompt Processing](https://arxiv.org/abs/2503.21598)                        |                                          **Jailbreaking**&**Distributed Prompting**&**LLM Jury**                                          |
| 25.03 |                                                                 NAVER Cloud, KAIST, Republic of Korea Naval Academy, AITRICS                                                                 |                                 CVPR 2025                                 |                                        [Playing the Fool: Jailbreaking LLMs and Multimodal LLMs with Out-of-Distribution Strategy](https://arxiv.org/abs/2503.20823)                                         |                                    **Jailbreaking**&**Out-of-Distribution Attack**&**Multimodal LLMs**                                    |
| 25.03 |                                                                               ICT, Chinese Academy of Sciences                                                                               |                                   arxiv                                   |                                  [Output Constraints as Attack Surface: Exploiting Structured Generation to Bypass LLM Safety Mechanisms](https://arxiv.org/abs/2503.24191)                                  |                                       **Jailbreak Attacks**&**Constrained Decoding**&**LLM Safety**                                       |
| 25.04 |                                                                         University of North Carolina at Chapel Hill                                                                          |                                   arxiv                                   |                                       [Agents Under Siege: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks](https://arxiv.org/abs/2504.00218)                                       |                       **Multi-Agent LLMs**&**Jailbreak Attacks**&**Permutation-Invariant Adversarial Optimization**                       |
| 25.04 |                                                                             University of Massachusetts Amherst                                                                              |                                   arxiv                                   |                                                         [Multilingual and Multi-Accent Jailbreaking of Audio LLMs](https://arxiv.org/abs/2504.01094)                                                         |                               **Audio LLMs**&**Multilingual Jailbreak**&**Adversarial Audio Perturbations**                               |
| 25.04 |                                                                                       Wuhan University                                                                                       |                                   arxiv                                   |                                         [PiCo: Jailbreaking Multimodal Large Language Models via Pictorial Code Contextualization](https://arxiv.org/abs/2504.01444)                                         |                                  **Adversarial Attacks**&**Multimodal LLMs**&**Visual Prompt Jailbreak**                                  |
| 25.04 |                                                                                              ‚Äî                                                                                               |                                   arxiv                                   |                                                   [Evolving Security in LLMs: A Study of Jailbreak Attacks and Defenses](https://arxiv.org/abs/2504.02080)                                                   |                                       **Jailbreak Attacks**&**LLM Defense**&**Security Evaluation**                                       |
| 25.04 |                                                                                    University of Granada                                                                                     |                                   arxiv                                   |                                              [A DOMAIN-BASED TAXONOMY OF JAILBREAK VULNERABILITIES IN LARGE LANGUAGE MODELS](https://arxiv.org/abs/2504.04976)                                               |                                                **Jailbreak**&**LLMs**&**Model alignment**                                                 |
| 25.04 |                                                                                             CUHK                                                                                             |                                   arxiv                                   |                                                             [Emerging Cyber Attack Risks of Medical AI Agents](https://arxiv.org/abs/2504.03759)                                                             |                                       **Medical AI Agents**&**Cybersecurity**&**Prompt Injection**                                        |
| 25.04 |                                                                              University of Southern California                                                                               |                                   arxiv                                   |                                               [JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language Model](https://arxiv.org/abs/2504.03770)                                                |                                **Jailbreak Detection**&**Vision-Language Models**&**Test-Time Adaptation**                                |
| 25.04 |                                                                                      Nankai University                                                                                       |                                   arxiv                                   |                                                    [Practical Poisoning Attacks against Retrieval-Augmented Generation](https://arxiv.org/abs/2504.03957)                                                    |                                 **Retrieval-Augmented Generation**&**Poisoning Attacks**&**LLM Security**                                 |
| 25.04 |                                                                                  Xi‚Äôan Jiaotong University                                                                                   |                                   arxiv                                   |                                [Separator Injection Attack: Uncovering Dialogue Biases in Large Language Models Caused by Role Separators](https://arxiv.org/abs/2504.05689)                                 |                                        **Prompt Injection**&**Dialogue Bias**&**Role Separators**                                         |
| 25.04 |                                                                                     Shanghai University                                                                                      |                                   arxiv                                   |                                                 [ShadowCoT: Cognitive Hijacking for Stealthy Reasoning Backdoors in LLMs](https://arxiv.org/abs/2504.05605)                                                  |                                      **Reasoning Backdoors**&**Chain-of-Thought**&**LLM Robustness**                                      |
| 25.04 |                                                                          Shanghai University of Engineering Science                                                                          |                                   arxiv                                   |                                                     [Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking](https://arxiv.org/abs/2504.05652)                                                      |                                    **Jailbreak**&**Defense Threshold Decay**&**Adversarial Reasoning**                                    |
| 25.04 |                                                                                            Amazon                                                                                            |                      TrustNLP Workshop @ NAACL 2025                       |                                                         [Multi-lingual Multi-turn Automated Red Teaming for LLMs](https://arxiv.org/abs/2504.03174)                                                          |                                **Automated Red Teaming**&**Multi-lingual Safety**&**Multi-turn Jailbreak**                                |
| 25.04 |                                                                                    Sun Yat-Sen University                                                                                    |                                 CVPR 2025                                 |                                         [Mind the Trojan Horse: Image Prompt Adapter Enabling Scalable and Deceptive Jailbreaking](https://arxiv.org/abs/2504.05838)                                         |                              **Image Prompt Adapter**&**Hijacking Attack**&**T2I Diffusion Model Security**                               |
| 25.04 |                                                                                      Tongji University                                                                                       |                                SIGIR 2025                                 |                      [PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented Generation in Large Language Models via Bilevel Optimization](https://arxiv.org/abs/2504.07717)                       |                             **Retrieval-Augmented Generation**&**Poisoning Attacks**&**Bilevel Optimization**                             |
| 25.04 |                                                                               National University of Singapore                                                                               |                            ICLR 2025 Workshop                             |                                                    [GENESHIFT: IMPACT OF DIFFERENT SCENARIO SHIFT ON JAILBREAKING LLM](https://arxiv.org/abs/2504.08104)                                                     |                                         **Jailbreaking**&**Black-box Attack**&**Scenario Shift**                                          |
| 25.04 |                                                                                Hefei University of Technology                                                                                |                                   arxiv                                   |                                 [StruPhantom: Evolutionary Injection Attacks on Black-Box Tabular Agents Powered by Large Language Models](https://arxiv.org/abs/2504.09841)                                 |                                  **Indirect Prompt Injection**&**LLM Agents**&**Tabular Data Security**                                   |
| 25.04 |                                                                                        GSA - FedRAMP                                                                                         |                                   arxiv                                   |                                                 [Demo: ViolentUTF as An Accessible Platform for Generative AI Red Teaming](https://arxiv.org/abs/2504.10603)                                                 |                                        **Red Teaming**&**LLM Evaluation**&**AI Security Platform**                                        |
| 25.04 |                                                                                          ETH Zurich                                                                                          |                                   arxiv                                   |                                                        [The Jailbreak Tax: How Useful are Your Jailbreak Outputs?](https://arxiv.org/abs/2504.10694)                                                         |                                        **Jailbreak Tax**&**LLM Alignment**&**Utility Evaluation**                                         |
| 25.04 |                                                                                      Xidian University                                                                                       |                                   arxiv                                   |                                               [Token-Level Constraint Boundary Search for Jailbreaking Text-to-Image Models](https://arxiv.org/abs/2504.11106)                                               |                             **Text-to-Image model**&**jailbreak attack**&**constraint optimization problem**                              |
| 25.04 |                                                                                           Mindgard                                                                                           |                                   arxiv                                   |                                                   [Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails](https://arxiv.org/abs/2504.11168)                                                   |                                      **Prompt Injection**&**Jailbreak Detection**&**LLM Guardrails**                                      |
| 25.04 |                                                                             The Hong Kong Polytechnic University                                                                             |                                   arxiv                                   |                                                 [Exploring Backdoor Attack and Defense for LLM-empowered Recommendations](https://arxiv.org/abs/2504.11182)                                                  |                                        **Backdoor Attack**&**LLM-based RecSys**&**Poison Scanner**                                        |
| 25.04 |                                                                               Case Western Reserve University                                                                                |                                   arxiv                                   |                                               [GraphAttack: Exploiting Representational Blindspots in LLM Safety Mechanisms](https://arxiv.org/abs/2504.13052)                                               |                                       **LLM Safety**&**Jailbreak Prompt**&**Semantic Graph Attack**                                       |
| 25.04 |                                                                            University of California, Los Angeles                                                                             |                                   arxiv                                   |                                                 [X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents](https://arxiv.org/abs/2504.13203)                                                 |                                 **Jailbreak Attacks**&**Multi-Turn Red-Teaming**&**LLM Safety Alignment**                                 |
| 25.04 |                                                                                China Agricultural University                                                                                 |                                   arxiv                                   |                                   [BADAPEX: Backdoor Attack Based on Adaptive Optimization Mechanism of Black-Box Large Language Models](https://arxiv.org/abs/2504.13775)                                   |                                      **Backdoor Attack**&**Black-box LLMs**&**Prompt Optimization**                                       |
| 25.04 |                                                                                  Xi‚Äôan Jiaotong University                                                                                   |                                   arxiv                                   |                               [Breaking the Prompt Wall (I): A Real-World Case Study of Attacking ChatGPT via Lightweight Prompt Injection](https://arxiv.org/abs/2504.16125)                                |                                    **Prompt Injection**&**ChatGPT Security**&**Adversarial Prompting**                                    |
| 25.04 |                                                                          National University of Defense Technology                                                                           |                                   arxiv                                   |                                    [Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection](https://arxiv.org/abs/2504.16429)                                     |                           **Retrieval-Augmented Code Generation**&**Software Security**&**Knowledge Injection**                           |
| 25.04 |                                                                                     Shandong University                                                                                      |                                   arxiv                                   |                                         [Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate](https://arxiv.org/abs/2504.16489)                                          |                                     **Multi-Agent Debate**&**Jailbreak Attacks**&**Prompt Rewriting**                                     |
| 25.04 |                                                                                    University of Virginia                                                                                    |                                   arxiv                                   |                                           [Steering the CensorShip: Uncovering Representation Vectors for LLM ‚ÄúThought‚Äù Control](https://arxiv.org/abs/2504.17130)                                           |                           **Censorship Steering**&**Representation Engineering**&**Refusal‚ÄìCompliance Vector**                            |
| 25.04 |                                                                                 East China Normal University                                                                                 |                                   arxiv                                   |                            [Unified attacks to large language model watermarks: spoofing and scrubbing in unauthorized knowledge distillation](https://arxiv.org/abs/2504.17480)                             |                               **LLM Watermark**&**Knowledge Distillation**&**Spoofing & Scrubbing Attacks**                               |
| 25.04 |                                                                                    Independent Researcher                                                                                    |                                   arxiv                                   |                                              [Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections](https://arxiv.org/abs/2504.18333)                                              |                                      **Prompt Injection**&**Adversarial Attacks**&**LLM-as-a-Judge**                                      |
| 25.04 |                                                                        Hong Kong University of Science and Technology                                                                        |                                   arxiv                                   |                                [BadMoE: Backdooring Mixture-of-Experts LLMs via Optimizing Routing Triggers and Infecting Dormant Experts](https://arxiv.org/abs/2504.18598)                                 |                                      **Backdoor Attack**&**Mixture-of-Experts LLMs**&**AI Security**                                      |
| 25.04 |                                                                                      Cardiff University                                                                                      |                                   arxiv                                   |                                                [Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs](https://arxiv.org/abs/2504.19019)                                                |                                 **Adversarial Prompting**&**Black-Box Jailbreaking**&**Graph Reasoning**                                  |
| 25.04 |                                                                        Huazhong University of Science and Technology                                                                         |                                   arxiv                                   |                                                         [Prompt Injection Attack to Tool Selection in LLM Agents](https://arxiv.org/abs/2504.19793)                                                          |                                      **Prompt Injection**&**Tool Selection**&**LLM Agent Security**                                       |
| 25.04 |                                                                                          Dreadnode                                                                                           |                                   arxiv                                   |                                                                [The Automation Advantage in AI Red Teaming](https://arxiv.org/abs/2504.19855)                                                                |                                   **AI Red Teaming**&**Automation vs Manual Attacks**&**LLM Security**                                    |
| 25.04 |                                                                               University of California, Merced                                                                               |                                   arxiv                                   |                               [Token-Efficient Prompt Injection Attack: Provoking Cessation in LLM Reasoning via Adaptive Token Compression](https://arxiv.org/abs/2504.20493)                               |                                   **Prompt Injection**&**Reasoning Interruption**&**Token Compression**                                   |
| 25.04 |                                                                                      No Institute Given                                                                                      |                                   arxiv                                   |                                                [Prefill-Based Jailbreak: A Novel Approach of Bypassing LLM Safety Boundary](https://arxiv.org/abs/2504.21038)                                                |                                      **Jailbreak Attack**&**Prefill-based Attack**&**LLM Security**                                       |
| 25.04 |                                                                          Binjiang Institute of Zhejiang University                                                                           |                                   arxiv                                   |                                            [NeuRel-Attack: Neuron Relearning for Safety Disalignment in Large Language Models](https://arxiv.org/abs/2504.21053)                                             |                                      **Jailbreak Attack**&**Neuron Relearning**&**Safety Alignment**                                      |
| 25.04 |                                                                      Beijing University of Posts and Telecommunications                                                                      |                                   arxiv                                   |                     [Hoist with His Own Petard: Inducing Guardrails to Facilitate Denial-of-Service Attacks on Retrieval-Augmented Generation of LLMs](https://arxiv.org/abs/2504.21680)                     |                                            **RAG**&**Jailbreak Prompt**&**Denial-of-Service**                                             |
| 25.04 |                                                                                     University of Pavia                                                                                      |                                   arxiv                                   |                                                   [XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs](https://arxiv.org/abs/2504.21700)                                                   |                                    **Jailbreak Attack**&**Explainable AI**&**White-box Manipulation**                                     |
| 25.04 |                                                                                      Nankai University                                                                                       |                                 WWW 2025                                  |                                                     [Traceback of Poisoning Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2504.21668)                                                     |                                           **RAG**&**Poisoning Attacks**&**Traceback Forensics**                                           |
| 25.05 |                                                                                    University of Toronto                                                                                     |                                   arxiv                                   |                                                             [Red Teaming Large Language Models for Healthcare](https://arxiv.org/abs/2505.00467)                                                             |                                   **Healthcare Red Teaming**&**Clinical Safety**&**LLM Vulnerability**                                    |
| 25.05 |                                                                              University of Texas at San Antonio                                                                              |                                   arxiv                                   |                               [CAMOUFLAGE: Exploiting Misinformation Detection Systems Through LLM-driven Adversarial Claim Transformation](https://arxiv.org/abs/2505.01900)                                |                              **Adversarial Attacks**&**Misinformation Detection**&**Large Language Models**                               |
| 25.05 |                                                                   University of Electronic Science and Technology of China                                                                   |                                   arxiv                                   |                                                [BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models](https://arxiv.org/abs/2505.03501)                                                 |                                    **Backdoor Attack**&**Multilingual LLMs**&**Adversarial Training**                                     |
| 25.05 |                                                                                  Universidad de Concepci√≥n                                                                                   |                                   arxiv                                   |                                        [A Proposal for Evaluating the Operational Risk for ChatBots based on Large Language Models](https://arxiv.org/abs/2505.04784)                                        |                                       **Operational Risk**&**Chatbot Security**&**LLM Evaluation**                                        |
| 25.05 |                                                                                    Independent Researcher                                                                                    |                                   arxiv                                   |                          [Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt Injection and Jailbreak Vulnerabilities in LLMs](https://arxiv.org/abs/2505.04806)                          |                                          **Prompt Injection**&**Jailbreaking**&**LLM Security**                                           |
| 25.05 |                                                                                    University at Buffalo                                                                                     |                                   arxiv                                   |                                       [System Prompt Poisoning: Persistent Attacks on Large Language Models Beyond User Injection](https://arxiv.org/abs/2505.06493v1)                                       |                                   **System Prompt Poisoning**&**LLM Security**&**Adversarial Attacks**                                    |
| 25.05 |                                                                                 Chinese Academy of Sciences                                                                                  |                                   arxiv                                   |                                      [POISONCRAFT: Practical Poisoning of Retrieval-Augmented Generation for Large Language Models](https://arxiv.org/abs/2505.06579v1)                                      |                             **Retrieval-Augmented Generation**&**Data Poisoning**&**Language Model Security**                             |
| 25.05 |                                                                               Beijing Institute of Technology                                                                                |                                   arxiv                                   |                                              [Practical Reasoning Interruption Attacks on Reasoning Large Language Models](https://arxiv.org/abs/2505.06643v1)                                               |                              **Reasoning Interruption Attack**&**Reasoning Token Overflow**&**LLM Security**                              |
| 25.05 |                                                                                    University of Virginia                                                                                    |                                   arxiv                                   |                                          [Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety](https://arxiv.org/abs/2505.06843v1)                                           |                              **Fine-tuning Vulnerabilities**&**LLM Safety Alignment**&**Outlier Detection**                               |
| 25.05 |                                                                                      Xidian University                                                                                       |                                   arxiv                                   |                              [One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models](https://arxiv.org/abs/2505.07167v1)                               |                                   **Jailbreak Defense**&**Shallow Safety Alignment**&**Trigger Token**                                    |
| 25.05 |                                                                              Ben Gurion University of the Negev                                                                              |                                   arxiv                                   |                                                          [Dark LLMs: The Growing Threat of Unaligned AI Models](https://arxiv.org/abs/2505.10066v1)                                                          |                                                 **Jailbreak**&**Dark LLMs**&**AI Safety**                                                 |
| 25.05 |                                                                                    Stony Brook University                                                                                    |                                   arxiv                                   |                                                      [AutoRAN: Weak-to-Strong Jailbreaking of Large Reasoning Models](https://arxiv.org/abs/2505.10846)                                                      |                                  **Jailbreak Attack**&**Large Reasoning Models**&**Prompt Engineering**                                   |
| 25.05 |                                                                   University of Electronic Science and Technology of China                                                                   |                                   arxiv                                   |                                                   [MPMA: Preference Manipulation Attack Against Model Context Protocol](https://arxiv.org/abs/2505.11154)                                                    |                         **Model Context Protocol**&**Preference Manipulation Attack**&**Tool Selection Security**                         |
| 25.05 |                                                                                       Duke University                                                                                        |                                   arxiv                                   |                                              [ENVINJECTION: Environmental Prompt Injection Attack to Multi-Modal Web Agents](https://arxiv.org/abs/2505.11717)                                               |                                 **Prompt Injection**&**Multi-modal Web Agent**&**Adversarial Robustness**                                 |
| 25.05 |                                                                                       Wuhan University                                                                                       |                                   arxiv                                   |                                                      [JULI: Jailbreak Large Language Models by Self-Introspection](https://arxiv.org/abs/2505.11790v2)                                                       |                                      **LLM Jailbreak**&**Log Probability Manipulation**&**BiasNet**                                       |
| 25.05 |                                                                                     University of Sydney                                                                                     |                                   arxiv                                   |                                   [The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models](https://arxiv.org/abs/2505.12287)                                    |                                  **Multilingual Jailbreak**&**Closed-Source LLMs**&**Prompt Injection**                                   |
| 25.05 |                                                                      The Hong Kong University of Science and Technology                                                                      |                                   arxiv                                   |                                                        [IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2505.12442)                                                        |                                      **LLM Multi-Agent Systems**&**IP Leakage**&**Prompt Injection**                                      |
| 25.05 |                                                                                   University of Minnesota                                                                                    |                                   arxiv                                   |                                                               [A Survey of Attacks on Large Language Models](https://arxiv.org/abs/2505.12567)                                                               |                                          **LLM Security**&**Jailbreaking**&**Prompt Injection**                                           |
| 25.05 |                                                                                   University of Edinburgh                                                                                    |                                   arxiv                                   |                                               [Evaluating the Efficacy of LLM Safety Solutions: The Palit Benchmark Dataset](https://arxiv.org/abs/2505.13028)                                               |                                           **LLM Security**&**Prompt Injections**&**Jailbreaks**                                           |
| 25.05 |                                                                                             None                                                                                             |                                   arxiv                                   |                                       [Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks](https://arxiv.org/abs/2505.13348)                                        |                                      **LLM-as-a-Judge**&**Prompt Injection**&**Adversarial Attacks**                                      |
| 25.05 |                                                                                     Pengcheng Laboratory                                                                                     |                                   arxiv                                   |                                             [AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models](https://arxiv.org/abs/2505.14103)                                             |                                     **Audio Jailbreak**&**End-to-End LALMs**&**Over-the-Air Attack**                                      |
| 25.05 |                                                                                   Universit√© Paris-Saclay                                                                                    |                                   arxiv                                   |                                     [‚ÄúHaet Bhasha aur Diskrimineshun‚Äù: Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs](https://arxiv.org/abs/2505.14226)                                     |                                         **Red-Teaming**&**Phonetic Perturbation**&**Code-Mixing**                                         |
| 25.05 |                                                                                   University of Cambridge                                                                                    |                                   arxiv                                   |                                                [Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs](https://arxiv.org/abs/2505.14286)                                                |                                   **Acoustic Adversarial Attack**&**Speech-LLM**&**Selective Control**                                    |
| 25.05 |                                                                                       Henan University                                                                                       |                                   arxiv                                   |                                               [Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion](https://arxiv.org/abs/2505.14316)                                               |                                    **Jailbreak Attack**&**Prompt Obfuscation**&**Security Benchmark**                                     |
| 25.05 |                                                                                          LMU Munich                                                                                          |                                   arxiv                                   |                                           [Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs](https://arxiv.org/abs/2505.14368)                                           |                                      **Prompt Injection**&**Open-Source LLMs**&**Hypnotism Attack**                                       |
| 25.05 |                                                                                Shanghai Jiao Tong University                                                                                 |                                   arxiv                                   |                                         [Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents](https://arxiv.org/abs/2505.14418)                                          |                                           **Backdoor Attack**&**GUI Agents**&**MLLM Security**                                            |
| 25.05 |                                                                           Indian Institute of Technology Kharagpur                                                                           |                                   arxiv                                   |                                          [Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations](https://arxiv.org/abs/2505.14469)                                           |                                       **Code-Mixing**&**Safety Alignment**&**Attribution Analysis**                                       |
| 25.05 |                                                                                       Google DeepMind                                                                                        |                                   arxiv                                   |                                                     [Lessons from Defending Gemini Against Indirect Prompt Injections](https://arxiv.org/abs/2505.14534)                                                     |                                 **Indirect Prompt Injection**&**Adversarial Evaluation**&**Red-Teaming**                                  |
| 25.05 |                                                                               National University of Singapore                                                                               |                                   arxiv                                   |                                        [Silent Leaks: Implicit Knowledge Extraction Attack on RAG Systems through Benign Queries](https://arxiv.org/abs/2505.15420v1)                                        |                                            **RAG**&**Knowledge Extraction**&**Benign Queries**                                            |
| 25.05 |                                                                                     Tsinghua University                                                                                      |                                   arxiv                                   |                                    [Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data Could Be Secretly Stolen!](https://arxiv.org/abs/2505.15656v1)                                    |                                   **Backdoor Attack**&**Data Extraction**&**Fine-Tuning Vulnerability**                                   |
| 25.05 |                                                                                   Imperial College London                                                                                    |                                   arxiv                                   |                                        [Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses](https://arxiv.org/abs/2505.15738v1)                                        |                                     **Prompt Injection**&**LLM Alignment**&**Adversarial Robustness**                                     |
| 25.05 |                                                                               Nanyang Technological University                                                                               |                                   arxiv                                   |                                         [Three Minds, One Legend: Jailbreak Large Reasoning Model with Adaptive Stacked Ciphers](https://arxiv.org/abs/2505.16241v1)                                         |                                  **Large Reasoning Models**&**Jailbreak Attacks**&**Stacked Encryption**                                  |
| 25.05 |                                                                                          ETH Zurich                                                                                          |                                   arxiv                                   |                                                                 [Finetuning-Activated Backdoors in LLMs](https://arxiv.org/abs/2505.16567v1)                                                                 |                                    **Backdoor Attack**&**Meta-Learning**&**LLM Fine-Tuning Security**                                     |
| 25.05 |                                                                        Huazhong University of Science and Technology                                                                         |                                   arxiv                                   |                                 [BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization](https://arxiv.org/abs/2505.16640v1)                                 |                        **Vision-Language-Action Models**&**Backdoor Attack**&**Objective-Decoupled Optimization**                         |
| 25.05 |                                                                               Nanyang Technological University                                                                               |                                   arxiv                                   |                                             [BitHydra: Towards Bit-flip Inference Cost Attack against Large Language Models](https://arxiv.org/abs/2505.16670v1)                                             |                                         **Bit-Flip Attack**&**Inference Cost**&**LLM Robustness**                                         |
| 25.05 |                                                                                      Nankai University                                                                                       |                                   arxiv                                   |                          [When Safety Detectors Aren't Enough: A Stealthy and Effective Jailbreak Attack on LLMs via Steganographic Techniques](https://arxiv.org/abs/2505.16765v1)                          |                                           **Jailbreak Attack**&**Steganography**&**LLM Safety**                                           |
| 25.05 |                                                                                      Indiana University                                                                                      |                                   arxiv                                   |                           [CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework](https://arxiv.org/abs/2505.16888v1)                           |                                     **Adversarial Prompt**&**System Prompt Attack**&**LLM Security**                                      |
| 25.05 |                                                                          Harbin Institute of Technology (Shenzhen)                                                                           |                                 ICLR 2025                                 |                                              [ONE MODEL TRANSFER TO ALL: ON ROBUST JAILBREAK PROMPTS GENERATION AGAINST LLMS](https://arxiv.org/abs/2505.17598)                                              |                                   **Jailbreak Attack**&**Robust Prompt Generation**&**Transferability**                                   |
| 25.05 |                                                                                       Google DeepMind                                                                                        |                                   arxiv                                   |                                     [Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models](https://arxiv.org/abs/2505.18773v1)                                     |                                **Membership Inference Attack**&**Large Language Models**&**Privacy Risk**                                 |
| 25.05 |                                                                              University of Missouri-Kansas City                                                                              |                             DSN 2025 Workshop                             |                                        [Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework](https://arxiv.org/abs/2505.18864v1)                                         |                               **Audio Jailbreak Attack**&**Multimodal Large Language Model**&**SpeechGPT**                                |
| 25.05 |                                                                                      Xidian University                                                                                       |                                   arxiv                                   |                                      [CPA-RAG: Covert Poisoning Attacks on Retrieval-Augmented Generation in Large Language Models](https://arxiv.org/abs/2505.19864v1)                                      |                               **Poisoning Attack**&**Retrieval-Augmented Generation**&**Black-box Attack**                                |
| 25.05 |                                                                                   University of G√∂ttingen                                                                                    |                                   arxiv                                   |                                        [TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent](https://arxiv.org/abs/2505.20118v2)                                         |                                           **Steganography**&**Privacy Leakage**&**Trojan LLM**                                            |
| 25.05 |                                                                                   ELLIS Institute T√ºbingen                                                                                   |                                   arxiv                                   |                                                           [Capability-Based Scaling Laws for LLM Red-Teaming](https://arxiv.org/abs/2505.20162v1)                                                            |                                           **Red-Teaming**&**Scaling Laws**&**Jailbreak Attack**                                           |
| 25.05 |                                                                Institute of Computing Technology, Chinese Academy of Sciences                                                                |                                   arxiv                                   |                                               [PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing](https://arxiv.org/abs/2505.21184)                                               |                               **Harmful Information Detection**&**Data Synthesis**&**Model Crowdsourcing**                                |
| 25.05 |                                                 State Key Laboratory of Intelligent Game, Institute of Software Chinese Academy of Sciences                                                  |                                   arxiv                                   |                                             [AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery](https://arxiv.org/abs/2505.21499v1)                                              |                            **Web Agent Security**&**Adversarial Environment Injection**&**Advertising Attack**                            |
| 25.05 |                                                                  Korea Advanced Institute of Science and Technology (KAIST)                                                                  |                                   arxiv                                   |                                             [Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts](https://arxiv.org/abs/2505.21556v1)                                             |                                     **Jailbreaking**&**Vision-Language Model**&**Adversarial Attack**                                     |
| 25.05 |                                                                                  The Ohio State University                                                                                   |                                   arxiv                                   |                                      [REDTEAMCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments](https://arxiv.org/abs/2505.21936)                                      |                                    **Adversarial Testing**&**Computer-Use Agent**&**Prompt Injection**                                    |
| 25.05 |                                                                                    University of Maryland                                                                                    |                                   arxiv                                   |                                               [DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors](https://arxiv.org/abs/2505.23001v1)                                                |                                 **Test Set Contamination**&**Backdoor Detection**&**Benchmark Integrity**                                 |
| 25.05 |                                                                      Beijing University of Posts and Telecommunications                                                                      |                                   arxiv                                   |                               [Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models](https://arxiv.org/abs/2505.23404v1)                               |                                 **LLM Jailbreaking**&**AI Security**&**Adaptive Jailbreaking Strategies**                                 |
| 25.05 |                                                                                      Beihang University                                                                                      |                             ACL 2025 Findings                             |                                  [Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space](https://arxiv.org/abs/2505.21277v2)                                   |                                **Jailbreak Attack**&**Strategy Space Expansion**&**Genetic Optimization**                                 |
| 25.05 |                                                               Huazhong University of Science and Technology; Lehigh University                                                               |                                 ACL 2025                                  |                                              [Merge Hijacking: Backdoor Attacks to Model Merging of Large Language Models](https://arxiv.org/abs/2505.23561v1)                                               |                                          **Model Merging**&**Backdoor Attack**&**LLM Security**                                           |
| 25.05 |                                                                               Florida International University                                                                               |                                   arxiv                                   |                                                  [System Prompt Extraction Attacks and Defenses in Large Language Models](https://arxiv.org/abs/2505.23817)                                                  |                                     **System Prompt Extraction**&**LLM Security**&**Prompt Defense**                                      |
| 25.05 |                                                                        Huazhong University of Science and Technology                                                                         |                                   arxiv                                   |                                                           [Spa-VLM: Stealthy Poisoning Attacks on RAG-based VLM](https://arxiv.org/abs/2505.23828)                                                           |                          **Vision-Language Model**&**Retrieval-Augmented Generation**&**Data Poisoning Attack**                           |
| 25.05 |                                                                                     Princeton University                                                                                     |                                   arxiv                                   |                                          [GeneBreaker: Jailbreak Attacks against DNA Language Models with Pathogenicity Guidance](https://arxiv.org/abs/2505.23839)                                          |                                        **DNA Language Model**&**Jailbreak Attack**&**Biosecurity**                                        |
| 25.05 |                                                                           University of Illinois Urbana-Champaign                                                                            |                                   arxiv                                   |                                       [From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models](https://arxiv.org/abs/2505.24232v1)                                       |                                           **Hallucination**&**Jailbreak**&**Foundation Models**                                           |
| 25.06 |                                                                   Mohamed bin Zayed University of Artificial Intelligence                                                                    |                                   arxiv                                   |                                 [Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities](https://arxiv.org/abs/2506.00548v1)                                 |                                        **Jailbreaking**&**Multimodal LLM**&**Adversarial Attack**                                         |
| 25.06 |                                                                                Harbin Institute of Technology                                                                                |                                   arxiv                                   |                                         [Jailbreak-R1: Exploring the Jailbreak Capabilities of LLMs via Reinforcement Learning](https://arxiv.org/abs/2506.00782v1)                                          |                                    **Jailbreak**&**Reinforcement Learning**&**Automated Red Teaming**                                     |
| 25.06 |                                                                                Hefei University of Technology                                                                                |      IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY       |                                  [Align is not Enough: Multimodal Universal Jailbreak Attack against Multimodal Large Language Models](https://arxiv.org/abs/2506.01307v1)                                   |                                 **Multimodal LLM**&**Jailbreak Attack**&**Universal Adversarial Attack**                                  |
| 25.06 |                                                                                      Sichuan University                                                                                      |                                   arxiv                                   |                                         [Beyond the Protocol: Unveiling Attack Vectors in the Model Context Protocol Ecosystem](https://arxiv.org/abs/2506.02040v2)                                          |                                  **Model Context Protocol**&**LLM Agent Security**&**Prompt Injection**                                   |
| 25.06 |                           State Key Laboratory of Intelligent Game, Institute of Software, Chinese Academy of Sciences, University of Chinese Academy of Sciences                            |                                   arxiv                                   |                                      [Joint-GCG: Unified Gradient-Based Poisoning Attacks on Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2506.06151)                                       |                              **Retrieval-Augmented Generation**&**RAG Poisoning**&**Gradient-Based Attack**                               |
| 25.06 |                                                                     Johns Hopkins University Applied Physics Laboratory                                                                      |                                   arxiv                                   |                                                 [A Systematic Review of Poisoning Attacks Against Large Language Models](https://arxiv.org/abs/2506.06518v1)                                                 |                                    **LLM Poisoning**&**Threat Model**&**Stealthiness**&**Persistence**                                    |
| 25.06 |                                University of Utah, Hong Kong University of Science and Technology (Guangzhou), Southern University of Science and Technology                                 |                                   arxiv                                   |                                              [Backdoor Attack on Vision Language Models with Stealthy Semantic Manipulation](https://arxiv.org/abs/2506.07214)                                               |                      **Backdoor Attack**&**Vision Language Model**&**Semantic Manipulation**&**Multimodal Security**                      |
| 25.06 |                                                                                       IBM Research AI                                                                                        |                                   arxiv                                   |                                                            [Effective Red-Teaming of Policy-Adherent Agents](https://arxiv.org/abs/2506.09600v1)                                                             |                      **Red-Teaming**&**Policy-Adherent Agent**&**LLM Safety**&**Multi-Agent**&**Adversarial Attack**                      |
| 25.06 |                                                                  Graduate School of Data Science, Seoul National University                                                                  |                                   arxiv                                   |                                          [From Threat to Tool: Leveraging Refusal-Aware Injection Attacks for Safety Alignment](https://arxiv.org/abs/2506.10020v1)                                          |       **Refusal-Aware Injection**&**LLM Safety Alignment**&**Synthetic Preference Data**&**Jailbreak Attack**&**Model Robustness**        |
| 25.06 |                                        Saarland University, Bosch Center for Artificial Intelligence, CISPA Helmholtz Center for Information Security                                        |                                 ICML 2025                                 |                                                              [Stealix: Model Stealing via Prompt Evolution](https://arxiv.org/abs/2506.05867v1)                                                              |                  **Model Stealing**&**Prompt Evolution**&**Diffusion Model**&**Black-box Attack**&**Genetic Algorithm**                   |
| 25.06 |                                                                          University of Illinois at Urbana-Champaign                                                                          |                                   arxiv                                   |                                                [InfoFlood: Jailbreaking Large Language Models with Information Overload](https://arxiv.org/abs/2506.12274v1)                                                 |                     **Jailbreaking**&**Information Overload**&**Adversarial Attacks**&**LLMs**&**Prompt Engineering**                     |
| 25.06 |                                                                              The Pennsylvania State University                                                                               |                                   arxiv                                   |                                      [Image Corruption-Inspired Membership Inference Attacks against Large Vision-Language Models](https://arxiv.org/abs/2506.12340v2)                                       |             **Membership Inference Attack**&**Vision-Language Models**&**Image Corruption**&**Privacy**&**Black-box Attack**              |
| 25.06 |                                                                                 East China Normal University                                                                                 |                                   arxiv                                   |                                                         [Exploring the Secondary Risks of Large Language Models](https://arxiv.org/abs/2506.12382v1)                                                         |                             **Secondary Risk**&**LLMs**&**Alignment**&**SecLens**&**Non-Adversarial Failure**                             |
| 25.06 |                                                                                    University of Toronto                                                                                     |                                   arxiv                                   |                                                [Alphabet Index Mapping: Jailbreaking LLMs through Semantic Dissimilarity](https://arxiv.org/abs/2506.12685v1)                                                | **Adversarial Attacks**&**Large Language Models**&**Jailbreaking**&**Semantic Similarity**&**Prompt Engineering**&**FlipAttack**&**AIM**  |
| 25.06 |                                                                          National University of Defense Technology                                                                           |                                   arxiv                                   |                                                  [Screen Hijack: Visual Poisoning of VLM Agents in Mobile Environments](https://arxiv.org/abs/2506.13205v1)                                                  |               **Mobile Agent**&**Vision-language Model**&**Backdoor Attack**&**Visual Injection**&**Clean-label Poisoning**               |
| 25.06 |                                                                        University of Science and Technology of China                                                                         |                                   arxiv                                   |                                                  [Mitigating Safety Fallback in Editing-based Backdoor Injection on LLMs](https://arxiv.org/abs/2506.13285)                                                  |                         **Backdoor Attack**&**LLM Security**&**Model Editing**&**Safety Alignment**&**DualEdit**                          |
| 25.06 |                                                                                     Southeast University                                                                                     |                                   arxiv                                   |                                  [Unlearning-Enhanced Website Fingerprinting Attack: Against Backdoor Poisoning in Anonymous Networks](https://arxiv.org/abs/2506.13563v1)                                   |            **Website Fingerprinting**&**Machine Unlearning**&**Backdoor Attacks**&**Anonymous Networks**&**Poison Detection**             |
| 25.06 |                                                                                     Tsinghua University                                                                                      |                                   arxiv                                   |                                                    [ExtendAttack: Attacking Servers of LRMs via Extending Reasoning](https://arxiv.org/abs/2506.13737v1)                                                     |       **Resource Depletion Attack**&**Large Reasoning Models**&**Computational Overhead**&**Prompt Obfuscation**&**Stealth Attack**       |
| 25.06 |                                                                       CISPA Helmholtz Center for Information Security                                                                        |                                   arxiv                                   |                                                              [Excessive Reasoning Attack on Reasoning LLMs](https://arxiv.org/abs/2506.14374v1)                                                              |      **Excessive Reasoning Attack**&**Reasoning LLMs**&**Computational Overhead**&**Adversarial Suffix**&**Model Denial of Service**      |
| 25.06 |                                                                                       Fudan University                                                                                       |                                   arxiv                                   |                                    [LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops](https://arxiv.org/abs/2506.14493v1)                                     |     **Denial-of-Service Attack**&**Multimodal Large Language Models**&**POS-Aware Delay**&**Repetitive Loop**&**Resource Exhaustion**     |
| 25.06 |                                                                                Meta FAIR, √âcole polytechnique                                                                                |                                   arxiv                                   |                                        [Winter Soldier: Backdooring Language Models at Pre-Training with Indirect Data Poisoning](https://arxiv.org/abs/2506.14913v1)                                        |              **Backdoor**&**Data Poisoning**&**Dataset Ownership Verification**&**Indirect Data Poisoning**&**LLM Security**              |
| 25.06 |                                                                                       Henan University                                                                                       |                                   arxiv                                   |                          [From LLMs to MLLMs to Agents: A Survey of Emerging Paradigms in Jailbreak Attacks and Defenses within LLM Ecosystem](https://arxiv.org/abs/2506.15170v1)                           |                                  **Jailbreak Attack**&**Defense Strategy**&**LLMs**&**MLLMs**&**Agents**                                  |
| 25.06 |                                                                                Shanghai Jiao Tong University                                                                                 |                                   arxiv                                   |                                                   [Multi-turn Jailbreaking via Global Refinement and Active Fabrication](https://arxiv.org/abs/2506.17881)                                                   |                                 **Multi-turn Jailbreaking**&**Global Refinement**&**Active Fabrication**                                  |
| 25.06 |                                                                                     Tsinghua University                                                                                      |                                   arxiv                                   |                                             [Probing the Robustness of Large Language Models Safety to Latent Perturbations](https://arxiv.org/abs/2506.16078v1)                                             |                                           **Safety Alignment**&**Robustness**&**Latent Attack**                                           |
| 25.06 |                                                                                Hefei University of Technology                                                                                |                                   arxiv                                   |                                            [MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning](https://arxiv.org/abs/2506.16792v1)                                            |                                         **Jailbreak Attack**&**Black-box LLM**&**Prompt Tuning**                                          |
| 25.06 |                                                                             Xi‚Äôan Jiaotong-Liverpool University                                                                              |                                   arxiv                                   |                                     [Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs](https://arxiv.org/abs/2506.17231v1)                                     |                                       **Jailbreak Attack**&**Prompt Distillation**&**LLM Security**                                       |
| 25.06 |                                                                                   Northeastern University                                                                                    |                                   arxiv                                   |                                                                          [LLM Jailbreak Oracle](https://arxiv.org/abs/2506.17299v1)                                                                          |                                          **Jailbreak Oracle**&**BOA Algorithm**&**LLM Security**                                          |
| 25.06 |                                                                                     Shenzhen University                                                                                      |                                   arxiv                                   |                                  [SAFEx: Analyzing Vulnerabilities of MoE-Based LLMs via Stable Safety-critical Expert Identification](https://arxiv.org/abs/2506.17368v1)                                   |                                      **MoE Vulnerability**&**Safety Alignment**&**Expert Analysis**                                       |
| 25.06 |                                                                                      Purdue University                                                                                       |                                   arxiv                                   |                           [Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses](https://arxiv.org/abs/2506.21972v1)                            |                                                **Jailbreak**&**LLM Security**&**Defense**                                                 |
| 25.06 |                               University of Notre Dame, Florida State University, Northwestern University, Duke University, University of Southern California                                |                                  KDD '25                                  |                                              [A Survey on Model Extraction Attacks and Defenses for Large Language Models](https://arxiv.org/abs/2506.22521v1)                                               |                                             **Model Extraction**&**LLM Security**&**Defense**                                             |
| 25.06 |                                                       New York University Abu Dhabi, New York University Tandon School of Engineering                                                        |                                   arxiv                                   |                    [MetaCipher: A General and Extensible Reinforcement Learning Framework for Obfuscation-Based Jailbreak Attacks on Black-Box LLMs](https://arxiv.org/abs/2506.22557v1)                     |                                         **Jailbreak**&**Obfuscation**&**Reinforcement Learning**                                          |
| 25.06 |                                                                                      Purdue University                                                                                       |                                   arxiv                                   |                                              [VERA: Variational Inference Framework for Jailbreaking Large Language Models](https://arxiv.org/abs/2506.22666v1)                                              |                                       **Jailbreak**&**Variational Inference**&**Black-box Attack**                                        |
| 25.06 |                                                                                      Palo Alto Networks                                                                                      |                                   arxiv                                   |                                        [Logit-Gap Steering: Efficient Short-Suffix Jailbreaks for Aligned Large Language Models](https://arxiv.org/abs/2506.24056v1)                                         |                                                 **Jailbreak**&**Logit-Gap**&**Alignment**                                                 |
| 25.07 |                                                          Fudan University, City University of Hong Kong, Hon Hai Research Institute                                                          |                                   arxiv                                   |                                                [ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models](https://arxiv.org/abs/2507.00026v1)                                                |                                           **Safety Evaluation**&**Adversarial Prompt**&**LLM**                                            |
| 25.07 |                                                              Del Norte High School, Bridgewater-Raritan HS, West Valley College                                                              |                                   arxiv                                   |                                     [AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2507.01020v1)                                      |                                          **Jailbreak**&**Adversarial Prompting**&**Multi-Turn**                                           |
| 25.07 |                                                                                Huawei Munich Research Center                                                                                 |                                   arxiv                                   |                                         [PII Jailbreaking in LLMs via Activation Steering Reveals Personal Information Leakage](https://arxiv.org/abs/2507.02332v1)                                          |                                         **PII Jailbreaking**&**Activation Steering**&**Privacy**                                          |
| 25.07 |                                                                   University of Calabria, IMT School for Advanced Studies                                                                    |                                   arxiv                                   |                                                [CyberRAG: An agentic RAG cyber attack classification and reporting tool](https://arxiv.org/abs/2507.02424v1)                                                 |                                    **Agentic RAG**&**Cyber Threat Detection**&**Security Classifier**                                     |
| 25.07 |                                                                       Sun Yat-sen University, Simon Fraser University                                                                        |                                   arxiv                                   |                                             [Control at Stake: Evaluating the Security Landscape of LLM-Driven Email Agents](https://arxiv.org/abs/2507.02699v1)                                             |                                             **Email Agent**&**Security**&**Prompt Injection**                                             |
| 25.07 |                                                                Shanghai Artificial Intelligence Laboratory, Purdue University                                                                |                                   arxiv                                   |                                            [Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection](https://arxiv.org/abs/2507.02844v1)                                            |                                            **Jailbreak**&**Multimodal LLM**&**Visual Attack**                                             |
| 25.07 |                                                          MIT Computer Science and Artificial Intelligence Laboratory, IBM Research                                                           |                                   arxiv                                   |                                        [LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users](https://arxiv.org/abs/2507.02850v1)                                        |                                          **Knowledge Injection**&**User Feedback**&**Security**                                           |
| 25.07 |                                                                                          Microsoft                                                                                           |                                   arxiv                                   |                                         [A Representation Engineering Perspective on the Effectiveness of Multi-Turn Jailbreaks](https://arxiv.org/abs/2507.02956v1)                                         |                                        **Jailbreak**&**Representation Engineering**&**Alignment**                                         |
| 25.07 |                                                                                   Northeastern University                                                                                    |                                   arxiv                                   |                                [‚ÄòFor Argument's Sake, Show Me How to Harm Myself!‚Äô: Jailbreaking LLMs in Suicide and Self-Harm Contexts](https://arxiv.org/abs/2507.02990v1)                                 |                                                **Jailbreak**&**Mental Health**&**Safety**                                                 |
| 25.07 |                                                                                     Tsinghua University                                                                                      |                                   arxiv                                   |                                          [Hijacking JARVIS: Benchmarking Mobile GUI Agents against Unprivileged Third Parties](https://arxiv.org/abs/2507.04227v1)                                           |                                            **GUI Agent**&**Robustness**&**Adversarial Attack**                                            |
| 25.07 |                                                                                      Cornell University                                                                                      |                                   arxiv                                   |                                   [Trojan Horse Prompting: Jailbreaking Conversational Multimodal Models by Forging Assistant Message](https://arxiv.org/abs/2507.04673v1)                                   |                                            **Jailbreak**&**Multimodal**&**Protocol Security**                                             |
| 25.07 |                                                                                     University of Padua                                                                                      |                                   arxiv                                   |                                                      [The Hidden Threat in Plain Text: Attacking RAG Data Loaders](https://arxiv.org/abs/2507.05093v1)                                                       |                                              **RAG Security**&**Document Poisoning**&**LLM**                                              |
| 25.07 |                                                                         Shanghai Artificial Intelligence Laboratory                                                                          |                                   arxiv                                   |                                           [Response Attack: Exploiting Contextual Priming to Jailbreak Large Language Models](https://arxiv.org/abs/2507.05248v1)                                            |                                               **Jailbreak**&**Contextual Priming**&**LLM**                                                |
| 25.07 |                                                                                       Wuhan University                                                                                       |                                   arxiv                                   |                          [CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative Adversarial Attacks on their Internal Representations](https://arxiv.org/abs/2507.06043v1)                           |                                                     **Jailbreak**&**Defense**&**GAN**                                                     |
| 25.07 |                                                                                       Google DeepMind                                                                                        |                                   arxiv                                   |                                                 [Attacker‚Äôs Noise Can Manipulate Your Audio-based LLM in the Real World](https://arxiv.org/abs/2507.06256v1)                                                 |                                        **Audio-based LLM**&**Jailbreaking**&**Adversarial Attack**                                        |
| 25.07 |                                                                                    University of Calabria                                                                                    |                                   arxiv                                   |                                               [The Dark Side of LLMs: Agent-based Attacks for Complete Computer Takeover](https://arxiv.org/abs/2507.06850v2)                                                |                                             **Agentic Attack**&**LLM Security**&**Takeover**                                              |
| 25.07 |                                                                        University of Science and Technology of China                                                                         |                                 COLM 2025                                 |                                         [VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation](https://arxiv.org/abs/2507.06899v1)                                         |                                          **Backdoor Attack**&**GUI Agent**&**Visual Grounding**                                           |
| 25.07 |                                                                             University of California, San Diego                                                                              |                                   arxiv                                   |                            [May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks](https://arxiv.org/abs/2507.07417v1)                            |                                              **Prompt Injection**&**Defense**&**Attention**                                               |
| 25.07 |                                                                       Compumacy for Artificial Intelligence Solutions                                                                        |                               GLSVLSI 2025                                |                                                [On Jailbreaking Quantized Language Models Through Fault Injection Attacks](https://arxiv.org/abs/2507.03236)                                                 |                                      **Jailbreaking**&**Bitflip Attack**&**Quantization Robustness**                                      |
| 25.07 |                                                                                  Xi‚Äôan Jiaotong University                                                                                   |                                   arxiv                                   |                                           [RAG Safety: Exploring Knowledge Poisoning Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2507.08862)                                            |                                                   **RAG**&**Poisoning**&**Robustness**                                                    |
| 25.07 |                                                                                    Microsoft Corporation                                                                                     |                                   arxiv                                   |                      [Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers](https://arxiv.org/abs/2507.09406v1)                       |                                               **Deception**&**Interpretability**&**Safety**                                               |
| 25.07 |                                                                                Shanghai Jiao Tong University                                                                                 |                                   arxiv                                   |                                              [LaSM: Layer-wise Scaling Mechanism for Defending Pop-up Attack on GUI Agents](https://arxiv.org/abs/2507.10610v1)                                              |                                                   **GUI Agent**&**Defense**&**Scaling**                                                   |
| 25.07 |                                                                                   University of Cambridge                                                                                    |                                   arxiv                                   |                                                   [Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](https://arxiv.org/abs/2507.11112v1)                                                   |                                                    **Backdoor**&**Poisoning**&**LLM**                                                     |
| 25.07 |                                                                                            FAR.AI                                                                                            |                                   arxiv                                   |                                                  [Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility](https://arxiv.org/abs/2507.11630v1)                                                   |                                              **Jailbreak**&**Fine-tuning**&**Vulnerability**                                              |
| 25.07 |                                                                             Indian Institute of Technology Jammu                                                                             |                                   arxiv                                   |                        [Exploiting Jailbreaking Vulnerabilities in Generative AI to Bypass Ethical Safeguards for Facilitating Phishing Attacks](https://arxiv.org/abs/2507.12185v1)                         |                                                  **Jailbreaking**&**Phishing**&**GenAI**                                                  |
| 25.07 |                                                                                      Huzhou University                                                                                       |                                   arxiv                                   |                                                      [Thought Purity: Defense Paradigm For Chain-of-Thought Attack](https://arxiv.org/abs/2507.12314v1)                                                      |                                               **Chain-of-Thought**&**Defense**&**Backdoor**                                               |
| 25.07 |                                                                                       Meridian Impact                                                                                        |                                   arxiv                                   |                                             [Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework](https://arxiv.org/abs/2507.12872v1)                                             |                                                   **Manipulation**&**Risk**&**Safety**                                                    |
| 25.07 |                                                                                        Preamble, Inc.                                                                                        |                                   arxiv                                   |                                                                [Prompt Injection 2.0: Hybrid AI Threats](https://arxiv.org/abs/2507.13169v1)                                                                 |                                            **Prompt Injection**&**Hybrid Threat**&**Security**                                            |
| 25.07 |                                                              Institute of Information Engineering, Chinese Academy of Sciences                                                               |                                   arxiv                                   |                                                   [Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers](https://arxiv.org/abs/2507.13474v1)                                                    |                                             **Jailbreaking**&**LLM Safety**&**Vulnerability**                                             |
| 25.07 |                                                                               National University of Singapore                                                                               |                                   arxiv                                   |                                                 [TopicAttack: An Indirect Prompt Injection Attack via Topic Transition](https://arxiv.org/abs/2507.13686v1)                                                  |                                             **Prompt Injection**&**LLM Security**&**Attack**                                              |
| 25.07 |                                                                                      Indiana University                                                                                      |                                   arxiv                                   |                                     [Manipulating LLM Web Agents with Indirect Prompt Injection Attack via HTML Accessibility Tree](https://arxiv.org/abs/2507.14799v1)                                      |                                         **Prompt Injection**&**Web Agent**&**Adversarial Attack**                                         |
| 25.07 |                                                                                         UC Berkeley                                                                                          |                                   arxiv                                   |                                                      [PromptArmor: Simple yet Effective Prompt Injection Defenses](https://arxiv.org/abs/2507.15219v1)                                                       |                                                 **Prompt Injection**&**Defense**&**LLM**                                                  |
| 25.07 |                                                                    Ukrainian State University of Science and Technologies                                                                    |                                   arxiv                                   |                                                     [Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems](https://arxiv.org/abs/2507.15613v1)                                                     |                                            **Prompt Injection**&**Enterprise LLM**&**Defense**                                            |
| 25.07 |                                                                               China Mobile Research Institute                                                                                |                                   arxiv                                   |                                           [RECALLED: An Unbounded Resource Consumption Attack on Large Vision-Language Models](https://arxiv.org/abs/2507.18053v1)                                           |                                       **Resource Consumption**&**Vision-Language Model**&**Attack**                                       |
| 25.07 |                                                                                  Universiti Sains Malaysia                                                                                   |                              EAI ICDF2C 2025                              |                                                [Talking Like a Phisher: LLM-Based Attacks on Voice Phishing Classifiers](https://arxiv.org/abs/2507.16291v1)                                                 |                                                **Adversarial Attack**&**Phishing**&**LLM**                                                |
| 25.07 |                                                                      Beijing University of Posts and Telecommunications                                                                      |                                   arxiv                                   |                              [Jailbreaking Large Language Diffusion Models: Revealing Hidden Safety Flaws in Diffusion-Based Text Generation](https://arxiv.org/abs/2507.19227)                              |                                                  **Jailbreak**&**Diffusion**&**Safety**                                                   |
| 25.07 |                                                            Chinese Academy of Sciences, University of Chinese Academy of Sciences                                                            |                                   arxiv                                   |                                                     [Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs](https://arxiv.org/abs/2507.22564)                                                     |                                                **CognitiveBias**&**Jailbreak**&**Safety**                                                 |
| 25.07 |                                                                                     360 AI Security Lab                                                                                      |                                   arxiv                                   |                                           [PRISM: Programmatic Reasoning with Image Sequence Manipulation for LVLM Jailbreaking](https://arxiv.org/abs/2507.21540)                                           |                                                      **Jailbreak**&**LVLM**&**ROP**                                                       |
| 25.07 |                                                                               North Carolina State University                                                                                |                                   arxiv                                   |                                                     [Prompt Optimization and Evaluation for LLM Automated Red Teaming](https://arxiv.org/abs/2507.22133)                                                     |                                               **RedTeaming**&**PromptOptimization**&**ASR**                                               |
| 25.07 |                                                                      The Hong Kong University of Science and Technology                                                                      |                                   arxiv                                   |                                                        [Enhancing Jailbreak Attacks on LLMs via Persona Prompts](https://arxiv.org/abs/2507.22171v1)                                                         |                                                   **Persona**&**Jailbreak**&**Genetic**                                                   |
| 25.07 |                                                                                    Independent Researcher                                                                                    |                                   arxiv                                   |                                    [Invisible Injections: Exploiting Vision-Language Models Through Steganographic Prompt Embedding](https://arxiv.org/abs/2507.22304v1)                                     |                                               **Steganography**&**PromptInjection**&**VLM**                                               |
| 25.07 |                                                                                       Kyoto University                                                                                       |                                   arxiv                                   |                                          [Counterfactual Evaluation for Blind Attack Detection in LLM-based Evaluation Systems](https://arxiv.org/abs/2507.23453v1)                                          |                                            **PromptInjection**&**BlindAttack**&**Evaluation**                                             |
| 25.08 |                                                                         Fudan University, Seoul National University                                                                          |                                   arxiv                                   |                    [A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2508.04276)                    |                                        **GraphRAG**&**Knowledge Poisoning**&**Adversarial Attack**                                        |
| 25.08 |                                                          University of Luxembourg, CISPA Helmholtz Center for Information Security                                                           |                                   arxiv                                   |                                               [Adversarial Attacks and Defenses on Graph-aware Large Language Models (LLMs)](https://arxiv.org/abs/2508.04894)                                               |                                   **Graph-aware LLMs**&**Adversarial Attacks**&**Adversarial Defense**                                    |
| 25.08 |                                                                      Tsinghua University, Zhipu Al, Beihang University                                                                       |                                    MM                                     |                               [JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering](https://arxiv.org/abs/2508.05087)                                |                            **Jailbreak Attacks**&**Multimodal Large Language Models**&**Collaborative Attack**                            |
| 25.08 | Beihang University, The Hong Kong University of Science and Technology, South China University of Technology, Nanyang Technological University, Kunming University of Science and Technology |                                   arxiv                                   |                                                         [Activation-Guided Local Editing for Jailbreaking Attacks](https://arxiv.org/abs/2508.00555)                                                         |                             **Jailbreaking Attacks**&**Large Language Models**&**Activation-Guided Editing**                              |
| 25.08 |                                                                                  Seoul National University                                                                                   |                                   arxiv                                   |                                                          [PUZZLED: Jailbreaking LLMs through Word-Based Puzzles](https://arxiv.org/abs/2508.01306)                                                           |                        **Jailbreak Attack**&**Large Language Models**&**Reasoning Capabilities**&**Word Puzzles**                         |
| 25.08 |                                                                                      Independent Author                                                                                      |                                   arxiv                                   |                                                  [Complete Evasion, Zero Modification: PDF Attacks on AI Text Detection](https://arxiv.org/abs/2508.01887)                                                   |                    **AI-Generated Text Detection**&**Evasion Attacks**&**PDF Vulnerability**&**Content Authenticity**                     |
| 25.08 |                                                                  Guangzhou University, The Hong Kong Polytechnic University                                                                  |                                   arxiv                                   |                                                [Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools](https://arxiv.org/abs/2508.02110)                                                 |                           **LLM Agents**&**Tool Metadata**&**Attractive Metadata Attack**&**Black-Box Attack**                            |
| 25.08 |                                                                              Microsoft Security Response Center                                                                              |                                   arxiv                                   |                                                            [Highlight & Summarize: RAG without the jailbreaks](https://arxiv.org/abs/2508.02872)                                                             |             **Retrieval-Augmented Generation (RAG)**&**Jailbreaking**&**Model Hijacking**&**System Design**&**LLM Security**              |
| 25.08 |                                                            Beihang University, Beijing University of Posts and Telecommunications                                                            |                                   arxiv                                   |                                   [Attack the Messages, Not the Agents: A Multi-round Adaptive Stealthy Tampering Framework for LLM-MAS](https://arxiv.org/abs/2508.03125)                                   | **LLM-based Multi-Agent Systems**&**Communication Vulnerabilities**&**Stealthy Tampering**&**Adaptive Attack**&**Reinforcement Learning** |
| 25.08 |                                                   AIM Intelligence, Yonsei University, Seoul National University, POSTECH, LG Electronics                                                    |                                   arxiv                                   |                                          [When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs](https://arxiv.org/abs/2508.03365)                                          |                    **Audio-Language Models**&**Adversarial Audio Attack**&**Jailbreaking**&**Reinforcement Learning**                     |
| 25.08 |                                                                           University of Stuttgart, ELLIS Alicante                                                                            |                                   arxiv                                   |                                                          [Large Reasoning Models Are Autonomous Jailbreak Agents](https://arxiv.org/abs/2508.04039)                                                          |                    **large reasoning models**&**large language models**&**jailbreaking**&**Al security**&**Al safety**                    |
| 25.08 |                                                                           University of California, Santa Barbara                                                                            |                                   arxiv                                   |                                                                         [Many-Turn Jailbreaking](https://arxiv.org/abs/2508.06755v1)                                                                         |                                       **Jailbreaking**&**Multi-turn Conversations**&**LLM Safety**                                        |
| 25.08 |                                                                                Oak Ridge National Laboratory                                                                                 |                                   arxiv                                   |                                                            [Attacks and Defenses Against LLM Fingerprinting](https://arxiv.org/abs/2508.09021v1)                                                             |                                 **LLM Fingerprinting**&**Reinforcement Learning**&**Defensive Filtering**                                 |
| 25.08 |                                                                                   Imperial College London                                                                                    |                                   arxiv                                   |                                          [Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity](https://arxiv.org/abs/2508.09218v1)                                           |                          **Multimodal Jailbreaking**&**Balanced Structural Decomposition**&**Safety Evaluation**                          |
| 25.08 |                                                                                Shanghai Jiao Tong University                                                                                 |                                   arxiv                                   |                                                     [IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding](https://arxiv.org/abs/2508.09456v1)                                                      |                                    **Backdoor Attack**&**Visual Grounding**&**Vision-Language Models**                                    |
| 25.08 |                                                                                     Zhejiang University                                                                                      |                                   arxiv                                   |                                  [Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs](https://arxiv.org/abs/2508.10029v1)                                   |                        **Latent Fusion Jailbreak**&**Hidden State Interpolation**&**Adversarial Training Defense**                        |
| 25.08 |                                                                                       Jinan University                                                                                       |                                   arxiv                                   |                                                [The Cost of Thinking: Increased Jailbreak Risk in Large Language Models](https://arxiv.org/abs/2508.10032v1)                                                 |                                  **Thinking Mode**&**Jailbreak Attacks**&**Safe Thinking Intervention**                                   |
| 25.08 |                                                                      Nanjing University of Aeronautics and Astronautics                                                                      |                                   arxiv                                   |                                                 [Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts](https://arxiv.org/abs/2508.10390v1)                                                 |                           **Jailbreak Attacks**&**Malicious Content Detection**&**Developer Message Exploits**                            |
| 25.08 |                                                                                     Zhejiang University                                                                                      |                                   arxiv                                   |                                   [MAJIC: Markovian Adaptive Jailbreaking via Iterative Composition of Diverse Innovative Strategies](https://arxiv.org/abs/2508.13048v1)                                    |                                  **Jailbreaking Attacks**&**Black-box LLMs**&**Markov Chain Adaptation**                                  |
| 25.08 |                               University of Notre Dame, Florida State University, Northwestern University, Duke University, University of Southern California                                |                                   arxiv                                   |                                     [A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives](https://arxiv.org/abs/2508.15031)                                      |                                          **Model Extraction Attacks**&**AI Security**&**Survey**                                          |
| 25.08 |                                                             Louisiana State University, Northeastern University, Yale University                                                             |                                   arxiv                                   |                                      [MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs](https://arxiv.org/abs/2508.15036)                                       |                                      **Mixture-of-Experts**&**Side-Channel Attacks**&**LLM Privacy**                                      |
| 25.08 |                                                                                China Agricultural University                                                                                 |                                   arxiv                                   |                                           [Self-Disguise Attack: Induce the LLM to disguise itself for AIGT detection evasion](https://arxiv.org/abs/2508.15848v1)                                           |                                **AI-Generated Text Detection**&**Adversarial Prompting**&**Self-Disguise**                                |
| 25.08 |                                                                                             MIPT                                                                                             |                                   arxiv                                   |                                                    [HAMSA: Hijacking Aligned Compact Models via Stealthy Automation](https://arxiv.org/abs/2508.16484v1)                                                     |                                 **Automated Red-Teaming**&**Jailbreak Attacks**&**Multilingual Security**                                 |
| 25.08 |                                                                              Ben Gurion University of the Negev                                                                              |                                   arxiv                                   |                                                [Trust Me, I Know This Function: Hijacking LLM Static Analysis using Bias](https://arxiv.org/abs/2508.17361v1)                                                |                                     **Static Analysis**&**Adversarial Examples**&**Abstraction Bias**                                     |
| 25.08 |                                                                             Texas A&M University‚ÄìCorpus Christi                                                                              |                                   arxiv                                   |                                      [Attacking LLMs and AI Agents: Advertisement Embedding Attacks Against Large Language Models](https://arxiv.org/abs/2508.17674v1)                                       |                              **Advertisement Embedding Attacks**&**Prompt Injection**&**Backdoored Models**                               |
| 25.08 |                                                                   University of Electronic Science and Technology of China                                                                   |                                   arxiv                                   |                                     [Hidden Tail: Adversarial Image Causing Stealthy Resource Consumption in Vision-Language Models](https://arxiv.org/abs/2508.18805v1)                                     |                             **Vision-Language Models**&**Resource Consumption Attack**&**Adversarial Images**                             |
| 25.08 |                                                                                     University of Exeter                                                                                     |                                   arxiv                                   |                                                [POT: Inducing Overthinking in LLMs via Black-Box Iterative Optimization](https://arxiv.org/abs/2508.19277v1)                                                 |                                  **Overthinking Attack**&**Prompt Injection**&**Black-Box Optimization**                                  |
| 25.08 |                                                                                     Hiroshima University                                                                                     |                                 NSS 2025                                  |                                              [Prompt-in-Content Attacks: Exploiting Uploaded Inputs to Hijack LLM Behavior](https://arxiv.org/abs/2508.19287v1)                                              |                                        **Prompt-in-Content**&**LLM Security**&**Output Hijacking**                                        |
| 25.08 |                                                                        National University of Defense and Technology                                                                         |                                   arxiv                                   |                                         [Stand on The Shoulders of Giants: Building JailExpert from Previous Attack Experience](https://arxiv.org/abs/2508.19292v1)                                          |                               **Jailbreak Attacks**&**Experience Formalization**&**Automated Red-Teaming**                                |
| 25.08 |                                                                        University of Science and Technology of China                                                                         |                                   arxiv                                   |                                [MindGuard: Tracking, Detecting, and Attributing MCP Tool Poisoning Attack via Decision Dependence Graph](https://arxiv.org/abs/2508.20412v1)                                 |                                **Tool Poisoning Attack**&**Decision Dependence Graph**&**Agent Security**                                 |
| 25.08 |                                                                                     University of Padua                                                                                      |                                   arxiv                                   |                                                [Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review](https://arxiv.org/abs/2508.20863v1)                                                 |                                           **Prompt Injection**&**Peer Review**&**LLM Security**                                           |
| 25.09 |                                                                                          HydroX AI                                                                                           |                                   arxiv                                   |                                                   [The Resurgence of GCG Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2509.00391v1)                                                   |                                    **Adversarial Attack**&**Large Language Models**&**GCG Algorithm**                                     |
| 25.09 |                                                                                     Zhejiang University                                                                                      |         IEEE Transactions on Visualization and Computer Graphics          |                                               [NeuroBreak: Unveil Internal Jailbreak Mechanisms in Large Language Models](https://arxiv.org/abs/2509.03985v1)                                                |                                       **Jailbreak Attacks**&**Visual Analytics**&**Safety Neurons**                                       |
| 25.09 |                                                                             Embry-Riddle Aeronautical University                                                                             |                               IEEE TPS 2025                               |                                        [Clone What You Can‚Äôt Steal: Black-Box LLM Replication via Logit Leakage and Distillation](https://arxiv.org/abs/2509.00973v1)                                        |                                     **Model Extraction**&**Logit Leakage**&**Knowledge Distillation**                                     |
| 25.09 |                                                                                     Tsinghua University                                                                                      |                                   arxiv                                   |                                            [Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs](https://arxiv.org/abs/2509.05367v1)                                             |                                         **Jailbreak Attacks**&**Ethical Reasoning**&**AI Safety**                                         |
| 25.09 |                                                                                  Manipal University Jaipur                                                                                   |                                   arxiv                                   |                                        [Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization](https://arxiv.org/abs/2509.05831v1)                                         |                                        **Prompt Injection**&**HTML Attacks**&**Web Summarization**                                        |
| 25.09 |                                                                                     Soongsil University                                                                                      |                                   arxiv                                   |                                                [Multimodal Prompt Injection Attacks: Risks and Defenses for Modern LLMs](https://arxiv.org/abs/2509.05883v1)                                                 |                                        **Prompt Injection**&**Multimodal Attacks**&**AI Security**                                        |
| 25.09 |                                                                   University of Electronic Science and Technology of China                                                                   |                                   arxiv                                   |                                              [Embedding Poisoning: Bypassing Safety Alignment via Embedding Semantic Shift](https://arxiv.org/abs/2509.06338v1)                                              |                                   **Embedding Poisoning**&**Safety Alignment**&**Adversarial Attacks**                                    |
| 25.09 |                                                                                    Politecnico di Milano                                                                                     |                                   arxiv                                   |                                           [Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?](https://arxiv.org/abs/2509.06350v1)                                            |                                  **Jailbreak Attacks**&**Token Redundancy**&**Adversarial Optimization**                                  |
| 25.09 |                                                                                       AIM Intelligence                                                                                       |                                   arxiv                                   |                                    [X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates](https://arxiv.org/abs/2509.08729v1)                                    |                                     **Jailbreak Attacks**&**M2S Templates**&**Automated Red Teaming**                                     |
| 25.09 |                                                                                  Seoul National University                                                                                   |                                   arxiv                                   |                                         [Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic Encoding](https://arxiv.org/abs/2509.10931)                                         |                                  **Jailbreaking Attacks**&**Symbolic Encoding**&**Abductive Reasoning**                                   |
| 25.09 |                                                       Beijing University of Posts and Telecommunications, Chinese Academy of Sciences                                                        |                                   arxiv                                   |                                                     [ENJ: Optimizing Noise with Genetic Algorithms to Jailbreak LSMs](https://arxiv.org/abs/2509.11128)                                                      |                                     **Audio Jailbreak**&**Genetic Algorithm**&**Large Speech Models**                                     |
| 25.09 |                                                         Technical University of Darmstadt, University of Zagreb, Radboud University                                                          |                                 NDSS 2026                                 |                                                            [NeuroStrike: Neuron-Level Attacks on Aligned LLMs](https://arxiv.org/abs/2509.11864)                                                             |                                    **Safety Neurons**&**Neuron-level Jailbreak**&**Black-box Attack**                                     |
| 25.09 |                                                                         BITS Pilani, Queen Mary University of London                                                                         |                                   arxiv                                   |           [Towards Inclusive Toxic Content Moderation: Addressing Vulnerabilities to Adversarial Attacks in Toxicity Classifiers Tackling LLM-generated Content](https://arxiv.org/abs/2509.12672)           |                           **Toxicity Classification**&**Adversarial Attacks**&**Mechanistic Interpretability**                            |
| 25.09 |                                                                        Fudan University, City University of Hong Kong                                                                        |                                   arxiv                                   |                                     [Defense-to-Attack: Bypassing Weak Defenses Enables Stronger Jailbreaks in Vision-Language Models](https://arxiv.org/abs/2509.12724)                                     |                               **Vision-Language Models**&**Jailbreak Attack**&**Adversarial Optimization**                                |
| 25.09 |                                                                              KTH Royal Institute of Technology                                                                               |                               GameSec 2025                                |                                                    [Jailbreaking Large Language Models Through Content Concretization](https://arxiv.org/abs/2509.12937)                                                     |                                 **Jailbreaking**&**Content Concretization**&**Malicious Code Generation**                                 |
| 25.09 |                                                    Harbin Institute of Technology, Hong Kong Polytechnic University, Shenzhen University                                                     |                                   arxiv                                   |                                                   [A Simple and Efficient Jailbreak Method Exploiting LLMs‚Äô Helpfulness](https://arxiv.org/abs/2509.14297)                                                   |                                    **LLM Jailbreak**&**Safety Alignment**&**Helpfulness Exploitation**                                    |
| 25.09 |                                                    East China Normal University, Xi‚Äôan Jiaotong University, Meituan, Zhejiang University                                                     |                                   arxiv                                   |                                 [MUSE: MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety in Large Language Models](https://arxiv.org/abs/2509.14651)                                 |                                       **Multi-Turn Jailbreak**&**Red Teaming**&**Safety Alignment**                                       |
| 25.09 |                                                         The Hong Kong Polytechnic University, Northwestern Polytechnical University                                                          |                                   arxiv                                   |                                                   [Semantic Representation Attack against Aligned Large Language Models](https://arxiv.org/abs/2509.19360)                                                   |                               **Adversarial Attack**&**Large Language Models**&**Semantic Representation**                                |
| 25.09 |                                                                        University of Science and Technology of China                                                                         |                                   arxiv                                   |                                               [BI-GRPO: Bidirectional Optimization for Jailbreak Backdoor Injection on LLMs](https://arxiv.org/abs/2509.19775)                                               |                            **Jailbreak Backdoor Attack**&**Reinforcement Learning**&**Large Language Models**                             |
| 25.09 |                                                      Singapore Management University & Alibaba Group & Dalian University of Technology                                                       |                                   arxiv                                   |                                                  [Benchmarking Gaslighting Attacks Against Speech Large Language Models](https://arxiv.org/abs/2509.19858)                                                   |                                     **Speech-LLM**&**Adversarial Attacks**&**Behavioral Robustness**                                      |
| 25.09 |                                                          Shanghai Jiao Tong University & National University of Defense Technology                                                           |                                   arxiv                                   |                                             [RLCracker: Exposing the Vulnerability of LLM Watermarks with Adaptive RL Attacks](https://arxiv.org/abs/2509.20924)                                             |                             **LLM Watermarking**&**Reinforcement Learning Attack**&**Robustness Evaluation**                              |
| 25.09 |                                                                           Zhejiang University & Palo Alto Networks                                                                           |                                   arxiv                                   |                                                 [Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools](https://arxiv.org/abs/2509.21011)                                                 |                                   **Model Context Protocol**&**LLM Security**&**Red Teaming Framework**                                   |
| 25.09 |                                                                            University of California, Los Angeles                                                                             |                                   arxiv                                   |                                     [Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and Agentic Mitigation in LLMs](https://arxiv.org/abs/2509.21080)                                     |                                   **Cultural Positioning Bias**&**CULTURELENS**&**Agentic Mitigation**                                    |
| 25.09 |                                                                                   Arizona State University                                                                                   |                                   arxiv                                   |                                                   [SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models](https://arxiv.org/abs/2509.21843v1)                                                   |                                **Bit-Flip Attack**&**Model Robustness**&**Large Language Models Security**                                |
| 25.09 |                                              Singapore Management University, Huazhong University of Science and Technology, Monash University                                               |              Proceedings of the ACM on Software Engineering               |                                        [‚ÄúYour AI, My Shell‚Äù: Demystifying Prompt Injection Attacks on Agentic AI Coding Editors](https://arxiv.org/abs/2509.22040v1)                                         |                                 **Prompt Injection Attack**&**Agentic AI Coding Editors**&**AI Security**                                 |
| 25.09 |                                                    Artificial Intelligence Research Institute, Moscow Institute of Physics and Technology                                                    |                                   arxiv                                   |                                                     [The Rogue Scalpel: Activation Steering Compromises LLM Safety](https://arxiv.org/abs/2509.22067v1)                                                      |                                  **Activation Steering**&**Mechanistic Interpretability**&**LLM Safety**                                  |
| 25.09 |                                                 Beijing Institute of Technology, Hefei University of Technology, The University of Auckland                                                  |                                   arxiv                                   |                                                [Formalization Driven LLM Prompt Jailbreaking via Reinforcement Learning](https://arxiv.org/abs/2509.23558v1)                                                 |                                      **Prompt Jailbreaking**&**Reinforcement Learning**&**GraphRAG**                                      |
| 25.09 |                                                                Tsinghua University, 201.AI, Nanyang Technological University                                                                 |                                   arxiv                                   |                                              [SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents](https://arxiv.org/abs/2509.23694v2)                                               |                                     **LLM-Based Search Agents**&**Safety Benchmark**&**Red-Teaming**                                      |
| 25.09 |                                                                                     Southeast University                                                                                     |                                   arxiv                                   |                                        [FuncPoison: Poisoning Function Library to Hijack Multi-agent Autonomous Driving Systems](https://arxiv.org/abs/2509.24408v1)                                         |                          **Function Library Poisoning**&**Multi-agent Systems**&**Autonomous Driving Security**                           |
| 25.09 |                                                                                   UC Berkeley, AWS AI Labs                                                                                   |                                   arxiv                                   |                                                [STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents](https://arxiv.org/abs/2509.25624v1)                                                 |                             **LLM Agents**&**Sequential Tool Attacks**&**Multi-turn Security Vulnerability**                              |
| 25.09 |                                           Wenzhou-Kean University, University of Bremen, Case Western Reserve University, Duke Kunshan University                                            |                                   arxiv                                   |                            [SafeBehavior: Simulating Human-Like Multistage Reasoning to Mitigate Jailbreak Attacks in Large Language Models](https://arxiv.org/abs/2509.26345v1)                             |                             **Jailbreak Defense**&**Human-like Reasoning**&**Hierarchical Safety Mechanism**                              |
| 25.10 |                                                                                  National Taiwan University                                                                                  |                                   arxiv                                   |                                         [Eyes-on-Me: Scalable RAG Poisoning through Transferable Attention-Steering Attractors](https://arxiv.org/abs/2510.00586v1)                                          |                                     **RAG Poisoning**&**Attention Steering**&**Transferable Attack**                                      |
| 25.10 |                                                                  √âcole de technologie sup√©rieure, Johns Hopkins University                                                                   |                                   arxiv                                   |                                                            [Backdoor Attacks Against Speech Language Models](https://arxiv.org/abs/2510.01157v1)                                                             |                                  **Speech Language Models**&**Backdoor Attacks**&**Fine-tuning Defense**                                  |
| 25.10 |                                                                     Anhui University, Swinburne University of Technology                                                                     |                                   arxiv                                   |                                       [Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge](https://arxiv.org/abs/2510.01223v1)                                       |                                  **Jailbreak Attack**&**Nested Scenario**&**Toxic Knowledge Injection**                                   |
| 25.10 |                                                                                 Chinese Academy of Sciences                                                                                  |                                   arxiv                                   |                                      [Fine-Tuning Jailbreaks under Highly Constrained Black-Box Settings: A Three-Pronged Approach](https://arxiv.org/abs/2510.01342v1)                                      |                              **Fine-Tuning Jailbreak**&**LLM Safety**&**Black-Box Attacks**&**Data Evasion**                              |
| 25.10 |                                                               University of Maryland, AWS AI Labs, Meta Superintelligence Labs                                                               |                                   arxiv                                   |                                    [Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks](https://arxiv.org/abs/2510.01359v1)                                    |                           **Code Agents**&**Jailbreak Benchmark**&**Executable-Aware Evaluation**&**AI Safety**                           |
| 25.10 |                                                             The Hong Kong Polytechnic University, UC San Diego, HKUST (GZ), HKBU                                                             |                               NeurIPS 2025                                |                                             [Virus Infection Attack on LLMs: Your Poisoning Can Spread ‚ÄúVIA‚Äù Synthetic Data](https://arxiv.org/abs/2509.23041v1)                                             |                      **Synthetic Data**&**Poisoning Propagation**&**Virus Infection Attack (VIA)**&**LLM Security**                       |
| 25.10 |                                                                               Nanyang Technological University                                                                               |                                   arxiv                                   |                                                   [A-MemGuard: A Proactive Defense Framework for LLM-Based Agent Memory](https://arxiv.org/abs/2510.02373)                                                   |                                 **Agent Security**&**Memory Poisoning Defense**&**Consensus Validation**                                  |
| 25.10 |                                                                                     University of Oxford                                                                                     |                                   arxiv                                   |                                                        [ToolTweak: An Attack on Tool Selection in LLM-Based Agents](https://arxiv.org/abs/2510.02554)                                                        |                                      **LLM Agents**&**Adversarial Attacks**&**Tool Selection Bias**                                       |
| 25.10 |                                                                               Nanyang Technological University                                                                               |                               NeurIPS 2025                                |                                                   [Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs](https://arxiv.org/abs/2510.02833)                                                   |                                   **Fine-tuning Attack**&**Overfitting Exploitation**&**LLM Jailbreak**                                   |
| 25.10 |                                                                                     Zhejiang University                                                                                      |                                   arxiv                                   |                                            [External Data Extraction Attacks against Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2510.02964)                                            |                             **Retrieval-Augmented Generation**&**Data Extraction Attack**&**Trustworthy AI**                              |
| 25.10 |                                                                                     Zhejiang University                                                                                      |                                   arxiv                                   |                                                                       [Untargeted Jailbreak Attack](https://arxiv.org/abs/2510.02999)                                                                        |                                **Jailbreak Attack**&**Gradient Optimization**&**Model Safety Evaluation**                                 |
| 25.10 |                                                                                   Old Dominion University                                                                                    |                                   arxiv                                   |                                         [NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn LLM Jailbreaks](https://arxiv.org/abs/2510.03417)                                          |                           **Multi-Turn Jailbreak**&**Adversarial Query Network**&**Feedback-Driven Refinement**                           |
| 25.10 |                                                                           University of Illinois Urbana-Champaign                                                                            |                                   arxiv                                   |                                                 [Quantifying Risks in Multi-turn Conversation with Large Language Models](https://arxiv.org/abs/2510.03969)                                                  |                     **Multi-turn Safety Certification**&**Catastrophic Risk Quantification**&**Formal Verification**                      |
| 25.10 |                                                                             The Hong Kong Polytechnic University                                                                             |                                   IEEE                                    |                                       [AgentTypo: Adaptive Typographic Prompt Injection Attacks against Black-box Multimodal Agents](https://arxiv.org/abs/2510.04257)                                       |                         **Multimodal Agent Security**&**Typographic Prompt Injection**&**Black-box Optimization**                         |
| 25.10 |                                                                               Beijing Institute of Technology                                                                                |                                   arxiv                                   |                                    [VORTEXPIA: Indirect Prompt Injection Attack against LLMs for Efficient Extraction of User Privacy](https://arxiv.org/abs/2510.04261)                                     |                                **Prompt Injection**&**User Privacy Extraction**&**Black-box LLM Security**                                |
| 25.10 |                                                                                     Tsinghua University                                                                                      |                                   arxiv                                   |                                                         [Imperceptible Jailbreaking against Large Language Models](https://arxiv.org/abs/2510.05025)                                                         |                        **Jailbreak Attack**&**Unicode Variation Selectors**&**Invisible Adversarial Perturbation**                        |
| 25.10 |                                                                       ServiceNow Research, Mila - Quebec AI Institute                                                                        |                                   arxiv                                   |                                             [Indirect Prompt Injections: Are Firewalls All You Need, or Stronger Benchmarks?](https://arxiv.org/abs/2510.05244)                                              |                           **Indirect Prompt Injection**&**Firewall Defense**&**Agentic Security Benchmarking**                            |
| 25.10 |                                                                                   Johns Hopkins University                                                                                   |                                   arxiv                                   |                                    [AutoDAN-Reasoning: Enhancing Strategies Exploration based Jailbreak Attacks with Test-Time Scaling](https://arxiv.org/abs/2510.05379)                                    |                               **Jailbreak Attack**&**Test-Time Scaling**&**Strategy Library Optimization**                                |
| 25.10 |                                                                            Google, University of Texas at Austin                                                                             |                                   arxiv                                   |                                                 [Adversarial Reinforcement Learning for Large Language Model Agent Safety](https://arxiv.org/abs/2510.05442)                                                 |                       **Adversarial Reinforcement Learning**&**Agent Safety**&**Indirect Prompt Injection Defense**                       |
| 25.10 |                                                                                   USTC, Purdue University                                                                                    |                                   arxiv                                   |                                                   [Membership Inference Attacks on Tokenizers of Large Language Models](https://arxiv.org/abs/2510.05699)                                                    |                           **Membership Inference Attack (MIA)**&**Tokenizer Privacy Leakage**&**LLM Security**                            |
| 25.10 |                                                                                  NOVA University of Lisbon                                                                                   |                                   arxiv                                   |                                                      [RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning](https://arxiv.org/abs/2510.06994)                                                       |                                   **Red Teaming**&**Jailbreak Detection**&**Adaptive Attack Planning**                                    |
| 25.10 |                                                         Beijing Electronic Science and Technology Institute, NTU, Hainan University                                                          |                                   arxiv                                   |                                                   [Rethinking Reasoning: A Survey on Reasoning-based Backdoors in LLMs](https://arxiv.org/abs/2510.07697)                                                    |                                 **Reasoning Security**&**Backdoor Attacks**&**LLM Reasoning Robustness**                                  |
| 25.10 |                                                                                  HUST, Tsinghua University                                                                                   |                                   arxiv                                   |                                           [Effective and Stealthy One-Shot Jailbreaks on Deployed Mobile Vision-Language Agents](https://arxiv.org/abs/2510.07809)                                           |                                **Mobile Agent Security**&**One-Shot Jailbreak**&**Vision-Language Models**                                |
| 25.10 |                                                                                          ETH Zurich                                                                                          |                                   arxiv                                   |                                                     [Fewer Weights, More Problems: A Practical Attack on LLM Pruning](https://arxiv.org/abs/2510.07985)                                                      |                       **LLM Pruning Security**&**Malicious Model Activation**&**Model Compression Vulnerabilities**                       |
| 25.10 |                                                                           National University of Singapore, HKUST                                                                            |                            EMNLP 2025 Findings                            |                                                    [Backdoor-Powered Prompt Injection Attacks Nullify Defense Methods](https://arxiv.org/abs/2510.03705)                                                     |                                **Prompt Injection**&**Backdoor Attacks**&**Instruction Hierarchy Defense**                                |
| 25.10 |                                                                University of South Florida & University of California, Davis                                                                 |                                   arxiv                                   |                                  [Less Diverse, Less Safe: The Indirect But Pervasive Risk of Test-Time Scaling in Large Language Models](https://arxiv.org/abs/2510.08592)                                  |                                      **Test-Time Scaling**&**LLM Safety**&**Adversarial Robustness**                                      |
| 25.10 |                                                            University of Cagliari & Centre for AI Governance & Cisco Systems Inc.                                                            |                                   arxiv                                   |                                              [LatentBreak: Jailbreaking Large Language Models through Latent Space Feedback](https://arxiv.org/abs/2510.08604)                                               |                              **Jailbreak Attack**&**Latent Space Optimization**&**Perplexity-Based Defense**                              |
| 25.10 |                                                                            Institute of Science Tokyo & RIKEN AIP                                                                            |                                   arxiv                                   |                                 [Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models](https://arxiv.org/abs/2510.08859)                                 |                                 **Multi-Turn Jailbreak**&**Conversation Pattern Analysis**&**AI Safety**                                  |
| 25.10 |                                       OpenAI & Anthropic & Google DeepMind & ETH Z√ºrich & Northeastern University & HackAPrompt & AI Security Company                                        |                                   arxiv                                   |                            [The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against LLM Jailbreaks and Prompt Injections](https://arxiv.org/abs/2510.09023)                             |                              **Adaptive Attacks**&**LLM Jailbreak Defense**&**Prompt Injection Robustness**                               |
| 25.10 |                                     Nanyang Technological University & Nanjing University of Aeronautics and Astronautics & Tsinghua University & A*STAR                                     |                                   arxiv                                   |                           [CREST-Search: Comprehensive Red-teaming for Evaluating Safety Threats in Large Language Models Powered by Web Search](https://arxiv.org/abs/2510.09689)                           |                          **Web-Search LLMs**&**Red-teaming**&**Citation Risk**&**Adversarial Query Generation**                           |
| 25.10 |                                                                                    University of Georgia                                                                                     |                                   arxiv                                   |                                                [MetaBreak: Jailbreaking Online LLM Services via Special Token Manipulation](https://arxiv.org/abs/2510.10271)                                                |                                      **LLM Jailbreak**&**Special Tokens**&**Security Exploitation**                                       |
| 25.10 |                                                                           National Taiwan University & NYCU & NDHU                                                                           |                                   arxiv                                   |                                                [ArtPerception: ASCII Art-based Jailbreak on LLMs with Recognition Pre-test](https://arxiv.org/abs/2510.10281)                                                |                                         **ASCII Jailbreak**&**Black-box Attack**&**LLM Security**                                         |
| 25.10 |                                                                       Fudan University & City University of Hong Kong                                                                        |                                   arxiv                                   |                                                    [TabVLA: Targeted Backdoor Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2510.10932)                                                    |                                         **Backdoor Attack**&**VLA Models**&**Robotics Security**                                          |
| 25.10 |                                                                                      Yonsei University                                                                                       |                                   arxiv                                   |                                            [DITTO: A Spoofing Attack Framework on Watermarked LLMs via Knowledge Distillation](https://arxiv.org/abs/2510.10987)                                             |                                    **Watermark Spoofing**&**Knowledge Distillation**&**LLM Security**                                     |
| 25.10 |                                                                          EPFL & Huawei Technologies Switzerland AG                                                                           |                                   arxiv                                   |                                                    [RAG-Pull: Imperceptible Attacks on RAG Systems for Code Generation](https://arxiv.org/abs/2510.11195)                                                    |                                       **RAG Security**&**Imperceptible Attack**&**Code Generation**                                       |
| 25.10 |                                                 BUPT & Shanghai AI Lab & Xiamen University Malaysia & North China Electric Power University                                                  |                                   arxiv                                   |                                           [Collaborative Shadows: Distributed Backdoor Attacks in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2510.11246)                                           |                                **Multi-Agent Security**&**Backdoor Attack**&**Collaboration Exploitation**                                |
| 25.10 |                                                         LMU Munich & University of Oxford & Technical University of Berlin & AWS AI                                                          |                                   arxiv                                   |                                                      [Bag of Tricks for Subverting Reasoning-Based Safety Guardrails](https://arxiv.org/abs/2510.11570)                                                      |                                       **Reasoning Guardrails**&**Jailbreak Attack**&**LLM Safety**                                        |
| 25.10 |                                                                                     Independent Research                                                                                     |                                   arxiv                                   |                                       [In-Browser LLM-Guided Fuzzing for Real-Time Prompt Injection Testing in Agentic AI Browsers](https://arxiv.org/abs/2510.13543)                                        |                                     **Prompt Injection**&**LLM Security Testing**&**Browser Agents**                                      |
| 25.10 |                                                                              VNPT AI & University of Wollongong                                                                              |                                   arxiv                                   |                                                    [RAID: Refusal-Aware and Integrated Decoding for Jailbreaking LLMs](https://arxiv.org/abs/2510.13901)                                                     |                               **Jailbreak Attack**&**Refusal-Aware Decoding**&**Adversarial Optimization**                                |
| 25.10 |                                                                           Duke University & Penn State University                                                                            |                               IEEE S&P 2026                               |                                                            [PromptLocate: Localizing Prompt Injection Attacks](https://arxiv.org/abs/2510.12252)                                                             |                                      **Prompt Injection**&**LLM Security**&**Forensic Localization**                                      |
| 25.10 |                                                                      The University of Tokyo, Xi‚Äôan Jiaotong University                                                                      |                                   arxiv                                   |                                            [Active Honeypot Guardrail System: Probing and Confirming Multi-Turn LLM Jailbreaks](https://arxiv.org/abs/2510.15017)                                            |                                         **Jailbreak Defense**&**Honeypot System**&**LLM Safety**                                          |
| 25.10 |                                                                360 AI Security Lab, Politecnico di Milano, Beihang University                                                                |                                   arxiv                                   |                                  [Sequential Comics for Jailbreaking Multimodal Large Language Models via Structured Visual Storytelling](https://arxiv.org/abs/2510.15068)                                  |                                   **Multimodal Jailbreak**&**Visual Storytelling**&**Safety Alignment**                                   |
| 25.10 |                                                                                   Universit√© Paris-Saclay                                                                                    |                                   arxiv                                   |                                              [PoTS: Proof-of-Training-Steps for Backdoor Detection in Large Language Models](https://arxiv.org/abs/2510.15106)                                               |                                       **Backdoor Detection**&**Proof-of-Training**&**LLM Security**                                       |
| 25.10 |                                                               Tsinghua University, Shanghai Artificial Intelligence Laboratory                                                               |                                   arxiv                                   |                                                    [HarmRLVR: Weaponizing Verifiable Rewards for Harmful LLM Alignment](https://arxiv.org/abs/2510.15499)                                                    |                                        **RLVR**&**Backdoor Alignment**&**Reinforcement Learning**                                         |
| 25.10 |                                                                ETH Zurich, University at Buffalo, Bytedance Security Research                                                                |                                   arxiv                                   |                                                      [Black-box Optimization of LLM Outputs by Asking for Directions](https://arxiv.org/abs/2510.16794)                                                      |                                **Black-box Attacks**&**Confidence Calibration**&**Jailbreak Optimization**                                |
| 25.10 |                                                 University of Central Florida, Mississippi State University, North Carolina State University                                                 |                                   arxiv                                   |                                  [Can Transformer Memory Be Corrupted? Investigating Cache-Side Vulnerabilities in Large Language Models](https://arxiv.org/abs/2510.17098)                                  |                                    **Transformer Security**&**KV Cache**&**Adversarial Perturbation**                                     |
| 25.10 |                                                                           Sichuan University, Zhejiang University                                                                            |                                   arxiv                                   |                                         [Multimodal Safety Is Asymmetric: Cross-Modal Exploits Unlock Black-Box MLLMs Jailbreaks](https://arxiv.org/abs/2510.17277)                                          |                        **Multimodal Large Language Models**&**Jailbreak Attacks**&**Cross-Modal Vulnerabilities**                         |
| 25.10 |                                                                                      Purdue University                                                                                       |                                   arxiv                                   |                                             [VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models](https://arxiv.org/abs/2510.17759)                                              |                                **Vision-Language Models**&**Jailbreak Attacks**&**Variational Inference**                                 |
| 25.10 |                                                                 Yildiz Technical University, Intellica Business Intelligence                                                                 |                                   arxiv                                   |                                                           [BreakFun: Jailbreaking LLMs via Schema Exploitation](https://arxiv.org/abs/2510.17904)                                                            |                                       **Jailbreak Attacks**&**LLM Safety**&**Structured Reasoning**                                       |
| 25.10 |                                                                     Case Western Reserve University, Tel Aviv University                                                                     |                                   arxiv                                   |                                             [Exploring Membership Inference Vulnerabilities in Clinical Large Language Models](https://arxiv.org/abs/2510.18674)                                             |                                 **Membership Inference Attacks**&**Healthcare Privacy**&**Clinical LLMs**                                 |
| 25.10 |                                                                        Old Dominion University, University of Arizona                                                                        |                                   arxiv                                   |                                         [HarmNet: A Framework for Adaptive Multi-Turn Jailbreak Attacks on Large Language Models](https://arxiv.org/abs/2510.18728)                                          |                                  **Jailbreak Attacks**&**Adversarial Framework**&**Multi-turn Dialogue**                                  |
| 25.10 |                                                                                            MBZUAI                                                                                            |                               NeurIPS 2025                                |                                         [Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs](https://arxiv.org/abs/2510.17000)                                          |                                  **Information-Theoretic Bound**&**LLM Security**&**Adversarial Attack**                                  |
| 25.10 | University of Oxford, Humane Intelligence, Amazon, Infocomm Media Development Authority, Carnegie Mellon University | arxiv | [Ask What Your Country Can Do For You: Towards a Public Red Teaming Model](https://arxiv.org/abs/2510.20061) | **AI Safety Evaluation**&**Public Red Teaming**&**Responsible AI Governance** |
| 25.10 | Enkrypt AI | arxiv | [Beyond Text: Multimodal Jailbreaking of Vision-Language and Audio Models through Perceptually Simple Transformations](https://arxiv.org/abs/2510.20223) | **Multimodal Jailbreak**&**Vision-Language Models**&**Audio-Language Models** |
| 25.10 | ST Engineering | arxiv | [Enhanced MLLM Black-Box Jailbreaking Attacks and Defenses](https://arxiv.org/abs/2510.21214) | **Multimodal Large Language Models**&**Black-box Jailbreak Attack**&**Defense Mechanisms** |
| 25.10 | University of Guelph | CIKM 2025 | [Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks](https://arxiv.org/abs/2510.21983v1) | **LLM Security**&**Jailbreak Prompts**&**Persuasion Principles** |
| 25.10 | University of Athens | arxiv | [Jailbreak Mimicry: Automated Discovery of Narrative-Based Jailbreaks for Large Language Models](https://arxiv.org/abs/2510.22085v1) | **Jailbreak Automation**&**Narrative Reframing**&**AI Safety** |
| 25.10 | The Hong Kong University of Science and Technology | arxiv | [CompressionAttack: Exploiting Prompt Compression as a New Attack Surface in LLM-Powered Agents](https://arxiv.org/abs/2510.22963v1) | **Prompt Compression**&**Adversarial Attacks**&**AI Agent Security** |
| 25.10 | Independent Researcher, Nikkei Inc. | arxiv | [Fast-MIA: Efficient and Scalable Membership Inference for LLMs](https://arxiv.org/abs/2510.23074v1) | **Membership Inference Attack**&**LLM Privacy**&**Benchmark Library** |
| 25.10 | The Hong Kong University of Science and Technology, Fudan University | arxiv | [QueryIPI: Query-agnostic Indirect Prompt Injection on Coding Agents](https://arxiv.org/abs/2510.23675v1) | **Indirect Prompt Injection**&**LLM Agent Security**&**Query-Agnostic Attack** |
| 25.10 | University of California, Santa Cruz; Microsoft Responsible AI Research | arxiv | [SIRAJ: Diverse and Efficient Red-Teaming for LLM Agents via Distilled Structured Reasoning](https://arxiv.org/abs/2510.26037v1) | **Red-Teaming**&**Structured Reasoning**&**LLM Agents Safety** |
| 25.10 | CISPA Helmholtz Center for Information Security | NeurIPS 2025 | [Adjacent Words, Divergent Intents: Jailbreaking Large Language Models via Task Concurrency](https://arxiv.org/abs/2510.21189v1) | **Jailbreak Attack**&**Task Concurrency**&**LLM Safety** |
| 25.11 | Federal University of Rio Grande do Sul | arxiv | [Exploiting Latent Space Discontinuities for Building Universal LLM Jailbreaks and Data Extraction Attacks](https://arxiv.org/abs/2511.00346) | **Latent Space**&**Universal Jailbreak**&**Data Extraction Attack** |
| 25.11 | Nanjing University | arxiv | [Friend or Foe: How LLMs‚Äô Safety Mind Gets Fooled by Intent Shift Attack](https://arxiv.org/abs/2511.00556) | **Intent Shift Attack**&**Jailbreak Defense**&**Safety Alignment** |
| 25.11 | HiddenLayer, Inc. | Applied Machine Learning for Information Security (PMLR) | [ShadowLogic: Backdoors in Any Whitebox LLM](https://arxiv.org/abs/2511.00664) | **Whitebox LLM**&**Computational Graph**&**Backdoor Injection** |
| 25.11 | Institute of Information Engineering, Chinese Academy of Sciences | arxiv | [‚ÄúGive a Positive Review Only‚Äù: An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers](https://arxiv.org/abs/2511.01287) | **Prompt Injection**&**AI Review Systems**&**Adversarial Defense** |
| 25.11 | Yonsei University | arxiv | [Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges](https://arxiv.org/abs/2511.01375) | **Jailbreak Optimization**&**Meta-Optimization**&**LLM Safety** |
| 25.11 | University of California, Berkeley | arxiv | [AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2511.02376) | **Jailbreak Attack**&**Adversarial Prompting**&**Multi-Turn Interaction** |
| 25.11 | University of Agder | arxiv | [On The Dangers of Poisoned LLMs in Security Automation](https://arxiv.org/abs/2511.02600) | **LLM Poisoning**&**Security Automation**&**Fine-tuning Vulnerability** |
| 25.11 | Harvard University | arxiv | [Verifying LLM Inference to Prevent Model Weight Exfiltration](https://arxiv.org/abs/2511.02620) | **Model Weight Exfiltration**&**Inference Verification**&**LLM Security** |
| 25.11 | Communication University of China | arxiv | [Let the Bees Find the Weak Spots: A Path Planning Perspective on Multi-Turn Jailbreak Attacks against LLMs](https://arxiv.org/abs/2511.03271) | **Multi-turn Jailbreak**&**Path Planning**&**Swarm Intelligence** |
| 25.11 | City University of Hong Kong | arxiv | [Black-Box Guardrail Reverse-engineering Attack](https://arxiv.org/abs/2511.04215) | **Guardrail Reverse Engineering**&**Reinforcement Learning**&**Model Extraction** |
| 25.11 | Carnegie Mellon University | arxiv | [Jailbreaking in the Haystack](https://arxiv.org/abs/2511.04707) | **Jailbreaking**&**Long-context Safety**&**NINJA Attack** |
| 25.11 | University of California | IEEE Symposium on Security and Privacy 2026 | [When AI Meets the Web: Prompt Injection Risks in Third-Party AI Chatbot Plugins](https://arxiv.org/abs/2511.05797) | **Prompt Injection**&**Web Chatbot Plugins**&**LLM Security** |
| 25.11 | Technical University of Munich | arxiv | [Injecting Falsehoods: Adversarial Man-in-the-Middle Attacks Undermining Factual Recall in LLMs](https://arxiv.org/abs/2511.05919) | **Prompt Injection**&**Man-in-the-Middle Attack**&**Factual Recall** |
| 25.11 | Tsinghua University | arxiv | [JPRO: Automated Multimodal Jailbreaking via Multi-Agent Collaboration Framework](https://arxiv.org/abs/2511.07315) | **Multimodal Jailbreak**&**Multi-Agent Collaboration**&**VLM Security** |
| 25.11 | Tsinghua University | arxiv | [Why does weak-OOD help? A Further Step Towards Understanding Jailbreaking VLMs](https://arxiv.org/abs/2511.08367) | **VLM Jailbreak**&**Weak-OOD**&**OCR Robustness** |
| 25.11 | Soongsil University | arxiv | [Self-HarmLLM: Can Large Language Model Harm Itself?](https://arxiv.org/abs/2511.08597) | **Self-HarmLLM**&**Jailbreak**&**LLM Safety** |
| 25.11 | Independent Researcher | arxiv | [Say It Differently: Linguistic Styles as Jailbreak Vectors](https://arxiv.org/abs/2511.10519) | **Linguistic Style**&**Jailbreak Attacks**&**LLM Safety** |
| 25.11 | China Academy of Engineering Physics | AAAI 2026 | [LoopLLM: Transferable Energy-Latency Attacks in LLMs via Repetitive Generation](https://arxiv.org/abs/2511.07876) | **Energy-Latency Attacks**&**Repetitive Generation**&**Transferable Adversarial Prompts** |
| 25.11 | Tongji University | AAAI 2026 | [Uncovering Pretraining Code in LLMs: A Syntax-Aware Attribution Approach](https://arxiv.org/abs/2511.07033) | **Membership Inference**&**Code Copyright Auditing**&**Syntax-aware Attribution** |
| 25.11 | Institute of Software, Chinese Academy of Sciences / Shandong University | arxiv | [BudgetLeak: Membership Inference Attacks on RAG Systems via the Generation Budget Side Channel](https://arxiv.org/abs/2511.12043) | **Membership Inference Attack**&**RAG Security**&**Side-channel Attack** |
| 25.11 | hydrox.ai | arxiv | [The ‚ÄòSure‚Äô Trap: Multi-Scale Poisoning Analysis of Stealthy Compliance-Only Backdoors in Fine-Tuned Large Language Models](https://arxiv.org/abs/2511.12414) | **Backdoor Attacks**&**Compliance-only Backdoor**&**Fine-tuning Security** |
| 25.11 | Fudan University / Shanghai Artificial Intelligence Laboratory | arxiv | [Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs](https://arxiv.org/abs/2511.12710) | **Jailbreak Attacks**&**Evolutionary Synthesis**&**Automated Red Teaming** |
| 25.11 | University of Southern California | arxiv | [Whose Narrative is it Anyway? A KV Cache Manipulation Attack](https://arxiv.org/abs/2511.12752) | **KV Cache Manipulation**&**History Swapping Attack**&**LLM Internal State Integrity** |
| 25.11 | iFLYTEK Security Laboratory | arxiv | [FORGEDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models](https://arxiv.org/abs/2511.13548) | **Jailbreak Attack**&**Evolutionary Algorithm**&**Adversarial Prompt Generation** |
| 25.11 | Johns Hopkins University Applied Physics Laboratory (APL) / University of Maryland Baltimore County | arxiv | [Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments](https://arxiv.org/abs/2511.13788) | **Adversarial Alignment**&**Multi-LLM Jailbreak**&**Scaling Laws** |
| 25.11 | Florida International University | arxiv | [Jailbreaking Large Vision Language Models in Intelligent Transportation Systems](https://arxiv.org/abs/2511.13892) | **Jailbreaking LVLMs**&**Intelligent Transportation Systems**&**Typography-based Attack** |
| 25.11 | The Hong Kong University of Science and Technology (Guangzhou) | arxiv | [GRPO Privacy Is at Risk: A Membership Inference Attack Against Reinforcement Learning With Verifiable Rewards](https://arxiv.org/abs/2511.14045) | **Membership Inference Attack**&**RLVR Privacy**&**Behavioral Divergence** |
| 25.11 | Beijing Jiaotong University | arxiv | [Membership Inference Attack against Large Language Model-based Recommendation Systems: A New Distillation-based Paradigm](https://arxiv.org/abs/2511.14763) | **Membership Inference Attack**&**LLM-based Recommendation Systems**&**Knowledge Distillation** |
| 25.11 | Sapienza University of Rome | arxiv | [Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models](https://arxiv.org/abs/2511.15304) | **Adversarial Poetry**&**Jailbreak Attacks**&**LLM Safety** |
| 25.11 | The Hong Kong Polytechnic University | arxiv | [HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation](https://arxiv.org/abs/2511.15435) | **Hierarchical Visual Attack**&**Multimodal Retrieval-Augmented Generation**&**Adversarial Robustness** |
| 25.11 | Westlake University | arxiv | [When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.16203) | **Vision-Language-Action Models**&**Adversarial Attacks**&**Multimodal Robustness** |
| 25.11 | The Hong Kong University of Science and Technology (Guangzhou) | arxiv | [‚ÄúTo Survive, I Must Defect‚Äù: Jailbreaking LLMs via the Game-Theory Scenarios](https://arxiv.org/abs/2511.16278) | **Jailbreak Attacks**&**Game-Theoretic Scenarios**&**Black-Box Red Teaming** |
| 25.11 | The Chinese University of Hong Kong | arxiv | [Multi-Faceted Attack: Exposing Cross-Model Vulnerabilities in Defense-Equipped Vision-Language Models](https://arxiv.org/abs/2511.16110) | **Multi-Faceted Attack**&**Vision-Language Models**&**Safety Robustness** |
| 25.11 | Southwest University | AAAI 2026 | [BadThink: Triggered Overthinking Attacks on Chain-of-Thought Reasoning in Large Language Models](https://arxiv.org/abs/2511.10714) | **Backdoor Attacks**&**Chain-of-Thought Reasoning**&**Overthinking Attacks** |




## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |        Type        |                                       Title                                       |                                  URL                                  |
|:-----:|:------------------:|:---------------------------------------------------------------------------------:|:---------------------------------------------------------------------:|
| 23.01 |     Community      |                              Reddit/ChatGPTJailbrek                               |           [link](https://www.reddit.com/r/ChatGPTJailbreak)           |
| 23.02 | Resource&Tutorials |                                  Jailbreak Chat                                   |                [link](https://www.jailbreakchat.com/)                 |
| 23.10 |     Tutorials      |                                Awesome-LLM-Safety                                 |         [link](https://github.com/ydyjya/Awesome-LLM-Safety)          |
| 23.10 |      Article       |                 Adversarial Attacks on LLMs(Author: Lilian Weng)                  | [link](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/) |
| 23.11 |       Video        | [1hr Talk] Intro to Large Language Models<br/>From 45:45(Author: Andrej Karpathy) |          [link](https://www.youtube.com/watch?v=zjkBMFhNj_g)          |

## üì∞News & Articles

| Date  |  Type   |            Title            |   Author    |                                  URL                                  |
|:-----:|:-------:|:---------------------------:|:-----------:|:---------------------------------------------------------------------:|
| 23.10 | Article | Adversarial Attacks on LLMs | Lilian Weng | [link](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/) |


## üßë‚Äçüè´Scholars