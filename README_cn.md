# ğŸ›¡ï¸Awesome LLM-SafetyğŸ›¡ï¸[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

<p align="center">
<a href=""> <img src="https://img.shields.io/github/stars/ydyjya/Awesome-LLM-Safety?style=flat-square&logo=github" alt="GitHub stars"></a>
<a href=""> <img src="https://img.shields.io/github/forks/ydyjya/Awesome-LLM-Safety?style=flat-square&logo=github" alt="GitHub forks"></a>
<a href=""> <img src="https://img.shields.io/github/issues/ydyjya/Awesome-LLM-Safety?style=flat-square&logo=github" alt="GitHub issues"></a>
<a href=""> <img src="https://img.shields.io/github/last-commit/ydyjya/Awesome-LLM-Safety?style=flat-square&logo=github" alt="GitHub Last commit"></a>
</p>

<div align="center">

[English](README.md) | ä¸­æ–‡

</div>

## ğŸ¤—ä»‹ç»

è¿™æ˜¯ä¸€ä¸ªæœ‰å…³llm-safetyçš„å®è—ä»“åº“ï¼ğŸ¥°ğŸ¥°ğŸ¥°

**ğŸ§‘â€ğŸ’»æˆ‘ä»¬çš„å·¥ä½œï¼š**
æˆ‘ä»¬ç²¾å¿ƒæŒ‘é€‰å¹¶ç½—åˆ—äº†æœ‰å…³å¤§æ¨¡å‹å®‰å…¨æ–¹é¢ï¼ˆllm-safetyï¼‰æœ€æ–°ğŸ˜‹ã€æœ€å…¨é¢ğŸ˜ã€æœ€æœ‰ä»·å€¼ğŸ¤©çš„è®ºæ–‡ã€‚ä¸ä»…å¦‚æ­¤ï¼Œæˆ‘ä»¬è¿˜é™„ä¸Šäº†æœ‰å…³çš„æ¼”è®²ã€æ•™ç¨‹ã€ä¼šè®®ã€æ–°é—»ä»¥åŠæ–‡ç« ã€‚è¿™ä¸ªä»“åº“å°†å®æ—¶æ›´æ–°ï¼Œä¿è¯ç¬¬ä¸€æ‰‹èµ„æ–™ã€‚

>å¦‚æœä¸€ä»½èµ„æ–™åŒæ—¶å±äºå¤šä¸ªå­åˆ†ç±»ï¼Œé‚£ä¹ˆå®ƒå°†è¢«åŒæ—¶æ”¾åœ¨è¿™äº›å­åˆ†ç±»ä¸‹ã€‚æ¯”å¦‚ â€œ Awesome-LLM-Safetyâ€è¿™ä¸ªä»“åº“å°†è¢«æ”¾åœ¨æ¯ä¸ªå­åˆ†ç±»ä¸‹ã€‚

**âœ”ï¸é€‚åˆå¤šæ•°äººï¼š**
- å¯¹äºå¸Œæœ›äº†è§£llm-safetyçš„åˆå­¦è€…ï¼Œè¿™ä¸ªä»“åº“å¯ä»¥ä½œä¸ºä½ æŠŠæ¡æ¡†æ¶ï¼Œå¹¶äº†è§£ç»†èŠ‚çš„å¯¼èˆªã€‚æˆ‘ä»¬åœ¨READMEä¸­ä¿ç•™äº†æ¯”è¾ƒç»å…¸æˆ–æœ‰å½±å“åŠ›çš„è®ºæ–‡ï¼Œå¯¹åˆå­¦è€…å¯»æ‰¾æ„Ÿå…´è¶£çš„æ–¹å‘ååˆ†å‹å¥½ï¼›
- å¯¹äºèµ„æ·±çš„ç ”ç©¶è€…ï¼Œè¿™ä¸ªä»“åº“å¯ä»¥ä½œä¸ºä½ äº†è§£å®å†µä¿¡æ¯ã€æŸ¥æ¼è¡¥ç¼ºçš„å·¥å…·ï¼Œåœ¨subtopicä¸­ï¼Œæˆ‘ä»¬æ­£åœ¨åŠªåŠ›æ›´æ–°è¿™ä¸ªsubtopicä¸‹çš„æ‰€æœ‰æœ€æ–°å†…å®¹ï¼Œå¹¶ä¸”å°†ä¼šä¸æ–­è¡¥å®Œä¹‹å‰çš„å†…å®¹ã€‚å…¨é¢çš„èµ„æ–™æœé›†ä»¥åŠç”¨å¿ƒåœ°ç­›é€‰å¯ä»¥å¸®åŠ©ä½ èŠ‚çœæ—¶é—´ï¼›

**ğŸ§­ä½¿ç”¨æŒ‡å—ï¼š**
- ç®€ç•¥ç‰ˆï¼šåœ¨READMEä¸­ï¼Œä½¿ç”¨è€…å¯ä»¥æ‰¾åˆ°æŒ‰æ—¶é—´æ’åˆ—å¥½çš„ç²¾é€‰èµ„è®¯ï¼Œä»¥åŠå„ç§å’¨è¯¢çš„é“¾æ¥
- è¯¦ç»†ç‰ˆï¼šå¦‚æœå¯¹æŸä¸€å­è¯é¢˜ç‰¹åˆ«æ„Ÿå…´è¶£ï¼Œå¯ä»¥ç‚¹å¼€â€œsubtopicâ€æ–‡ä»¶å¤¹ï¼Œè¿›ä¸€æ­¥äº†è§£ã€‚é‡Œé¢æœ‰å¯¹æ¯ç¯‡æ–‡ç« æˆ–è€…èµ„è®¯çš„ç®€ç•¥ä»‹ç»ï¼Œå¯ä»¥å¸®åŠ©ç ”ç©¶è€…å¿«é€Ÿé”å®šå†…å®¹ã€‚

<center>ğŸ¥°ğŸ¥°ğŸ¥°è®©æˆ‘ä»¬å¼€å§‹llm-safetyå­¦ä¹ ä¹‹æ—…å§ğŸ¥°ğŸ¥°ğŸ¥°</center>

---

## ğŸš€ç›®å½•

- [ğŸ›¡ï¸Awesome LLM-SafetyğŸ›¡ï¸](#ï¸awesome-llm-safetyï¸)
  - [ğŸ¤—ä»‹ç»](#ä»‹ç»)
  - [ğŸš€ç›®å½•](#ç›®å½•)
  - [ğŸ”æ¨¡å‹å®‰å…¨ï¼ˆSecurityï¼‰](#æ¨¡å‹å®‰å…¨security)
    - [ğŸ“‘è®ºæ–‡](#è®ºæ–‡)
    - [ğŸ“–æ•™ç¨‹, æ–‡ç« , æ¼”ç¤º, æ¼”è®²](#æ•™ç¨‹-æ–‡ç« -æ¼”ç¤º-æ¼”è®²)
    - [å…¶ä»–](#å…¶ä»–)
  - [ğŸ”éšç§ä¿æŠ¤ï¼ˆPrivacy Tutorialï¼‰](#éšç§ä¿æŠ¤privacy-tutorial)
    - [ğŸ“‘è®ºæ–‡](#è®ºæ–‡-1)
    - [ğŸ“–æ•™ç¨‹, æ–‡ç« , æ¼”ç¤º, æ¼”è®²](#æ•™ç¨‹-æ–‡ç« -æ¼”ç¤º-æ¼”è®²-1)
    - [å…¶ä»–](#å…¶ä»–-1)
  - [ğŸ“°äº‹å®æ€§\&é”™è¯¯ä¿¡æ¯ï¼ˆTruthfulness \& Misinformationï¼‰](#äº‹å®æ€§é”™è¯¯ä¿¡æ¯truthfulness--misinformation)
    - [ğŸ“‘è®ºæ–‡](#è®ºæ–‡-2)
    - [ğŸ“–æ•™ç¨‹, æ–‡ç« , æ¼”ç¤º, æ¼”è®²](#æ•™ç¨‹-æ–‡ç« -æ¼”ç¤º-æ¼”è®²-2)
    - [å…¶ä»–](#å…¶ä»–-2)
  - [ğŸ˜ˆè¶Šç‹±\&æ”»å‡»ï¼ˆJailBreak \& Attacksï¼‰](#è¶Šç‹±æ”»å‡»jailbreak--attacks)
    - [ğŸ“‘è®ºæ–‡](#è®ºæ–‡-3)
    - [ğŸ“–æ•™ç¨‹, æ–‡ç« , æ¼”ç¤º, æ¼”è®²](#æ•™ç¨‹-æ–‡ç« -æ¼”ç¤º-æ¼”è®²-3)
    - [å…¶ä»–](#å…¶ä»–-3)
  - [ğŸ›¡ï¸é˜²å¾¡æªæ–½ï¼ˆDefensesï¼‰](#ï¸é˜²å¾¡æªæ–½defenses)
    - [ğŸ“–æ•™ç¨‹, æ–‡ç« , æ¼”ç¤º, æ¼”è®²](#æ•™ç¨‹-æ–‡ç« -æ¼”ç¤º-æ¼”è®²-4)
    - [å…¶ä»–](#å…¶ä»–-4)
  - [ğŸ’¯æ•°æ®é›† \& è¯„æµ‹åŸºå‡†ï¼ˆDatasets \& Benchmarkï¼‰](#æ•°æ®é›†--è¯„æµ‹åŸºå‡†datasets--benchmark)
    - [ğŸ“‘è®ºæ–‡](#è®ºæ–‡-4)
    - [ğŸ“–æ•™ç¨‹, æ–‡ç« , æ¼”ç¤º, æ¼”è®²](#æ•™ç¨‹-æ–‡ç« -æ¼”ç¤º-æ¼”è®²-5)
    - [ğŸ“šèµ„æºğŸ“š](#èµ„æº)
    - [å…¶ä»–](#å…¶ä»–-5)
  - [ğŸ§‘â€ğŸ« å­¦è€… ğŸ‘©â€ğŸ«](#-å­¦è€…-)
  - [ğŸ§‘â€ğŸ“ä½œè€…ä¿¡æ¯](#ä½œè€…ä¿¡æ¯)

---
## ğŸ”æ¨¡å‹å®‰å…¨ï¼ˆSecurityï¼‰

### ğŸ“‘è®ºæ–‡
|æ—¥æœŸ|æœºæ„|   å‡ºç‰ˆä¿¡æ¯   |è®ºæ–‡&é“¾æ¥|
|:-:|:-:|:--------:|:-:|
|20.10|Facebook AI Research|  arxiv   |[Recipes for Safety in Open-domain Chatbots](https://arxiv.org/abs/2010.07079)|
|23.07|UC Berkeley| NIPS2023 |[Jailbroken: How Does LLM Safety Training Fail?](https://arxiv.org/abs/2307.02483)|
|23.07|CMU|  arxiv   |[Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)|


### ğŸ“–æ•™ç¨‹, æ–‡ç« , æ¼”ç¤º, æ¼”è®²

|æ—¥æœŸ|åˆ†ç±»|æ ‡é¢˜|é“¾æ¥åœ°å€|
|:-:|:-:|:-:|:-:|
|23.10|æ•™ç¨‹|Awesome-LLM-Safety|[é“¾æ¥](https://github.com/ydyjya/Awesome-LLM-Safety)|


### å…¶ä»–

ğŸ‘‰[Latest&Comprehensive Security Paper](.//subtopic/Security.md)

---
## ğŸ”éšç§ä¿æŠ¤ï¼ˆPrivacy Tutorialï¼‰


### ğŸ“‘è®ºæ–‡

|  æ—¥æœŸ   |       æœºæ„        |   å‡ºç‰ˆä¿¡æ¯   |                                                           è®ºæ–‡&é“¾æ¥                                                           |
|:-----:|:---------------:|:--------:|:-------------------------------------------------------------------------------------------------------------------------:|
| 19.12 |    Microsoft    | CCS2020  | [Analyzing Information Leakage of Updates to Natural Language Models](https://dl.acm.org/doi/abs/10.1145/3372297.3417880) |
| 21.07 | Google Research | ACL2022  |          [Deduplicating Training Data Makes Language Models Better](https://aclanthology.org/2022.acl-long.577/)          |
| 21.10 |    Stanford     | ICLR2022 |    [Large language models can be strong differentially private learners](https://openreview.net/forum?id=bVuP3ltATMz)     |
| 22.02 | Google Research | ICLR2023 |           [Quantifying Memorization Across Neural Language Models](https://openreview.net/forum?id=TatRHT_1cK)            |


### ğŸ“–æ•™ç¨‹, æ–‡ç« , æ¼”ç¤º, æ¼”è®²

|æ—¥æœŸ|åˆ†ç±»|æ ‡é¢˜|é“¾æ¥åœ°å€|
|:-:|:-:|:-:|:-:|
|23.10|æ•™ç¨‹|Awesome-LLM-Safety|[é“¾æ¥](https://github.com/ydyjya/Awesome-LLM-Safety)|


### å…¶ä»–

ğŸ‘‰[Latest&Comprehensive Privacy Paper](.//subtopic/Privacy.md)

---
## ğŸ“°äº‹å®æ€§&é”™è¯¯ä¿¡æ¯ï¼ˆTruthfulness & Misinformationï¼‰


### ğŸ“‘è®ºæ–‡
|  æ—¥æœŸ   |               æœºæ„               |  å‡ºç‰ˆä¿¡æ¯   |                                                                    è®ºæ–‡&é“¾æ¥                                                                     |
|:-----:|:------------------------------:|:-------:|:--------------------------------------------------------------------------------------------------------------------------------------------:|
| 21.09 |      University of Oxford      | ACL2022 |                         [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)                          |
| 23.11 | Harbin Institute of Technology |  arxiv  | [A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/abs/2311.05232) |
| 23.11 |    Arizona State University    |  arxiv  |                      [Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey](https://arxiv.org/abs/2311.07914)                      |

### ğŸ“–æ•™ç¨‹, æ–‡ç« , æ¼”ç¤º, æ¼”è®²

|  æ—¥æœŸ   | åˆ†ç±» |          æ ‡é¢˜           |                            é“¾æ¥åœ°å€                             |
|:-----:|:--:|:---------------------:|:-----------------------------------------------------------:|
| 23.10 | ä»“åº“ | LLM-Factuality-Survey | [é“¾æ¥](https://github.com/wangcunxiang/LLM-Factuality-Survey) |
| 23.10 | æ•™ç¨‹ |  Awesome-LLM-Safety   |     [é“¾æ¥](https://github.com/ydyjya/Awesome-LLM-Safety)      |

### å…¶ä»–

ğŸ‘‰[Latest&Comprehensive Truthfulness&Misinformation Paper](./subtopic/Truthfulness&Misinformation.md)

---
## ğŸ˜ˆè¶Šç‹±&æ”»å‡»ï¼ˆJailBreak & Attacksï¼‰

### ğŸ“‘è®ºæ–‡
|  æ—¥æœŸ   |    æœºæ„     |             å‡ºç‰ˆä¿¡æ¯             |                                                                   è®ºæ–‡&é“¾æ¥                                                                   |
|:-----:|:---------:|:----------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------:|
| 20.12 |  Google   |     USENIX Security 2021     | [Extracting Training Data from Large Language Models](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting) |
| 22.11 | AE Studio | NIPS2022(ML Safety Workshop) |                     [Ignore Previous Prompt: Attack Techniques For Language Models](https://arxiv.org/abs/2211.09527)                     |
| 23.06 |  Google   |            arxiv             |                          [Are aligned neural networks adversarially aligned?](https://arxiv.org/abs/2306.15447)                           |
| 23.07 |    CMU    |            arxiv             |               [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)               |

### ğŸ“–æ•™ç¨‹, æ–‡ç« , æ¼”ç¤º, æ¼”è®²

|æ—¥æœŸ|åˆ†ç±»|æ ‡é¢˜|é“¾æ¥åœ°å€|
|:-:|:-:|:-:|:-:|
|23.10|æ•™ç¨‹|Awesome-LLM-Safety|[é“¾æ¥](https://github.com/ydyjya/Awesome-LLM-Safety)|

### å…¶ä»–

ğŸ‘‰[Latest&Comprehensive JailBreak & Attacks Paper](./subtopic/Jailbreaks&Attack.md)

---
## ğŸ›¡ï¸é˜²å¾¡æªæ–½ï¼ˆDefensesï¼‰

### ğŸ“‘è®ºæ–‡
|æ—¥æœŸ|æœºæ„|å‡ºç‰ˆä¿¡æ¯|è®ºæ–‡&é“¾æ¥|
|:-:|:-:|:-:|:-:|
|21.07|Google Research|ACL2022|[Deduplicating Training Data Makes Language Models Better](https://aclanthology.org/2022.acl-long.577/)|
|22.04|Anthropic|arxiv|[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2204.05862)|



### ğŸ“–æ•™ç¨‹, æ–‡ç« , æ¼”ç¤º, æ¼”è®²

|æ—¥æœŸ|åˆ†ç±»|æ ‡é¢˜|é“¾æ¥åœ°å€|
|:-:|:-:|:-:|:-:|
|23.10|æ•™ç¨‹|Awesome-LLM-Safety|[é“¾æ¥](https://github.com/ydyjya/Awesome-LLM-Safety)|

### å…¶ä»–

ğŸ‘‰[Latest&Comprehensive Defenses Paper](./subtopic/Defenses.md)


--- 
## ğŸ’¯æ•°æ®é›† & è¯„æµ‹åŸºå‡†ï¼ˆDatasets & Benchmarkï¼‰

### ğŸ“‘è®ºæ–‡
|æ—¥æœŸ|æœºæ„|å‡ºç‰ˆä¿¡æ¯|è®ºæ–‡&é“¾æ¥|
|:-:|:-:|:-:|:-:|
|20.09|University of Washington|EMNLP2020(findings)|[RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/abs/2009.11462)|
|21.09|University of Oxford|ACL2022|[TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)|
|22.03|MIT|ACL2022|[ToxiGen: A Large-Scale Machine-Generated datasets for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/abs/2203.09509)|


### ğŸ“–æ•™ç¨‹, æ–‡ç« , æ¼”ç¤º, æ¼”è®²

|æ—¥æœŸ|åˆ†ç±»|æ ‡é¢˜|é“¾æ¥åœ°å€|
|:-:|:-:|:-:|:-:|
|23.10|æ•™ç¨‹|Awesome-LLM-Safety|[é“¾æ¥](https://github.com/ydyjya/Awesome-LLM-Safety)|

### ğŸ“šèµ„æºğŸ“š
- Toxicity - [RealToxicityPrompts datasets](https://toxicdegeneration.allenai.org/)
- Truthfulness - [TruthfulQA datasets](https://github.com/sylinrl/TruthfulQA)

### å…¶ä»–
ğŸ‘‰[Latest&Comprehensive datasets & Benchmark Paper](./subtopic/Datasets&Benchmark.md)

---
## ğŸ§‘â€ğŸ« å­¦è€… ğŸ‘©â€ğŸ« 
**åœ¨è¿™ä¸ªéƒ¨åˆ†ï¼Œæˆ‘ä»¬ä¼šåˆ—å‡ºä¸€äº›æˆ‘ä»¬è§‰å¾—åœ¨LLM Safetyé¢†åŸŸå¾ˆæœ‰å»ºæ ‘çš„ç ”ç©¶è€…!**

|å­¦è€…|ä¸»é¡µ&è°·æ­Œå­¦æœ¯|å…³é”®è¯&å…´è¶£|
|:-:|:-:|:-:|
|Nicholas Carlini|[ä¸»é¡µ](https://nicholas.carlini.com/) \| [è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?hl=zh-CN&user=q4qDvAoAAAAJ&view_op=list_works&sortby=pubdate)|**the intersection of machine learning and computer security**&**neural networks from an adversarial perspective**|
|Daphne Ippolito|[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?hl=zh-CN&user=COEsqLYAAAAJ&view_op=list_works&sortby=pubdate)|**Natural Language Processing**|
|Chiyuan Zhang|[ä¸»é¡µ](https://pluskid.org/) \| [è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?hl=zh-CN&user=l_G2vr0AAAAJ&view_op=list_works&sortby=pubdate) |**Especially interested in understanding the generalization and memorization in machine and human learning, as well as implications in related areas like privacy**|
|Katherine Lee|[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?hl=zh-CN&user=bjdB4K8AAAAJ&view_op=list_works&sortby=pubdate)|**natural language processing**&**translation**&**machine learning**&**computational neuroscienceattention**|
|Florian TramÃ¨r|[ä¸»é¡µ](https://floriantramer.com/) \| [è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?hl=zh-CN&user=ijH0-a8AAAAJ)|**Computer Security**&**Machine Learning**&**Cryptography**&**the worst-case behavior of Deep Learning systems from an adversarial perspective, to understand and mitigate long-term threats to the safety and privacy of users**|
|Jindong Wang| [ä¸»é¡µ](https://scholar.google.com/citations?hl=zh-CN&user=hBZ_tKsAAAAJ&view_op=list_works&sortby=pubdate) \| [è°·æ­Œå­¦æœ¯](https://jd92.wang/) | **Large Language Models (LLMs) evaluation and robustness enhancement** ã€€|
|Andy Zou|[ä¸»é¡µ](https://andyzoujm.github.io/) \| [è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?hl=zh-CN&user=zir09KwAAAAJ)|**ML Safety**&**AI Safety**|
|Jie Huang|[ä¸»é¡µ](https://jeffhj.github.io/) \| [è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=GIoPkMoAAAAJ&hl=zh-CN&oi=sra)|**Knowledge: Factuality, Hallucination,Retrieval-Augmentation**&**Reasoning: Complex Reasoning, Faithful Reasoning**&**Ethics: Privacy Leakage Analysis, Citation**|

---
## ğŸ§‘â€ğŸ“ä½œè€…ä¿¡æ¯


**ğŸ¤—å¦‚æœä½ æœ‰ä»»ä½•ç–‘é—®æ¬¢è¿å’¨è¯¢ä½œè€…!ğŸ¤—**

âœ‰ï¸: [ydyjya](https://github.com/ydyjya) â¡ï¸ zhouzhenhong@bupt.edu.cn

ğŸ’¬: **äº¤æµLLM Safety**

<div align="center">

<div align="center">

[Wechat Group](./resource/wechat.png) | [My Wechat](./resource/wechat.png)

</div>

</div>

---

[![Star History Chart](https://api.star-history.com/svg?repos=ydyjya/Awesome-LLM-Safety&type=Date)](https://star-history.com/#ydyjya/Awesome-LLM-Safety&Date)

**[â¬† å›åˆ°é¡¶éƒ¨](#table-of-contents)**