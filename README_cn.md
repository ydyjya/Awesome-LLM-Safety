# 🛡️Awesome LLM-Safety🛡️[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

<p align="center">
<a href=""> <img src="https://img.shields.io/github/stars/ydyjya/Awesome-LLM-Safety?style=flat-square&logo=github" alt="GitHub stars"></a>
<a href=""> <img src="https://img.shields.io/github/forks/ydyjya/Awesome-LLM-Safety?style=flat-square&logo=github" alt="GitHub forks"></a>
<a href=""> <img src="https://img.shields.io/github/issues/ydyjya/Awesome-LLM-Safety?style=flat-square&logo=github" alt="GitHub issues"></a>
<a href=""> <img src="https://img.shields.io/github/last-commit/ydyjya/Awesome-LLM-Safety?style=flat-square&logo=github" alt="GitHub Last commit"></a>
</p>

<div align="center">

[English](README.md) | 中文

</div>

## 🤗介绍

这是一个有关llm-safety的宝藏仓库！🥰🥰🥰

**🧑‍💻我们的工作：**
我们精心挑选并罗列了有关大模型安全方面（llm-safety）最新😋、最全面😎、最有价值🤩的论文。不仅如此，我们还附上了有关的演讲、教程、会议、新闻以及文章。这个仓库将实时更新，保证第一手资料。

>如果一份资料同时属于多个子分类，那么它将被同时放在这些子分类下。比如 “ Awesome-LLM-Safety”这个仓库将被放在每个子分类下。

**✔️适合多数人：**
- 对于希望了解llm-safety的初学者，这个仓库可以作为你把握框架，并了解细节的导航。我们在README中保留了比较经典或有影响力的论文，对初学者寻找感兴趣的方向十分友好；
- 对于资深的研究者，这个仓库可以作为你了解实况信息、查漏补缺的工具，在subtopic中，我们正在努力更新这个subtopic下的所有最新内容，并且将会不断补完之前的内容。全面的资料搜集以及用心地筛选可以帮助你节省时间；

**🧭使用指南：**
- 简略版：在README中，使用者可以找到按时间排列好的精选资讯，以及各种咨询的链接
- 详细版：如果对某一子话题特别感兴趣，可以点开“subtopic”文件夹，进一步了解。里面有对每篇文章或者资讯的简略介绍，可以帮助研究者快速锁定内容。

<center>🥰🥰🥰让我们开始llm-safety学习之旅吧🥰🥰🥰</center>

---

## 🚀目录

- [🛡️Awesome LLM-Safety🛡️](#️awesome-llm-safety️)
  - [🤗介绍](#介绍)
  - [🚀目录](#目录)
  - [🔐模型安全（Security）](#模型安全security)
    - [📑论文](#论文)
    - [📖教程, 文章, 演示, 演讲](#教程-文章-演示-演讲)
    - [其他](#其他)
  - [🔏隐私保护（Privacy Tutorial）](#隐私保护privacy-tutorial)
    - [📑论文](#论文-1)
    - [📖教程, 文章, 演示, 演讲](#教程-文章-演示-演讲-1)
    - [其他](#其他-1)
  - [📰事实性\&错误信息（Truthfulness \& Misinformation）](#事实性错误信息truthfulness--misinformation)
    - [📑论文](#论文-2)
    - [📖教程, 文章, 演示, 演讲](#教程-文章-演示-演讲-2)
    - [其他](#其他-2)
  - [😈越狱\&攻击（JailBreak \& Attacks）](#越狱攻击jailbreak--attacks)
    - [📑论文](#论文-3)
    - [📖教程, 文章, 演示, 演讲](#教程-文章-演示-演讲-3)
    - [其他](#其他-3)
  - [🛡️防御措施（Defenses）](#️防御措施defenses)
    - [📖教程, 文章, 演示, 演讲](#教程-文章-演示-演讲-4)
    - [其他](#其他-4)
  - [💯数据集 \& 评测基准（Datasets \& Benchmark）](#数据集--评测基准datasets--benchmark)
    - [📑论文](#论文-4)
    - [📖教程, 文章, 演示, 演讲](#教程-文章-演示-演讲-5)
    - [📚资源📚](#资源)
    - [其他](#其他-5)
  - [🧑‍🏫 学者 👩‍🏫](#-学者-)
  - [🧑‍🎓作者信息](#作者信息)

---
## 🔐模型安全（Security）

### 📑论文
|  日期   |          机构          |   出版信息   |                                                                                            论文&链接                                                                                            |
|:-----:|:--------------------:|:--------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
| 20.10 | Facebook AI Research |  arxiv   |                                                       [Recipes for Safety in Open-domain Chatbots](https://arxiv.org/abs/2010.07079)                                                        |
| 22.03 |        OpenAI        | NIPS2022 | [Training language models to follow instructions with human feedback](https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html) |
| 23.07 |     UC Berkeley      | NIPS2023 |                                                     [Jailbroken: How Does LLM Safety Training Fail?](https://arxiv.org/abs/2307.02483)                                                      |
| 23.12 |        OpenAI        | Open AI  |                                 [Practices for Governing Agentic AI Systems](https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf)                                  |


### 📖教程, 文章, 演示, 演讲

|  日期   |   分类    |          标题          |                                      链接地址                                       |
|:-----:|:-------:|:--------------------:|:-------------------------------------------------------------------------------:|
| 22.02 | 毒性检测API |   Perspective API    | [链接](https://www.perspectiveapi.com/)<br/>[论文](https://arxiv.org/abs/2202.11176 |
| 23.07 |   仓库    | Awesome LLM Security |             [链接](https://github.com/corca-ai/awesome-llm-security)              |
| 23.10 |   教程    |  Awesome-LLM-Safety  |               [链接](https://github.com/ydyjya/Awesome-LLM-Safety)                |


### 其他

👉[Latest&Comprehensive Security Paper](.//subtopic/Security.md)

---
## 🔏隐私保护（Privacy Tutorial）


### 📑论文

|  日期   |       机构        |   出版信息   |                                                           论文&链接                                                           |
|:-----:|:---------------:|:--------:|:-------------------------------------------------------------------------------------------------------------------------:|
| 19.12 |    Microsoft    | CCS2020  | [Analyzing Information Leakage of Updates to Natural Language Models](https://dl.acm.org/doi/abs/10.1145/3372297.3417880) |
| 21.07 | Google Research | ACL2022  |          [Deduplicating Training Data Makes Language Models Better](https://aclanthology.org/2022.acl-long.577/)          |
| 21.10 |    Stanford     | ICLR2022 |    [Large language models can be strong differentially private learners](https://openreview.net/forum?id=bVuP3ltATMz)     |
| 22.02 | Google Research | ICLR2023 |           [Quantifying Memorization Across Neural Language Models](https://openreview.net/forum?id=TatRHT_1cK)            |


### 📖教程, 文章, 演示, 演讲

|日期|分类|标题|链接地址|
|:-:|:-:|:-:|:-:|
|23.10|教程|Awesome-LLM-Safety|[链接](https://github.com/ydyjya/Awesome-LLM-Safety)|


### 其他

👉[Latest&Comprehensive Privacy Paper](.//subtopic/Privacy.md)

---
## 📰事实性&错误信息（Truthfulness & Misinformation）


### 📑论文
|  日期   |               机构               |  出版信息   |                                                                    论文&链接                                                                     |
|:-----:|:------------------------------:|:-------:|:--------------------------------------------------------------------------------------------------------------------------------------------:|
| 21.09 |      University of Oxford      | ACL2022 |                         [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)                          |
| 23.11 | Harbin Institute of Technology |  arxiv  | [A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/abs/2311.05232) |
| 23.11 |    Arizona State University    |  arxiv  |                      [Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey](https://arxiv.org/abs/2311.07914)                      |

### 📖教程, 文章, 演示, 演讲

|  日期   | 分类 |            标题            |                              链接地址                               |
|:-----:|:--:|:------------------------:|:---------------------------------------------------------------:|
| 23.07 | 仓库 | llm-hallucination-survey | [链接](https://github.com/HillZhang1999/llm-hallucination-survey) |
| 23.10 | 仓库 |  LLM-Factuality-Survey   |   [链接](https://github.com/wangcunxiang/LLM-Factuality-Survey)   |
| 23.10 | 教程 |    Awesome-LLM-Safety    |       [链接](https://github.com/ydyjya/Awesome-LLM-Safety)        |

### 其他

👉[Latest&Comprehensive Truthfulness&Misinformation Paper](./subtopic/Truthfulness&Misinformation.md)

---
## 😈越狱&攻击（JailBreak & Attacks）

### 📑论文
|  日期   |    机构     |             出版信息             |                                                                   论文&链接                                                                   |
|:-----:|:---------:|:----------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------:|
| 20.12 |  Google   |     USENIX Security 2021     | [Extracting Training Data from Large Language Models](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting) |
| 22.11 | AE Studio | NIPS2022(ML Safety Workshop) |                     [Ignore Previous Prompt: Attack Techniques For Language Models](https://arxiv.org/abs/2211.09527)                     |
| 23.06 |  Google   |            arxiv             |                          [Are aligned neural networks adversarially aligned?](https://arxiv.org/abs/2306.15447)                           |
| 23.07 |    CMU    |            arxiv             |               [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)               |
| 23.10 | University of Pennsylvania |            arxiv             |                    [Jailbreaking Black Box Large Language Models in Twenty Queries](https://arxiv.org/abs/2310.08419)                     |


### 📖教程, 文章, 演示, 演讲

|  日期   |  分类   |                        标题                        |                                链接地址                                 |
|:-----:|:-----:|:------------------------------------------------:|:-------------------------------------------------------------------:|
| 23.01 |  社区   |              Reddit/ChatGPTJailbrek              |           [链接](https://www.reddit.com/r/ChatGPTJailbreak)           |
| 23.02 | 资源&教程 |                  Jailbreak Chat                  |                [链接](https://www.jailbreakchat.com/)                 |
| 23.10 |  教程   |                Awesome-LLM-Safety                |         [链接](https://github.com/ydyjya/Awesome-LLM-Safety)          |
| 23.10 |  博客   | Adversarial Attacks on LLMs(Author: Lilian Weng) | [链接](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/) |
| 23.11 |  视频   | [1hr Talk] Intro to Large Language Models<br/>From 45:45(Author: Andrej Karpathy) |         [中字链接](https://www.bilibili.com/video/BV1Hj41177fb)         |

### 其他

👉[Latest&Comprehensive JailBreak & Attacks Paper](./subtopic/Jailbreaks&Attack.md)

---
## 🛡️防御措施（Defenses）

### 📑论文
|日期|机构|出版信息|论文&链接|
|:-:|:-:|:-:|:-:|
|21.07|Google Research|ACL2022|[Deduplicating Training Data Makes Language Models Better](https://aclanthology.org/2022.acl-long.577/)|
|22.04|Anthropic|arxiv|[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2204.05862)|



### 📖教程, 文章, 演示, 演讲

|日期|分类|标题|链接地址|
|:-:|:-:|:-:|:-:|
|23.10|教程|Awesome-LLM-Safety|[链接](https://github.com/ydyjya/Awesome-LLM-Safety)|

### 其他

👉[Latest&Comprehensive Defenses Paper](./subtopic/Defenses.md)


--- 
## 💯数据集 & 评测基准（Datasets & Benchmark）

### 📑论文
|日期|机构|出版信息|论文&链接|
|:-:|:-:|:-:|:-:|
|20.09|University of Washington|EMNLP2020(findings)|[RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/abs/2009.11462)|
|21.09|University of Oxford|ACL2022|[TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)|
|22.03|MIT|ACL2022|[ToxiGen: A Large-Scale Machine-Generated datasets for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/abs/2203.09509)|


### 📖教程, 文章, 演示, 演讲

|日期|分类|标题|链接地址|
|:-:|:-:|:-:|:-:|
|23.10|教程|Awesome-LLM-Safety|[链接](https://github.com/ydyjya/Awesome-LLM-Safety)|

### 📚资源📚
- Toxicity - [RealToxicityPrompts datasets](https://toxicdegeneration.allenai.org/)
- Truthfulness - [TruthfulQA datasets](https://github.com/sylinrl/TruthfulQA)

### 其他
👉[Latest&Comprehensive datasets & Benchmark Paper](./subtopic/Datasets&Benchmark.md)

---
## 🧑‍🏫 学者 👩‍🏫 
**在这个部分，我们会列出一些我们觉得在LLM Safety领域很有建树的研究者!**

|学者|                                                                     主页&谷歌学术                                                                      |关键词&兴趣|
|:-:|:------------------------------------------------------------------------------------------------------------------------------------------------:|:-:|
|Nicholas Carlini| [主页](https://nicholas.carlini.com/) \| [谷歌学术](https://scholar.google.com/citations?hl=zh-CN&user=q4qDvAoAAAAJ&view_op=list_works&sortby=pubdate) |**the intersection of machine learning and computer security**&**neural networks from an adversarial perspective**|
|Daphne Ippolito|                    [谷歌学术](https://scholar.google.com/citations?hl=zh-CN&user=COEsqLYAAAAJ&view_op=list_works&sortby=pubdate)                     |**Natural Language Processing**|
|Chiyuan Zhang|     [主页](https://pluskid.org/) \| [谷歌学术](https://scholar.google.com/citations?hl=zh-CN&user=l_G2vr0AAAAJ&view_op=list_works&sortby=pubdate)      |**Especially interested in understanding the generalization and memorization in machine and human learning, as well as implications in related areas like privacy**|
|Katherine Lee|                    [谷歌学术](https://scholar.google.com/citations?hl=zh-CN&user=bjdB4K8AAAAJ&view_op=list_works&sortby=pubdate)                     |**natural language processing**&**translation**&**machine learning**&**computational neuroscienceattention**|
|Florian Tramèr|                   [主页](https://floriantramer.com/) \| [谷歌学术](https://scholar.google.com/citations?hl=zh-CN&user=ijH0-a8AAAAJ)                    |**Computer Security**&**Machine Learning**&**Cryptography**&**the worst-case behavior of Deep Learning systems from an adversarial perspective, to understand and mitigate long-term threats to the safety and privacy of users**|
|Jindong Wang|      [主页](https://scholar.google.com/citations?hl=zh-CN&user=hBZ_tKsAAAAJ&view_op=list_works&sortby=pubdate) \| [谷歌学术](https://jd92.wang/)       | **Large Language Models (LLMs) evaluation and robustness enhancement** 　|
|   Chaowei Xiao   |                       [主页](https://xiaocw11.github.io/) \| [谷歌学术](https://scholar.google.com/citations?user=Juoqtj8AAAAJ)                        |                                     **interested in exploring the trustworthy problem in (MultiModal) Large Language Models and studying the role of LLMs in different application domains.**                                     |
|Andy Zou|                  [主页](https://andyzoujm.github.io/) \| [谷歌学术](https://scholar.google.com/citations?hl=zh-CN&user=zir09KwAAAAJ)                   |**ML Safety**&**AI Safety**|
---
## 🧑‍🎓作者信息


**🤗如果你有任何疑问欢迎咨询作者!🤗**

✉️: [ydyjya](https://github.com/ydyjya) ➡️ zhouzhenhong@bupt.edu.cn

💬: **交流LLM Safety**

<div align="center">

<div align="center">

[Wechat Group](./resource/wechat.png) | [My Wechat](./resource/wechat.png)

</div>

</div>

---

[![Star History Chart](https://api.star-history.com/svg?repos=ydyjya/Awesome-LLM-Safety&type=Date)](https://star-history.com/#ydyjya/Awesome-LLM-Safety&Date)

**[⬆ 回到顶部](#table-of-contents)**